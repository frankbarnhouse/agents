{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab - Week 1, Day 3\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Important point - please read</h2>\n",
    "            <span style=\"color:#ff7800;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AI\n",
      "DeepSeek API Key exists and begins sk-\n",
      "Groq API Key exists and begins gsk_\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Design a comprehensive, adversarial‑resistant test-suite that reliably distinguishes between black‑box LLMs that rely primarily on pattern matching and memorization and LLMs that demonstrate genuine compositional reasoning and robust generalization; for your test‑suite, provide (1) at least six concrete task types with example inputs and expected outputs that probe different aspects of compositional reasoning (e.g., systematic generalization, causal reasoning, abstraction, counterfactuals, planning, variable‑binding), (2) a clear scoring rubric for each task, (3) protocols to prevent or detect memorized answers and prompt‑engineering gaming (including generation of unseen variants and statistical controls), (4) proposed sample sizes and statistical tests to assert with high confidence a difference in capability, (5) possible adversarial behaviors the LLM might use to appear capable and how your design addresses them, and (6) an experimental validation plan including metrics for false positives/negatives and how to iterate on the suite based on results?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "display(Markdown(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note - update since the videos\n",
    "\n",
    "I've updated the model names to use the latest models below, like GPT 5 and Claude Sonnet 4.5. It's worth noting that these models can be quite slow - like 1-2 minutes - but they do a great job! Feel free to switch them for faster models if you'd prefer, like the ones I use in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is a complete, practical test‑suite design for reliably distinguishing LLMs that primarily pattern‑match / memorize from LLMs that show genuine compositional reasoning and robust generalization. It is written to be implementable against black‑box LLMs (only I/O access). The suite mixes synthetic, randomized and structured tasks, uses statistical controls and adversarial checks, and includes validation and iteration plans.\n",
       "\n",
       "Summary of components\n",
       "- (1) Six task types (with concrete examples and expected outputs) that probe different aspects of compositional reasoning.\n",
       "- (2) A clear scoring rubric for each task.\n",
       "- (3) Protocols to prevent/detect memorized answers and prompt‑engineering gaming (randomization, held‑out vocab, paraphrase invariance, adversarial decoys, statistical controls).\n",
       "- (4) Sample sizes and statistical tests for confident capability differences.\n",
       "- (5) Likely adversarial behaviors and mitigations.\n",
       "- (6) Experimental validation plan with metrics and iteration strategy.\n",
       "\n",
       "1) Task types — descriptions, concrete inputs and expected outputs\n",
       "Each task type includes rationale (what aspect it probes), a canonical task template, 2 example items and their expected outputs. All tasks are designed to support automatic grading by canonicalization or deterministic execution where feasible.\n",
       "\n",
       "Important: In production tests, instantiate each template with many randomized variants (random tokens, symbol renamings, unseen combinations). Use synthetic vocabulary (random strings) in many items (see protocols below) to avoid memorization.\n",
       "\n",
       "Task A — Systematic generalization (composition of primitive instructions)\n",
       "- What it probes: ability to apply known primitive operations in novel combinations, i.e., SCAN‑style compositionality / systematicity.\n",
       "- Template: Define a small set of primitive actions and composition operators. Ask model to produce ground‑truth composed actions.\n",
       "- Example primitives (in an item):\n",
       "  \"Primitive actions: WALK (W) = move forward 1; JUMP (J) = move forward 2; TURN_LEFT (L) = rotate left. Composition: X and then Y means do X then Y; twice(X) means do X two times; opposite(X) means do TURN_LEFT twice then X then TURN_LEFT twice.\"\n",
       "  Input 1: \"Instruction: opposite(twice(JUMP)) then WALK.\"\n",
       "  Expected output 1: The sequence of primitive actions expanded: \"TURN_LEFT, TURN_LEFT, JUMP, JUMP, TURN_LEFT, TURN_LEFT, WALK\" (canonicalized as comma‑sep).\n",
       "  Input 2 (novel composition): \"Instruction: twice(opposite(WALK)) then JUMP.\"\n",
       "  Expected output 2: \"TURN_LEFT, TURN_LEFT, WALK, TURN_LEFT, TURN_LEFT, WALK, JUMP\"\n",
       "- Why hard for memorization: create many primitives and composition rules and withhold many combinations from training set; test on novel nesting depths and permutations.\n",
       "\n",
       "Task B — Variable binding and long‑range reference (symbolic variables)\n",
       "- What it probes: ability to bind variables to values, carry and reuse bindings across steps, disambiguate references like \"the one referenced two steps earlier\".\n",
       "- Template: Provide assignments, transformations and queries referencing earlier variables (use randomized variable names).\n",
       "- Example:\n",
       "  Input 1:\n",
       "    \"Let a = 7, b = a + 5, c = 2*b. Now update a = c - 3. What is b + a?\"\n",
       "  Expected output 1: compute stepwise:\n",
       "    b = a + 5 (using original a=7) => b=12\n",
       "    c = 2*b => c=24\n",
       "    a = c - 3 => a=21\n",
       "    b + a = 12 + 21 = 33\n",
       "    Output: \"33\"\n",
       "  Input 2 (using randomly named vars, novel referencing):\n",
       "    \"Let X1 = 4, Y_2 = X1 * 3, Z = Y_2 - X1. Then set X1 = Z + 1. What is Y_2 - X1?\"\n",
       "  Expected output 2:\n",
       "    Y_2 = 12, Z = 8, X1 becomes 9. Y_2 - X1 = 3. Output: \"3\"\n",
       "- Why hard for memorization: use long sequences (10+ assignments) and random token names; require maintaining bindings across updates.\n",
       "\n",
       "Task C — Causal reasoning and counterfactuals (graph / structural causal model)\n",
       "- What it probes: causal inference and do‑interventions, not mere correlation or associative retrieval.\n",
       "- Template: Give a small causal graph or set of structural equations. Ask consequence of interventions or counterfactual statements.\n",
       "- Example:\n",
       "  Input 1:\n",
       "    \"Variables: A -> B (B = A + noise), B -> C (C = 2*B). If we set A = 0 (do(A=0)), what happens to C? Provide final numeric relation.\"\n",
       "  Expected output 1:\n",
       "    After do(A=0): B = 0 (+noise mean 0) → C = 0. Output: \"C becomes 0 (given deterministic functions, C = 2*B, so C = 0).\"\n",
       "  Input 2 (counterfactual):\n",
       "    \"Structural model: X = 3*U, Y = X + V. Observed U=2, V=1 (so observed X=6, Y=7). Counterfactual: if U had been 1 (but V unchanged), what would Y have been?\"\n",
       "  Expected output 2:\n",
       "    New X = 3*1 = 3. Y = X + V = 3 + 1 = 4. Output: \"4\"\n",
       "- Why hard for memorization: Use freshly generated graphs and numeric values, ask do‑intervention vs observation differences; require understanding intervention semantics.\n",
       "\n",
       "Task D — Abstraction and analogical mapping (learn mapping rule, apply to novel exemplars)\n",
       "- What it probes: ability to induce abstract relations from examples and apply them to new tokens not seen in training.\n",
       "- Template: Provide a few mapping examples using a tiny invented \"language\", then ask to map new items applying the same relation.\n",
       "- Example:\n",
       "  Input 1:\n",
       "    \"Rule examples: hefo -> jopi, goro -> luma. Now apply the same transformation to zeta.\"\n",
       "    (Transformation pattern: prepend 'j' and substitute vowels mapping e->o, o->u — but hidden to the model; only examples show behavior.)\n",
       "  Expected output 1:\n",
       "    If pattern mapping produces j + (vowel shift) then zeta -> juta (example result). But to make deterministic and unambiguous, use explicitly definable transformations in generation pipeline so expected output can be computed.\n",
       "  Input 2 (using symbols):\n",
       "    \"Examples: blim -> ba-lim, srun -> sa-run. Apply to krup.\"\n",
       "    Expected output 2: \"ka-krup\" (or whatever canonical transformation defined by the example set).\n",
       "- Why hard for memorization: use arbitrary synthetic token spaces and withhold large portions of mapping space; require generalizing relational rule.\n",
       "\n",
       "Task E — Planning and hierarchical problem solving (constrained optimization)\n",
       "- What it probes: ability to plan multi-step actions under constraints, to generalize plan construction rules to larger/new configurations.\n",
       "- Template: Provide a grid or a pickup/delivery planning problem with constraints (capacity, order) and ask for a (near‑)optimal step sequence or cost.\n",
       "- Example:\n",
       "  Input 1:\n",
       "    \"Agent at (0,0) on 3x3 grid. Boxes at (1,0) (A), (2,2) (B). Deliver A then B to goal (0,2). Agent can carry 1 box. Moves: Up/Down/Left/Right cost 1; Pickup/Drop cost 0. Plan minimal steps.\"\n",
       "  Expected output 1:\n",
       "    One optimal plan, canonicalized: \"Right, Pickup, Up, Up, Drop, Right, Right, Pickup, Left, Left, Up, Drop\" — or a canonical minimal step count and sequence. Expected minimal length numeric also acceptable: \"Minimal steps = 10 (sequence: ...)\"\n",
       "  Input 2 (scale-up generalization):\n",
       "    Same rules but 5x5 grid and three boxes with constraint that order must be A then third then B. Ask for plan.\n",
       "- Why hard for memorization: use random placements, require true planning, and scale tests to larger grids / more boxes than in training.\n",
       "\n",
       "Task F — Nested recursion / compositional evaluation (interpreted mini‑language)\n",
       "- What it probes: ability to parse and evaluate expressions under user‑defined semantics, including nested composition and recursion, and to generalize to deeper nesting than seen in examples.\n",
       "- Template: Define a tiny functional language (e.g., inc(x) = x+1, dbl(x) = 2*x, swap(a,b) returns pair reversed), then evaluate nested expressions.\n",
       "- Example:\n",
       "  Input 1:\n",
       "    \"Definitions: inc(x) = x+1; dbl(x) = 2*x; compose(f,g)(x) = f(g(x)). Evaluate compose(inc, dbl)(3).\"\n",
       "  Expected output 1:\n",
       "    dbl(3)=6; inc(6)=7. Output: \"7\"\n",
       "  Input 2 (deeper / novel nesting with random function names):\n",
       "    Randomly name functions: f_z(x)=3*x, g_q(x)=x-2. Evaluate f_z(g_q(g_q(10))).\n",
       "    Expected output 2:\n",
       "    g_q(10)=8; g_q(8)=6; f_z(6)=18. Output: \"18\"\n",
       "- Why hard for memorization: use randomized function names and nesting depths beyond training examples.\n",
       "\n",
       "Cross‑task consistency probes (meta‑checks)\n",
       "- After initial answer, rephrase problem, permute variable names or ask logically equivalent forms (alpha‑renaming). A reasoning model should give consistent answers; a memorizer or pattern matcher will often fail.\n",
       "\n",
       "2) Scoring rubrics\n",
       "For each task we use a combination of exact match (for deterministic outputs), graded structural equivalence, partial credit for intermediate step correctness, and consistency checks. All scoring should be automated where possible.\n",
       "\n",
       "General guidelines\n",
       "- Normalize outputs: strip punctuation, collapse whitespace, canonicalize commutative orders if problem allows, convert numbers to canonical numeric form.\n",
       "- Where multiple correct sequences exist (planning), accept any plan that meets constraints and minimal (or near‑minimal) length. Validate by deterministic simulator.\n",
       "- Require explanation optionally as corroboration — explanations are graded separately and used to detect shallow patterning (see later).\n",
       "\n",
       "Detailed rubrics per task\n",
       "\n",
       "Task A — Systematic generalization\n",
       "- 0/1 exact correctness: full credit (1.0) if produced canonical expanded action sequence exactly matches ground truth.\n",
       "- Partial credit 0.5 if sequence is correct up to reordering of independent commuting actions or contains only superficial tokenization differences.\n",
       "- 0 if wrong action types or wrong ordering violating composition semantics.\n",
       "- Bonus +0.2 if model also outputs a correct short reasoning trace (e.g., shows stepwise expansion).\n",
       "\n",
       "Task B — Variable binding\n",
       "- Full credit (1.0) if final numeric (or symbolic) answer matches.\n",
       "- Partial credit 0.5 if intermediate steps are internally inconsistent but final answer reachable by plausible alternative interpretation; give 0 if incorrect.\n",
       "- Additionally, ask for an explanation of each assignment; give +0.1 if explanation justifies final answer.\n",
       "\n",
       "Task C — Causal reasoning / counterfactuals\n",
       "- Full credit (1.0) for correct intervention answer plus correct reasoning (identifying difference between observation and do()).\n",
       "- Partial credit 0.5 if numeric outcome correct but model incorrectly describes the causal/non‑causal distinction.\n",
       "- 0 for wrong or inconsistent counterfactual.\n",
       "\n",
       "Task D — Abstraction / analogy\n",
       "- Full credit (1.0) for correct mapped output on held‑out tokens.\n",
       "- Partial credit 0.5 if pattern partially matched (e.g., one of two transformation components correct).\n",
       "- Zero if incorrect mapping.\n",
       "\n",
       "Task E — Planning\n",
       "- Full credit (1.0) if plan meets all constraints and is optimal (or within predetermined optimality gap, e.g., +0 steps for optimal or +1 allowance for near‑optimal).\n",
       "- Partial credit 0.75 if plan valid but suboptimal within allowed slack; 0.4 if partially valid or violates minor constraints; 0 if plan invalid/unexecutable.\n",
       "- Also score plan length and feasibility automatically using a simulator.\n",
       "\n",
       "Task F — Nested evaluation\n",
       "- Full credit (1.0) if evaluated result correct.\n",
       "- Partial credit 0.5 if calculation partially correct or correct for a different but plausible semantics.\n",
       "- 0 if wrong.\n",
       "\n",
       "Consistency / adversarial checks (applies across tasks)\n",
       "- Self‑consistency score: ask the same question twice with variable renaming and paraphrase. Award extra trust when answers are stable. If model flips answers > threshold (e.g., >10% of items), mark suspicious.\n",
       "\n",
       "Aggregate scoring\n",
       "- Report per‑task accuracy and an overall composite score weighted equally or by task importance.\n",
       "- Also report error types: arithmetic errors, reference errors, plausibility but contradiction, inconsistent explanation, etc.\n",
       "\n",
       "3) Protocols to prevent/detect memorized answers and prompt‑engineering gaming\n",
       "Use a multi‑layered approach: (A) avoid giving any canonical public benchmark examples in test items; (B) randomization and synthetic languages; (C) paraphrase invariance; (D) decoys and traps; (E) statistical controls and cross‑validation; (F) output verification via external simulators.\n",
       "\n",
       "A. Synthetic, randomized, and held‑out elements\n",
       "- Random tokens and names: generate variable, function, and symbol names randomly (e.g., strings of 4–6 chars drawn from letters not composing common words). Example: X1 -> \"qerf\", function \"f_x\".\n",
       "- Random numeric seeds: numbers, positions, and graphs sampled uniformly from ranges beyond typical training corpora.\n",
       "- Use on‑the‑fly generated domain rules (e.g., transformation rules) so items are unlikely to appear in model training.\n",
       "- For each template, reserve a held‑out set of compositions (combinations of primitives, deeper nestings) not revealed anywhere else. Test on these held‑outs.\n",
       "\n",
       "B. Paraphrase invariance & renaming checks\n",
       "- For each item, probe with several paraphrases and alpha‑renamings (rename all symbols consistently). A reasoning model gives consistent answers; a pattern matcher that memorized specific surface forms will often fail on renamed forms.\n",
       "- Examples:\n",
       "  Item 1: original problem\n",
       "  Item 1a: same problem with variable renaming\n",
       "  Item 1b: logically equivalent restatement\n",
       "  If answers disagree, down‑weight confidence.\n",
       "\n",
       "C. Held‑out composition splits (systematic generalization protocol)\n",
       "- Use compositional splits analogous to SCAN: train/evaluate pairs are constructed so primitives are seen in training but some combinations are held out and used only for testing. Similarly for mapping rules and function compositions. This tests systematic generalization rather than memorization.\n",
       "\n",
       "D. Trap items and decoys\n",
       "- Embed \"publicly memorizeable\" variants that look like standard benchmarks but with small modifications. A pattern‑matcher that only recognizes the public form will output the public memorized answer rather than adapt; detect by comparing to valid answer for the modified item.\n",
       "- Adversarial decoys: ask for outputs in both canonical and unusual formats to catch prompt‑tuned models that only work in one style.\n",
       "\n",
       "E. Explain‑and‑verify protocol\n",
       "- Request both an answer and a concise 1–3 step justification. Use an automated verifier to check whether the explanation logically entails the answer (e.g., compute intermediate values or check stated steps). Pattern matchers often hallucinate plausible justifications; the verifier will catch mismatches between claimed steps and actual output.\n",
       "\n",
       "F. Multiple independent samplings + self‑consistency\n",
       "- Query the model multiple times with temperature sampling to observe variability. Pattern matching models tuned to be deterministic may produce the same memorized output; reasoning models may produce either stable or explainable variations. Use statistics of answer distribution to infer brittleness.\n",
       "\n",
       "G. Cross‑model and cross‑item statistical controls\n",
       "- Randomly split items into multiple forms; estimate item difficulty using item‑response theory (IRT) to control for item variance when comparing models.\n",
       "\n",
       "H. Avoid prompting that guides the model to memorize the test pattern\n",
       "- Randomize prompt templates; do not leak examples from the test set in prompts. Use standardized prompt skeletons with random syntactic surfaces.\n",
       "\n",
       "I. Time & leakage controls\n",
       "- Run tests before and after major model updates; if possible, time‑stamp datasets and store seeds to detect leak. Rotate new item sets periodically.\n",
       "\n",
       "4) Sample sizes and statistical tests\n",
       "Goal: confidently assert a difference in capability between two black‑box LLMs (Model A and Model B).\n",
       "\n",
       "Definitions and assumptions:\n",
       "- Treat each model's response to an item as correct/incorrect (binary) for primary analysis. Use per‑task accuracy as primary metric; composite score as secondary.\n",
       "- Tests are paired (same items evaluated by both models). Use paired tests (McNemar’s test or paired permutation) to leverage within‑item correlations.\n",
       "\n",
       "Power & sample size rules of thumb\n",
       "- For a two‑proportion comparison (unpaired) with modest effect sizes (difference = 10–20 percentage points), recommended sample per model per task ≈ 300–500 items to get power ≈ 0.8 at alpha = 0.05. But because tests are paired, required number of distinct items is lower.\n",
       "\n",
       "Paired sample: approximate calculation\n",
       "- If expecting Model A accuracy = 50% and Model B = 70% on a task (difference 20 ppt), and assuming moderate within‑item correlation, ~150–250 paired items suffices for 80% power (McNemar or paired permutation). If difference expected smaller (e.g., 10 ppt), need ~400–800 items.\n",
       "- Conservative recommended sample sizes:\n",
       "  - Per task: 400 items (distinct problems) sampled from randomized template variants.\n",
       "  - Per composite suite (6 tasks): 6 * 400 = 2400 items total. You can distribute effort (e.g., 400 per high‑importance task, 200 per less critical).\n",
       "\n",
       "Statistical tests and controls\n",
       "- Primary test: paired permutation test on per‑item difference (nonparametric, robust) or McNemar’s test for binary paired data.\n",
       "- Secondary tests:\n",
       "  - Mixed‑effects logistic regression (item as random effect, model as fixed effect) to control for item difficulty and estimate model effect across heterogeneous items.\n",
       "  - Bootstrapped confidence intervals (resample items) for per‑task accuracy difference.\n",
       "  - Multiple comparisons correction: Benjamini–Hochberg when testing multiple tasks.\n",
       "- Report effect sizes (difference in proportions, odds ratio), 95% CI, and p‑values.\n",
       "- Power analyses: compute required sample size for expected minimal detectable effect size BEFORE running full test; pilot with small N to estimate item variance.\n",
       "\n",
       "Decision thresholds\n",
       "- Define thresholds for claiming superiority. Example: model A is better than B on task if:\n",
       "  - Paired difference in accuracy > δ (e.g., δ = 0.10) AND\n",
       "  - p < 0.01 after multiple comparisons correction AND\n",
       "  - Effect robust to bootstrap and mixed‑effects regression controlling for item variance.\n",
       "\n",
       "5) Possible adversarial behaviors and countermeasures\n",
       "Below are adversarial strategies a model might use to appear capable, and corresponding mitigations in the suite.\n",
       "\n",
       "Adversarial: Surface retrieval / memorized sequence regurgitation\n",
       "- Behavior: Model stores fixed outputs for common instructions; returns memorized outputs for superficially similar prompts.\n",
       "- Mitigation: synthetic tokens, held‑out compositions, trap items that mimic public benchmarks with slight changes; paraphrase and renaming consistency checks; require correct computation on non‑natural synthetic data.\n",
       "\n",
       "Adversarial: Template matching / prompt engineering (exploiting fixed prompt formats)\n",
       "- Behavior: Model trained to respond well to specific prompt templates; game tests by matching those templates exactly.\n",
       "- Mitigation: randomize prompt phrasing and structure; test with out‑of‑template paraphrases; hide test instructions across several rewordings.\n",
       "\n",
       "Adversarial: Post‑hoc plausible explanations (hallucinated chain‑of‑thought)\n",
       "- Behavior: Model produces a plausible‑looking explanation that does not correspond to internal computation.\n",
       "- Mitigation: automatic verification of explanation steps; require computation that can be executed in a simulator and check consistency between explanation and final answer. Ask for explicit intermediate numeric values that can be validated.\n",
       "\n",
       "Adversarial: Strategic stochasticity (choose most socially pleasing answer)\n",
       "- Behavior: Model outputs an answer that appears reasonable but isn't derived by correct reasoning.\n",
       "- Mitigation: use items with unique numeric/structural answers and built simulators to check exactness. Evaluate per‑answer consistency across multiple samples.\n",
       "\n",
       "Adversarial: Learning to memorize test suite by repeated exposure (test leakage)\n",
       "- Behavior: Providers fine‑tune model on leaked items and then pass tests.\n",
       "- Mitigation: maintain a large pool of test items, rotate, and ensure many items are generated on the fly. Keep some items only as one‑time use. Audit and track tests over time to detect sudden accuracy jumps.\n",
       "\n",
       "Adversarial: Exploiting world knowledge\n",
       "- Behavior: Model uses web knowledge (e.g., named entities) instead of reasoning.\n",
       "- Mitigation: use synthetic domains and names that aren't in corpus; prefer abstract variables and gibberish tokens.\n",
       "\n",
       "Adversarial: Exploiting consistent wrong heuristics (e.g., always answer \"no\" to avoid errors)\n",
       "- Behavior: Model learns a default safe answer that is occasionally correct by chance.\n",
       "- Mitigation: measure baselines and chance rates and compare; include control items where a naive heuristic fails frequently.\n",
       "\n",
       "6) Experimental validation plan, metrics for false positives/negatives, and iteration\n",
       "Validation phases\n",
       "- Phase 0 — Unit tests: small sample (N=50 per task) to verify item generation, canonicalization, and grading logic.\n",
       "- Phase 1 — Pilot: evaluate 3–5 diverse models (known baselines: simple pattern models, older LLM, and an advanced recent LLM) on a larger pilot (N=200 per task). Use pilot to estimate item difficulties and variance.\n",
       "- Phase 2 — Full evaluation: run full suite (recommended N=400 per task) across models under test.\n",
       "- Phase 3 — Adversarial robustness checks: invite models purposely trained to game tests; analyze failures and iterate.\n",
       "\n",
       "Metrics\n",
       "- Primary: per‑task accuracy (binary), composite accuracy.\n",
       "- Secondary: explanation correctness rate; self‑consistency rate (fraction of paraphrases with same answer); variability under sampling (entropy of answers).\n",
       "- Discrimination metrics: Area Under ROC curve if defining continuous scoring; effect sizes and odds ratios.\n",
       "- Error analysis metrics:\n",
       "  - False positives (FP): model flagged as reasoning‑capable but actually relying on memorization. Estimate by hand‑inspecting suspicious items and adversarially designed holdouts. Also measure cases where model passes synthetic tasks but fails renamed/scrubbed variants.\n",
       "  - False negatives (FN): model truly capable but fails tests (e.g., due to prompt format, minor ambiguity). Detect by analyzing cases where model yields correct reasoning traces but output form differs; retrain canonicalizer.\n",
       "\n",
       "Estimating FP/FN rates\n",
       "- Use validation with ground‑truth labeled model types:\n",
       "  - Create/collect models we know: (i) a trained memorization baseline (e.g., n‑gram or memorization‑augmented seq2seq), (ii) a symbolic reasoner (deterministic oracle), (iii) a hybrid model.\n",
       "  - Run full suite and measure classification (declared as reasoning vs not) vs known ground truth. Compute sensitivity (TPR) and specificity (1−FPR). Tune decision thresholds to maintain acceptable FP (e.g., <5%) while maximizing sensitivity.\n",
       "\n",
       "Iterative refinement based on results\n",
       "- Analyze item‑level statistics: identify low discrimination items (very easy or very hard) using IRT or item discrimination index. Remove or rework items with poor discrimination.\n",
       "- Increase adversarial item proportion if many models game the suite.\n",
       "- Expand synthetic vocabulary and composition depth if memorization leakage is observed.\n",
       "- Automate item generation pipelines to produce fresh held‑out sets periodically.\n",
       "- Maintain an audit log of test runs and items to detect potential leakage and overfitting by model providers.\n",
       "\n",
       "Example iteration cycle\n",
       "1. Pilot run → compute per‑task accuracies, item difficulty.\n",
       "2. Identify items where memorizing baselines scored as well as strong reasoning oracle → mark for redesign (add randomization, deeper nesting, or rename tokens).\n",
       "3. Add paraphrase/renaming tests for items showing potential prompt‑template exploitation.\n",
       "4. Rebalance item pool to maintain discrimination and rerun.\n",
       "\n",
       "Practical considerations and recommended implementation checklist\n",
       "- Implement automatic generators for each template with reproducible random seeds.\n",
       "- Implement deterministic simulators/validators for each task (action sequence executor, arithmetic evaluator, planner checker) so grading is exact.\n",
       "- For each item store (seed, tokens, canonical answer, distractors).\n",
       "- Maintain multiple prompt forms and sample at random per query.\n",
       "- Log all outputs, probabilistic sampling states (temperature), and timestamps.\n",
       "- Use paired testing (same items across all models) to maximize power and fairness.\n",
       "- Publicly release small sanitized subsets for community verification but keep main test pools private to reduce overfitting.\n",
       "\n",
       "Concluding guidance\n",
       "- No single test proves genuine reasoning forever; use this suite as a robust, multi‑task battery that measures “behavioral evidence” of compositional reasoning and generalization.\n",
       "- Emphasize cross‑task consistency, randomness/held‑out composition, and rigorous statistical testing.\n",
       "- Make verification automated where possible (simulators) and include human review for ambiguous failures.\n",
       "- Iterate the pool and keep fresh synthetic items to reduce memorization/leakage risk.\n",
       "\n",
       "If you’d like, I can:\n",
       "- Produce a reference implementation (Python) that generates randomized items for each template and includes canonicalizers and simulators for automated scoring.\n",
       "- Compute precise sample size calculations for specific expected effect sizes you care about and provide a ready‑to‑run experimental script."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The API we know well\n",
    "# I've updated this with the latest model, but it can take some time because it likes to think!\n",
    "# Replace the model with gpt-5-mini if you'd prefer not to wait 1-2 mins\n",
    "\n",
    "model_name = \"gpt-5-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Comprehensive Test Suite for Distinguishing Pattern Matching from Compositional Reasoning in LLMs\n",
       "\n",
       "## Executive Summary\n",
       "\n",
       "This test suite employs six task categories designed to probe genuine compositional reasoning while resisting memorization and gaming. Each task incorporates systematic variation, procedural generation, and adversarial controls to ensure that success requires compositional understanding rather than pattern matching.\n",
       "\n",
       "---\n",
       "\n",
       "## 1. Task Types with Examples\n",
       "\n",
       "### Task 1: Systematic Compositional Generalization (Novel Recombinations)\n",
       "\n",
       "**Objective**: Test ability to combine known primitives in novel configurations never seen during training.\n",
       "\n",
       "**Design Principle**: Use a microworld with defined rules, then test combinations statistically unlikely to appear in training data.\n",
       "\n",
       "**Example Input**:\n",
       "```\n",
       "Rules of the GLORP system:\n",
       "- FLIM(x) reverses the order of elements in x\n",
       "- DRAK(x, y) interleaves elements from x and y\n",
       "- ZOOP(x) duplicates every second element in x\n",
       "\n",
       "Training examples:\n",
       "FLIM([A, B, C]) = [C, B, A]\n",
       "DRAK([1, 2], [3, 4]) = [1, 3, 2, 4]\n",
       "ZOOP([P, Q, R]) = [P, Q, Q, R]\n",
       "\n",
       "Test question:\n",
       "What is FLIM(DRAK(ZOOP([X, Y]), [M, N]))?\n",
       "```\n",
       "\n",
       "**Expected Output**: \n",
       "```\n",
       "Step-by-step reasoning:\n",
       "1. ZOOP([X, Y]) = [X, Y, Y]\n",
       "2. DRAK([X, Y, Y], [M, N]) = [X, M, Y, N, Y]\n",
       "3. FLIM([X, M, Y, N, Y]) = [Y, N, Y, M, X]\n",
       "\n",
       "Answer: [Y, N, Y, M, X]\n",
       "```\n",
       "\n",
       "**Scoring Rubric**:\n",
       "- 4 points: Correct answer with valid step-by-step reasoning\n",
       "- 3 points: Correct answer with minor reasoning errors\n",
       "- 2 points: Incorrect answer but demonstrates understanding of composition\n",
       "- 1 point: Partially correct intermediate steps\n",
       "- 0 points: Incorrect with no valid reasoning\n",
       "\n",
       "**Variants for Anti-Memorization**:\n",
       "- Generate 10,000 unique microworlds with different operation names, symbols, and rule sets\n",
       "- Use procedural generation with random seeds\n",
       "- Operation names drawn from pronounceable non-words (e.g., BLICKET, WUGGY, FEPS)\n",
       "- Vary depth of composition (2-5 levels)\n",
       "- Test both symbolic and numeric domains\n",
       "\n",
       "---\n",
       "\n",
       "### Task 2: Causal Reasoning Under Intervention (Counterfactual Inference)\n",
       "\n",
       "**Objective**: Distinguish correlation from causation and reason about interventions.\n",
       "\n",
       "**Design Principle**: Present causal graphs implicitly through scenarios, then test counterfactual reasoning that requires understanding causal structure.\n",
       "\n",
       "**Example Input**:\n",
       "```\n",
       "Scenario: In the town of Millbrook, the following patterns have been observed over 10 years:\n",
       "\n",
       "- When the reservoir level is high, the water treatment plant runs at full capacity\n",
       "- When the treatment plant runs at full capacity, downtown water pressure is strong\n",
       "- When downtown water pressure is strong, the fountain in Central Park operates\n",
       "- The reservoir level depends only on rainfall\n",
       "- Rainfall also directly affects whether street cleaning happens (rain = no cleaning)\n",
       "\n",
       "Historical data shows that on days when the fountain operates, streets are usually dirty.\n",
       "\n",
       "Question 1: The town installs a new pump that allows the fountain to operate regardless of downtown water pressure. After this intervention, will the streets be cleaner or dirtier on days when the fountain operates, compared to before?\n",
       "\n",
       "Question 2: Explain your reasoning using the causal structure.\n",
       "```\n",
       "\n",
       "**Expected Output**:\n",
       "```\n",
       "Answer: The streets will be CLEANER on fountain-operating days after the intervention.\n",
       "\n",
       "Reasoning: \n",
       "Before intervention: Fountain operating → High reservoir → Recent rainfall → No street cleaning → Dirty streets\n",
       "\n",
       "The correlation between fountain operation and dirty streets was due to a common cause (rainfall), not because the fountain caused dirty streets.\n",
       "\n",
       "After intervention: The fountain can operate without rainfall, breaking the spurious correlation. Fountain operation is now independent of rainfall, so there's no reason to expect systematically dirtier streets.\n",
       "```\n",
       "\n",
       "**Scoring"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "model_name = \"claude-sonnet-4-5\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This test-suite, titled **CORE-Eval (Compositional & Operational Reasoning Evaluation)**, is designed to bypass the \"stochastic parrot\" effect by focusing on out-of-distribution (OOD) tasks that require the dynamic manipulation of novel variables and rules.\n",
       "\n",
       "---\n",
       "\n",
       "### 1. Concrete Task Types\n",
       "\n",
       "#### Task A: Recursive Nested State Tracking (Variable Binding)\n",
       "*   **Concept:** Track the state of objects through recursive \"if-then\" swaps and containment changes.\n",
       "*   **Input:** \"There are three boxes: Red, Blue, and Green. Red contains a 'Glint'. Blue contains a 'Spark'. Green is empty. Rule 1: If an object is moved to an empty box, its name reverses. Rule 2: If two boxes swap, their contents swap. Step 1: Swap Red and Blue. Step 2: Move contents of Blue to Green. Step 3: Rule 1 applies to Green. What is in Green?\"\n",
       "*   **Expected Output:** \"tnilG\" (The 'Glint' moved from Red to Blue in Step 1, then from Blue to Green in Step 2, triggering Rule 1).\n",
       "\n",
       "#### Task B: Counterfactual Physics Reasoning (Causal Reasoning)\n",
       "*   **Concept:** Apply logical deductions in a world where one fundamental law of physics is altered.\n",
       "*   **Input:** \"In this world, gravity acts as a repellent for liquids but a vacuum for solids. If I tip a glass of water upside down over a table, and there is a wooden block on that table, what happens to the water and the block?\"\n",
       "*   **Expected Output:** The water moves upward (away from the center of mass/floor) and the block is pulled toward the ceiling (vacuum effect).\n",
       "\n",
       "#### Task C: The \"Zylophon\" Syntax (Systematic Generalization)\n",
       "*   **Concept:** Learn a 3-rule pseudo-grammar and apply it to a 20-word sentence.\n",
       "*   **Input:** \"Grammar: (1) Nouns end in '-ox'. (2) Verbs precede nouns. (3) Adjectives follow the noun they modify and must be repeated twice. Translate: 'The fast cat chases a small mouse' into Zylophon.\"\n",
       "*   **Expected Output:** \"Chases catox fast fast mouseox small small.\" (Requires consistent rule application over lexical substitution).\n",
       "\n",
       "#### Task D: Strategic Pathfinding with Dynamic Obstacles (Planning)\n",
       "*   **Concept:** Solve a grid-based navigation task where the \"cost\" of movement changes based on the history of moves.\n",
       "*   **Input:** \"Grid 4x4. Start (0,0), Goal (3,3). Moving East costs 1. Moving South costs 2. However, every time you move South, the cost of the next East move doubles. Provide the sequence of moves for the lowest cost.\"\n",
       "*   **Expected Output:** A specific path (e.g., E, E, E, S, S, S) with a calculated total cost.\n",
       "\n",
       "#### Task E: Functional Abstraction (Abstraction)\n",
       "*   **Concept:** Identify a latent function from input-output pairs and apply it to a complex, non-obvious case.\n",
       "*   **Input:** \"f(apple) = 1, f(banana) = 3, f(kiwi) = 2. Rule: f(x) = count of vowels in word. Apply f to the result of f('pomegranate') expressed as a word.\"\n",
       "*   **Expected Output:** \"f(5) -> f('five') -> 2.\"\n",
       "\n",
       "#### Task F: Adversarial Syllogisms (Logic vs. Pattern Matching)\n",
       "*   **Concept:** Logical structures where the conclusion is factually false in the real world but logically valid within the prompt.\n",
       "*   **Input:** \"All surgeons are cats. Some cats are astronauts. Therefore, are some surgeons astronauts? Explain the necessity of the conclusion.\"\n",
       "*   **Expected Output:** No. (Standard logical fallacy: the 'undistributed middle'). Memorization-based models often default to \"Yes\" because the semantic \"vibes\" of surgeons and astronauts are high-competence.\n",
       "\n",
       "---\n",
       "\n",
       "### 2. Scoring Rubric\n",
       "Each task is scored on a 0-3 scale:\n",
       "*   **0 (Failure):** Incorrect final answer and flawed logic or hallucinations.\n",
       "*   **1 (Partial):** Correct logic but arithmetic/minor execution error; or correct answer but \"hollow\" explanation.\n",
       "*   **2 (Successful):** Correct final answer and clear, step-by-step reasoning (Chain of Thought).\n",
       "*   **3 (Robust):** Correct answer even when the prompt includes \"distractor\" information (e.g., \"Note: the boxes are made of wood,\" which is irrelevant).\n",
       "\n",
       "---\n",
       "\n",
       "### 3. Protocols to Prevent Memorization/Gaming\n",
       "*   **The DSL Generator:** Use a Domain Specific Language (DSL) script to generate 1,000 unique variations of each task. This ensures no specific instance exists in the training data.\n",
       "*   **Nonce-Token Substitution:** Replace all semantic nouns (e.g., \"cat\") with random strings (e.g., \"blorp-7\"). This strips the model of \"semantic priming\" where it guesses based on word associations rather than logic.\n",
       "*   **Permutation Testing:** For every prompt, provide a version where the order of rules is changed. A reasoning model will adapt; a pattern-matcher will often fail if it memorized a specific sequence.\n",
       "\n",
       "---\n",
       "\n",
       "### 4. Sample Size and Statistical Tests\n",
       "*   **Sample Size:** $N=200$ unique prompts per task type (Total 1,200).\n",
       "*   **Metric:** Accuracy (binary) and Mean Score (0-3).\n",
       "*   **Statistical Test:** \n",
       "    *   **McNemar’s Test:** To compare two models on the same set of binary outcomes (to see if the performance difference is statistically significant).\n",
       "    *   **Standard Deviation of Error:** If a model’s performance drops significantly when nouns are replaced with nonce-tokens, it is flagged as relying on **pattern matching** (High Semantic Sensitivity).\n",
       "\n",
       "---\n",
       "\n",
       "### 5. Adversarial Behaviors & Design Counter-Measures\n",
       "*   **Behavior: Hedging/Refusal.** The model says \"This depends on the laws of physics\" to avoid commitment.\n",
       "    *   *Counter-measure:* Explicitly state \"Assume this hypothetical world is the only reality for this calculation.\"\n",
       "*   **Behavior: Chain-of-Thought Mimicry.** The model writes \"Step 1... Step 2...\" but the steps don't lead to the conclusion.\n",
       "    *   *Counter-measure:* Use **Conclusion-to-Step Verification**. Ask the model for the answer first, then the steps, then provide a *different* answer and ask it to find the error.\n",
       "*   **Behavior: Semantic Drift.** The model starts with the counterfactual but drifts back to real-world logic mid-sentence.\n",
       "    *   *Counter-measure:* **Long-Chain Dependency.** Ensure the final answer requires the output of Step 1 to be used in Step 10.\n",
       "\n",
       "---\n",
       "\n",
       "### 6. Experimental Validation & Iteration\n",
       "*   **Gold Standard:** Run the suite on a \"Ground Truth\" group of human logic experts.\n",
       "*   **False Positive (FP) Metric:** If a known \"shallow\" model (e.g., a 1B parameter model) passes, the task is too simple. Increase the recursive depth.\n",
       "*   **False Negative (FN) Metric:** If humans fail the task, it is a measure of complexity, not reasoning. Simplify until human accuracy is $>95\\%$.\n",
       "*   **Iteration Loop:** \n",
       "    1.  Deploy $N=50$. \n",
       "    2.  Check for \"Shortcut Cues\" (e.g., if the answer is always the last item mentioned). \n",
       "    3.  If a model identifies a shortcut, re-randomize the prompt generator to eliminate the cue. \n",
       "    4.  Scale to $N=1,200$."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-3-flash-preview\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# **Adversarial‑Resistant Test Suite for Compositional Reasoning vs. Pattern‑Matching in Black‑Box LLMs**\n",
       "\n",
       "## **1. Core Philosophy**\n",
       "The suite is designed to be **variants‑rich, systematically generated, and dynamically updated** to prevent memorization. It tests **out‑of‑distribution generalization** by requiring recombination of known primitives in novel ways, and uses **control tasks** to detect shallow heuristics.\n",
       "\n",
       "---\n",
       "\n",
       "## **2. Six Task Types with Examples**\n",
       "\n",
       "### **Task Type 1: Systematic Generalization (Syntax‑Semantics Mismatch)**\n",
       "**Goal:** Test if the model can apply known syntactic rules to novel lexical items or semantic domains.\n",
       "\n",
       "**Example Input:**\n",
       "```\n",
       "Rule: \"Every X that Ys is Z.\" → \"All Xs that Y are Z.\"\n",
       "Apply to: \"Every blimp that froods is tink.\"\n",
       "```\n",
       "**Expected Output:** `\"All blimps that frood are tink.\"`\n",
       "\n",
       "**Scoring Rubric:**\n",
       "- 2 pts: Correct transformation with morphological adjustment (frood→frood, not froods→frood).\n",
       "- 1 pt: Correct transformation but minor morphological error.\n",
       "- 0 pts: Failure to apply rule, or parroting input.\n",
       "\n",
       "**Adversarial Defense:** Use **nonce words** (blimp, frood, tink) with no web occurrence; generate 100 variants per test with random nonce triples.\n",
       "\n",
       "---\n",
       "\n",
       "### **Task Type 2: Causal Reasoning with Novel Mechanisms**\n",
       "**Goal:** Test understanding of causal chains, distinguishing causation from correlation, and handling novel causal verbs.\n",
       "\n",
       "**Example Input:**\n",
       "```\n",
       "In a novel system: \"Grobling causes flumps to shank, but shanking prevents grobling. If you grobl a flump, what happens after two cycles?\"\n",
       "```\n",
       "**Expected Output:** `\"After one cycle: flump shanks. After two cycles: shanking prevents grobling, so grobling stops, flump no longer shanks.\"`\n",
       "\n",
       "**Scoring Rubric:**\n",
       "- 2 pts: Correct step‑by‑step trace with state updates.\n",
       "- 1 pt: Correct final state but missing intermediate reasoning.\n",
       "- 0 pts: Wrong final state or contradictory reasoning.\n",
       "\n",
       "**Adversarial Defense:** Randomize causal graphs (A→B, B inhibits A, etc.) with random nonce names; require step‑by‑step explanation.\n",
       "\n",
       "---\n",
       "\n",
       "### **Task Type 3: Abstraction & Analogical Mapping**\n",
       "**Goal:** Test ability to extract relational schema and apply it to a new domain.\n",
       "\n",
       "**Example Input:**\n",
       "```\n",
       "\"A glorp is to a fleeb as a trank is to a ______.\"  \n",
       "Context: \"Glorps modify fleebs; tranks modify…\"\n",
       "```\n",
       "**Expected Output:** `\"spindle\"` (given novel mapping: tranks modify spindles).\n",
       "\n",
       "**Scoring Rubric:**\n",
       "- 2 pts: Correct answer + correct relation stated.\n",
       "- 1 pt: Correct answer only.\n",
       "- 0 pts: Wrong answer.\n",
       "\n",
       "**Adversarial Defense:** Use **randomly generated analogies** with arbitrary relations (modifies, contains, opposes, etc.) and novel object names.\n",
       "\n",
       "---\n",
       "\n",
       "### **Task Type 4: Counterfactual Reasoning with Novel Premises**\n",
       "**Goal:** Test reasoning about what would be true if a known fact were different.\n",
       "\n",
       "**Example Input:**\n",
       "```\n",
       "\"In world W: All zingers are fribble. Moops are not zingers. Are moops fribble?  \n",
       "Now consider: If zingers were not fribble, would moops be fribble?\"\n",
       "```\n",
       "**Expected Output:** `\"In actual world: unknown (moops may or may not be fribble). In counterfactual: moops are not fribble (because only zingers were fribble, now nothing is).\"`\n",
       "\n",
       "**Scoring Rubric:**\n",
       "- 2 pts: Correct both actual and counterfactual with justification.\n",
       "- 1 pt: Correct counterfactual only.\n",
       "- 0 pts: Wrong.\n",
       "\n",
       "**Adversarial Defense:** Randomize logical forms (∀x(Z(x)→F(x)), ¬∀x(M(x)→Z(x)), etc.) with nonce predicates.\n",
       "\n",
       "---\n",
       "\n",
       "### **Task Type 5: Hierarchical Planning with Novel Constraints**\n",
       "**Goal:** Test ability to decompose a goal into sub‑goals under new constraints.\n",
       "\n",
       "**Example Input:**\n",
       "```\n",
       "\"Goal: Get a sprocket from the grove. Actions: you can ‘zazz’ (moves sprockets to the grove if they are in the vault), ‘vinn’ (moves you to the grove), ‘lork’ (locks the vault if you are there). The sprocket is in the vault, which is locked. You are outside. What sequence works?\"\n",
       "```\n",
       "**Expected Output:** `\"Vinn to vault, lork to unlock, zazz sprocket to grove, vinn to grove.\"`\n",
       "\n",
       "**Scoring Rubric:**\n",
       "- 2 pts: Correct minimal‑step sequence.\n",
       "- 1 pt: Correct but redundant steps.\n",
       "- 0 pts: Incorrect or impossible sequence.\n",
       "\n",
       "**Adversarial Defense:** Generate random **STRIPS‑like planning problems** with novel action names and preconditions; vary initial/goal states.\n",
       "\n",
       "---\n",
       "\n",
       "### **Task Type 6: Variable Binding & Quantifier Scope**\n",
       "**Goal:** Test handling of quantifier scope ambiguities with novel predicates.\n",
       "\n",
       "**Example Input:**\n",
       "```\n",
       "\"Every blip is in a cloop. A cloop is damp. Does it follow that every blip is in a damp cloop? Explain.\"\n",
       "```\n",
       "**Expected Output:** `\"No. Every blip is in some cloop, but possibly different cloops; not all those cloops must be damp.\"`\n",
       "\n",
       "**Scoring Rubric:**\n",
       "- 2 pts: Correct answer with correct logical explanation.\n",
       "- 1 pt: Correct answer with vague explanation.\n",
       "- 0 pts: Wrong answer.\n",
       "\n",
       "**Adversarial Defense:** Randomize quantifier order (∀∃ vs ∃∀), use nonce predicates, ask for explanation to avoid guessing.\n",
       "\n",
       "---\n",
       "\n",
       "## **3. Protocols Against Memorization & Gaming**\n",
       "\n",
       "### **Dynamic Variant Generation**\n",
       "- Each task instance is generated from a **seed‑based procedural algorithm** with nonce words/relations.\n",
       "- Maintain a **held‑out variant pool** not used during public benchmark releases.\n",
       "- For each task type, generate **10,000 unique variants**; use random subsets per test session.\n",
       "\n",
       "### **Statistical Controls**\n",
       "- Include **“pattern‑matching catch trials”**: tasks that look superficially similar but require different reasoning; memorizing‑based models will fail.\n",
       "- Example: A sentence with same surface form as a known training example but with reversed logic.\n",
       "\n",
       "### **Answer Consistency Checks**\n",
       "- For each task, ask **the same underlying question in two different surface forms** (paraphrase, different nonce words). A reasoning model should give consistent answers; a memorizing model may not.\n",
       "- Use **temporal probing**: ask follow‑up questions that require maintaining variable bindings across turns.\n",
       "\n",
       "### **Explanations Required**\n",
       "- Force chain‑of‑thought (CoT) explanations. Use **explanation‑consistency scoring**: if the final answer contradicts the explanation, penalize heavily.\n",
       "\n",
       "### **Adversarial Training Data Detection**\n",
       "- Compare responses to **web‑search snippets** for nonce words; if any match appears, flag potential data contamination.\n",
       "- Deploy **canary tokens** in public descriptions of the suite; if models output these tokens, they have seen the test suite in training.\n",
       "\n",
       "---\n",
       "\n",
       "## **4. Sample Sizes & Statistical Tests**\n",
       "\n",
       "### **Per‑Task Design**\n",
       "- **Minimum sample**: 200 instances per task type (1200 total), balanced across sub‑types.\n",
       "- Each instance is scored 0‑2, yielding a **mean score per task**.\n",
       "\n",
       "### **Statistical Tests**\n",
       "- **Primary metric**: Mean composite score (sum over tasks) compared between two models.\n",
       "- Use **Mann‑Whitney U test** (non‑parametric, doesn’t assume normality) for score distributions.\n",
       "- **Confidence intervals** for per‑task score differences via bootstrap (1000 resamples).\n",
       "- **Effect size**: Cliff’s delta for robust interpretation.\n",
       "\n",
       "### **Power Analysis**\n",
       "- To detect a difference of **0.3 points** (out of 12 total) with 80% power and α=0.05, need ~100 instances per task for two‑group comparison.\n",
       "- Our 200 per task provides power to detect smaller effects.\n",
       "\n",
       "---\n",
       "\n",
       "## **5. Possible Adversarial Behaviors & Mitigations**\n",
       "\n",
       "| **Adversarial Behavior** | **Mitigation in Our Design** |\n",
       "|--------------------------|-------------------------------|\n",
       "| **Prompt‑engineering to guess pattern** | Require explanation; randomize surface forms; use nonce words. |\n",
       "| **Fine‑tuning on similar benchmarks** | Use dynamic generation; deploy canary tokens; test on held‑out variant pool. |\n",
       "| **Retrieval‑augmented generation (RAG)** | Nonce words ensure no matching documents; novel compositions unseen. |\n",
       "| **Over‑producing multiple answers** | Evaluate only first coherent answer; consistency checks across paraphrases. |\n",
       "| **Exploiting task‑specific heuristics** | Include “catch trials” that break heuristics; vary task parameters widely. |\n",
       "| **Memorizing from explanations in training** | Generate novel reasoning chains; randomize order of steps in explanations. |\n",
       "\n",
       "---\n",
       "\n",
       "## **6. Experimental Validation Plan**\n",
       "\n",
       "### **Phase 1: Baselines**\n",
       "- Test known **pattern‑matching‑heavy models** (e.g., smaller LLMs, n‑gram baselines) vs. **alleged reasoning models** (e.g., GPT‑4, Claude 3).\n",
       "- Expect clear separation in scores.\n",
       "\n",
       "### **Phase 2: Contamination Check**\n",
       "- For each model, test on **canary instances**; if performance is perfect on those but poor on random variants, flag contamination.\n",
       "- Compare performance on **public variant set** vs. **held‑out variant set**; a contaminated model will show a large drop.\n",
       "\n",
       "### **Phase 3: Iteration**\n",
       "- **False Positive Rate (FPR)**: Rate at which pattern‑matchers score above threshold. Estimate by testing on pure n‑gram/retrieval baselines.\n",
       "- **False Negative Rate (FNR)**: Rate at which true reasoners score below threshold. Estimate by testing humans (experts) on a subset.\n",
       "- **Iteration**: If FPR > 5% or FNR > 10%, analyze failure cases, add new task variants or new “catch” subtasks, and adjust scoring rubric.\n",
       "\n",
       "### **Metrics for Suite Quality**\n",
       "- **Discrimination index**: (Mean score of top known reasoners – mean score of pattern‑matchers) / pooled SD. Target > 2.0.\n",
       "- **Internal consistency**: Cronbach’s alpha across task instances; target > 0.8.\n",
       "- **Test‑retest reliability**: Same model tested on different variant sets; correlation target > 0.9.\n",
       "\n",
       "### **Long‑Term Upkeep**\n",
       "- **Quarterly updates** of nonce words and relation sets.\n",
       "- **Community contribution** of new task templates via controlled repository.\n",
       "- **Continuous adversarial red‑teaming** to find gaming strategies and patch them.\n",
       "\n",
       "---\n",
       "\n",
       "## **Conclusion**\n",
       "This test suite is **generative, scalable, and adversarial‑resistant** by design. It focuses on **systematic generalization** with novel primitives, requires **explanatory consistency**, and uses **statistical controls** to separate memorization from reasoning. With proper deployment and iteration, it can serve as a robust benchmark for genuine compositional reasoning in black‑box LLMs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is a **complete, adversarial‑resistant test‑suite** that can be used to separate **“pattern‑matching / memorisation‑heavy”** language models from those that truly **compose, reason and generalise**.  \n",
       "The design follows best‑practice guidelines from the psychometrics of AI evaluation (e.g., Item‑Response Theory, bootstrap hypothesis testing) and includes concrete items, scoring rules, anti‑gaming protocols, statistical power calculations, threat‑modelling, and a validation plan.\n",
       "\n",
       "---\n",
       "\n",
       "## 1. Six Concrete Task Types (with examples)\n",
       "\n",
       "| # | Task Type | What it probes | Core compositional demand | Example Prompt (input) | Expected Answer (output) | Why a memorisation‑only model should fail |\n",
       "|---|-----------|----------------|---------------------------|------------------------|--------------------------|------------------------------------------|\n",
       "| 1 | **Systematic Generalisation (SCG)** – novel slot‑filling on a known grammar | Ability to apply a learned rule to unseen lexical items | “<subject> <verb‑past> the <object>” → “<object> was <verb‑past‑by‑subject>” | *Prompt:* “Translate to passive voice: *The *florists* *arranged* the *vases*.” | “The vases were arranged by the florists.” | The model must recombine known syntactic transformations with *new* nouns and verbs it has never seen together. |\n",
       "| 2 | **Causal‑Chain Reasoning (CCR)** – multi‑step cause → effect inference | Understanding and chaining causal relations | *Prompt:* “If the thermostat is set to 22 °C, the heater turns on. If the heater turns on, the room warms up. What will happen if the thermostat is set to 22 °C?” | “The heater will turn on, and the room will warm up.” | Requires chaining two rules; a pure pattern‑matcher would need the exact whole‑sentence pattern in its training data, which is unlikely. |\n",
       "| 3 | **Abstract Symbol Manipulation (ASM)** – variable binding & substitution in a tiny “programming” language | Binding variables, applying functions, preserving scope | *Prompt:* “In a language where `F(x)=x+2` and `G(y)=y*3`, compute `F(G(4))`.” | “22” | The model must treat `F` and `G` as *functions* and apply them compositionally, not retrieve a memorised answer for “F(G(4))”. |\n",
       "| 4 | **Counterfactual Reasoning (CFR)** – “what‑if” world changes | Evaluating a scenario under a hypothetical change while keeping other facts constant | *Prompt:* “John is taller than Mary. If John were 5 cm shorter, would he still be taller than Mary? (Mary is 165 cm tall.)” | “Yes, because John would still be 166 cm tall (originally 171 cm).” | Requires keeping the original facts, applying the counterfactual transformation, and re‑evaluating the comparison. |\n",
       "| 5 | **Planning & Constraint Satisfaction (PCS)** – generate a sequence that satisfies a set of constraints | Multi‑step planning, maintaining state, respecting constraints | *Prompt:* “Place three red, two blue and one green token on a line of six cells so that no two tokens of the same colour are adjacent.” | Any valid ordering, e.g., “R B R G R B”. | The answer must be **constructed**; a memorised list of valid strings is improbable because the colour‑counts are randomised per item. |\n",
       "| 6 | **Relational Analogy with Variable Binding (RAVB)** – map relational structure from one domain to another | Abstract relational mapping, not surface similarity | *Prompt:* “In the story, the rabbit hides the carrot, and the fox steals the rabbit. Which of the following statements preserves the same relational pattern?  A) The cat eats the mouse, and the dog chases the cat.  B) The king crowns the queen, and the queen advises the king.  C) The painter paints a portrait, and the portrait hangs on the wall.” | “C) The painter paints a portrait, and the portrait hangs on the wall.” | Requires recognizing the *agent‑patient‑action* chain (A → B) and mapping it, not just picking the answer that shares surface words. |\n",
       "\n",
       "**Key design properties**\n",
       "\n",
       "* **Randomised lexical content** (nouns, verbs, numbers, colours) per item → eliminates exact‑match memorisation.  \n",
       "* **Controlled grammar** – the underlying rule stays the same while surface tokens change.  \n",
       "* **Multiple correct solutions** (e.g., PCS) → prevents a model from outputting a single memorised string.  \n",
       "\n",
       "---\n",
       "\n",
       "## 2. Scoring Rubric per Task\n",
       "\n",
       "| Task | Scoring granularity | Full credit (1.0) | Partial credit (0.5) | Zero (0) | Comments |\n",
       "|------|--------------------|-------------------|----------------------|----------|----------|\n",
       "| SCG | Exact string match after normalisation (case, punctuation) | Exact passive‑voice transformation | Minor grammatical slip (e.g., “by the florists” missing) | Wrong voice / wrong nouns | Normalise via regex. |\n",
       "| CCR | Proposition‑level correctness (two propositions) | Both cause‑effect statements correct | One of two statements correct | Neither correct or contradictory | Use automated entailment check (e.g., NLI model) + regex fallback. |\n",
       "| ASM | Numeric answer | Exact integer | Off‑by‑1 (e.g., due to rounding) | Wrong number / non‑numeric | Evaluate with `int()` conversion. |\n",
       "| CFR | Boolean + justification | Correct truth value **and** correct numeric comparison | Correct truth value but missing justification | Wrong truth value | Automatic parsing of “Yes/No” and numeric reasoning. |\n",
       "| PCS | Constraint satisfaction check | All constraints satisfied | Exactly one adjacency violation | More than one violation or length mismatch | Verify with a simple script that checks adjacency & colour counts. |\n",
       "| RAVB | Correct choice letter | Exact correct option | Selecting an answer that shares one relational component (e.g., same agent) | Incorrect relational mapping | Use string‑matching of relational pattern; optional human audit. |\n",
       "\n",
       "**Overall Score** – sum of task scores (max = 6). For statistical analysis we treat each task as an **item** with a binary/partial score and compute an *Item‑Response* vector per model.\n",
       "\n",
       "---\n",
       "\n",
       "## 3. Protocols to Prevent / Detect Memorised Answers & Prompt‑Engineering Gaming  \n",
       "\n",
       "| Threat | Counter‑measure (implementation) |\n",
       "|--------|-----------------------------------|\n",
       "| **Exact‑match memorisation** | *Dynamic item generation*: each evaluation run draws fresh random lexical items from a large curated pool (≥ 10 000 nouns, 5 000 verbs, etc.). The prompt is assembled on‑the‑fly; the exact string never appears in training data. |\n",
       "| **Few‑shot prompt injection** | Disallow any demonstration examples in the test prompt. The test harness supplies *only* the task description and the input. If a model internally “hallucinates” a few‑shot context, it will be penalised (no credit for self‑generated examples). |\n",
       "| **Self‑referential “I know the answer”** | Post‑hoc regex checks that the response does **not** contain the original prompt verbatim (or a near‑duplicate). Any such copy is flagged and the item is re‑issued. |\n",
       "| **Exploiting temperature / stochasticity** | Run **three independent generations** per item (different seeds) and aggregate via majority vote. High variance across runs is recorded as a *stability* metric; models that rely on randomness to “guess” get lower stability scores. |\n",
       "| **Prompt‑engineering to force chain‑of‑thought** | The evaluation harness **strips** any model‑generated “let me think step‑by‑step” text before scoring; only the final answer is considered. Optionally, a separate “chain‑of‑thought” sub‑task can be added to reward explicit reasoning, but it is **scored separately** from the core compositional test. |\n",
       "| **Data‑leak via test‑set publication** | Keep the **item seed list** secret. Publish only the *generation code* (open‑source) but not the random seeds used for a particular evaluation round. After a round is finished, the seed list can be released for reproducibility, but not before the run. |\n",
       "| **Model‑specific tokenisation tricks** | Normalise all outputs using Unicode NFKC, lower‑casing, and whitespace collapsing before scoring. This removes superficial token‑level tricks. |\n",
       "| **Batch‑size / context‑window cheating** | Enforce a strict **single‑turn** interaction: the model receives only the current prompt, no history. The harness discards any hidden system‑prompt that the model might have inserted (e.g., via system‑prompt injection). |\n",
       "\n",
       "**Statistical Controls**\n",
       "\n",
       "* For each task generate **N = 200** items per evaluation round (see Section 4).  \n",
       "* Randomly split the 200 items into **10 folds**; compute per‑fold scores to estimate variance and detect outliers (e.g., a fold where many items are accidentally repeated).  \n",
       "* Use **bootstrapped 95 % confidence intervals** on the mean item score to verify that the observed performance is not due to chance.\n",
       "\n",
       "---\n",
       "\n",
       "## 4. Sample Sizes & Statistical Tests\n",
       "\n",
       "### 4.1 Power analysis (binary/partial scores)\n",
       "\n",
       "Assume we want to detect a **Δ = 0.15** absolute improvement in mean item score (e.g., 0.70 vs 0.55) with **α = 0.05** and **power = 0.90**.\n",
       "\n",
       "* For a two‑sample **t‑test** on proportions (or on continuous scores 0‑1), the required per‑group sample size is roughly **n ≈ 140** items per model per task.  \n",
       "* To be conservative and to allow per‑task breakdowns, we use **n = 200** items per task (≈ 1 200 total items across the six tasks).  \n",
       "\n",
       "### 4.2 Hypothesis tests\n",
       "\n",
       "| Comparison | Test | Rationale |\n",
       "|------------|------|-----------|\n",
       "| **Model A vs Model B** overall compositional ability | Two‑sample **Welch’s t‑test** on the aggregated item‑score vector (6 × 200 = 1 200 observations) | Handles unequal variances; works for continuous 0‑1 scores. |\n",
       "| **Task‑specific advantage** (e.g., Model A better on PCS) | **McNemar’s test** on binary‑correct vs incorrect per‑item (paired because same items are given to both models) | Controls for item difficulty. |\n",
       "| **Stability across runs** (low variance) | **Levene’s test** on per‑item variance across the three generation seeds | Detects heteroscedasticity. |\n",
       "| **Effect of randomisation** (ensuring no memorisation) | **Permutation test**: shuffle model labels on the item‑score vector 10 000 times, compute empirical p‑value. | Non‑parametric safeguard. |\n",
       "\n",
       "**Effect‑size reporting** – alongside p‑values, always report **Cohen’s d** (or Hedges g for unequal N) and **95 % CI**.\n",
       "\n",
       "---\n",
       "\n",
       "## 5. Possible Adversarial Behaviours & How the Suite Handles Them\n",
       "\n",
       "| Adversarial behaviour | Why it could fool a naïve test | Mitigation in our suite |\n",
       "|-----------------------|--------------------------------|--------------------------|\n",
       "| **“Lookup‑table” memorisation** (store every possible SCG transformation) | If the lexical pool were tiny, the model could pre‑compute all combos. | Use a **large, randomly sampled lexical pool** (≥ 10 000 items) and **different random seeds** each run → combinatorial explosion makes full coverage impossible. |\n",
       "| **Prompt injection of chain‑of‑thought** (model internally asks for examples) | Could produce step‑by‑step reasoning that looks correct without true compositional ability. | Score only the **final answer**; optional chain‑of‑thought credit is a separate metric. |\n",
       "| **Self‑prompted few‑shot creation** (model fabricates a training example then uses it) | May artificially raise accuracy on tasks like PCS. | Run **three independent generations** and require **majority agreement**; fabricated examples rarely survive across seeds. |\n",
       "| **Exploiting test‑set leaks** (model was fine‑tuned on a released seed list) | Could produce perfect scores. | Keep the **seed list secret** until after the evaluation round; publish only generation code. |\n",
       "| **Using external tools (search, calculators)** via tool‑use APIs | Might compute ASM numerics perfectly. | For the pure‑LLM benchmark, **disable tool‑use**; run the model in a “no‑tools” sandbox. A separate “augmented‑LLM” track can be created where tool‑use is allowed, but the baseline must be tool‑free. |\n",
       "| **Biasing temperature to output “I don’t know”** (to avoid penalty) | Could inflate partial‑credit by abstaining. | **Zero‑score** for any non‑answer (e.g., “I don’t know”, empty response). Also enforce a **minimum length** (≥ 1 token) to prevent empty output. |\n",
       "| **Manipulating tokenisation to hide correct answer** (e.g., splitting “yes” into sub‑tokens) | Might bypass regex checks. | Normalise output using Unicode NFKC and **token‑agnostic string matching**; also run a **semantic classifier** (tiny NLI) to double‑check “yes/no” answers. |\n",
       "\n",
       "---\n",
       "\n",
       "## 6. Experimental Validation Plan  \n",
       "\n",
       "### 6.1 Phases\n",
       "\n",
       "| Phase | Goal | Procedure | Success criteria |\n",
       "|-------|------|-----------|-------------------|\n",
       "| **Pilot** | Verify that item generation, parsing and scoring pipelines work reliably. | Run a **small LLM (e.g., GPT‑2‑XL)** on 30 items per task. Manually audit 10 % of outputs for parsing errors. | < 5 % parsing failures; clear separation of correct/incorrect. |\n",
       "| **Baseline Establishment** | Obtain performance distribution of *known pattern‑matching models*. | Evaluate 5 publicly available “large‑mem‑only” models (e.g., GPT‑2‑large, LLaMA‑7B, Falcon‑7B) on the full suite (200 × 6 items). | Mean score ≤ 0.55 (i.e., below chance for tasks requiring composition). |\n",
       "| **Target Model Evaluation** | Test a candidate *compositional* model (e.g., a fine‑tuned T5‑XXL or a transformer with explicit modular architecture). | Same protocol as baseline, three independent runs per item. | Mean score ≥ 0.75 and statistically significantly higher than baselines (p < 0.01, d ≥ 0.8). |\n",
       "| **Adversarial Stress Test** | Verify robustness to gaming strategies. | For each model, run an extra batch where the **temperature** is set to 0.9, and where we *prepend* a “few‑shot” instruction (e.g., “Answer step‑by‑step”). | Scores should not improve > 0.03 relative to the standard run; variance should increase, indicating instability. |\n",
       "| **Generalisation Check** | Ensure the test is not over‑fitted to a particular set of lexical items. | Regenerate the entire suite with a **new random seed** (different nouns/verbs) and re‑run the top‑performing model. | Score drop ≤ 0.05, confirming true compositional ability. |\n",
       "\n",
       "### 6.2 Metrics for False Positives / Negatives\n",
       "\n",
       "| Metric | Definition | How it is measured |\n",
       "|--------|------------|--------------------|\n",
       "| **False Positive Rate (FPR)** – proportion of pattern‑matching models that achieve “high” compositional score (≥ 0.70). | Count models meeting threshold / total pattern‑matching models. | Desired FPR < 0.05. |\n",
       "| **False Negative Rate (FNR)** – proportion of genuinely compositional models (e.g., models with known modular architectures) that fall below the threshold. | Count such models below threshold / total compositional models. | Desired FNR < 0.10. |\n",
       "| **Stability Index** – average pairwise disagreement across the three seeds per item. | `1 - (agreement_rate)`. | Low instability (< 0.07) signals reliable reasoning rather than random guessing. |\n",
       "| **Item Discrimination (I‑D)** – point‑biserial correlation between item score and overall model score. | Compute per‑item correlation; discard items with I‑D < 0.1. | Ensures each item differentiates models. |\n",
       "\n",
       "### 6.3 Iteration Loop\n",
       "\n",
       "1. **Collect results** → compute all metrics.  \n",
       "2. **Identify weak items** (low discrimination, high variance, high memorisation suspicion).  \n",
       "3. **Modify**: change lexical pool, increase constraint complexity, or add a small “distractor” clause.  \n",
       "4. **Regenerate** the affected items and re‑run the pilot.  \n",
       "5. **Repeat** until the suite shows **≥ 0.80** average discrimination and **≤ 0.05** FPR on a held‑out set of pattern‑matching models.\n",
       "\n",
       "---\n",
       "\n",
       "## 7. Summary Checklist (what to implement)\n",
       "\n",
       "| Component | Implementation |\n",
       "|-----------|----------------|\n",
       "| **Item generator** (Python script) – random noun/verb pools, template filling, seed‑control. |\n",
       "| **Scorer** – regex‑based normalisers, task‑specific validators, majority‑vote aggregator. |\n",
       "| **Evaluation harness** – API wrapper that forces single‑turn interaction, three‑seed repeats, temperature logging. |\n",
       "| **Statistical analysis notebook** – computes t‑tests, effect sizes, bootstrapped CIs, item discrimination. |\n",
       "| **Adversarial‑behavior detector** – copy‑detection, output‑length filter, variance logger. |\n",
       "| **Documentation** – protocol for secret seed handling, reproducibility instructions, reporting template (mean ± CI, d, FPR/FNR). |\n",
       "\n",
       "With these six tasks, a rigorous scoring rubric, anti‑gaming safeguards, statistically powered sample sizes, and a clear validation pipeline, the suite can **reliably differentiate** between LLMs that merely stitch together memorised patterns and those that truly **compose, reason and generalise**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Updated with the latest Open Source model from OpenAI\n",
    "\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"openai/gpt-oss-120b\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Below is a comprehensive design of an adversarial-resistant test-suite to differentiate black-box large language models (LLMs) that rely primarily on pattern matching and memorization from those that demonstrate genuine compositional reasoning and robust generalization.\n",
       "\n",
       "---\n",
       "\n",
       "# 1. Task Types with Examples\n",
       "\n",
       "Each task type probes a core aspect of compositional reasoning, using concrete example inputs and expected outputs.\n",
       "\n",
       "---\n",
       "\n",
       "### Task Type 1: **Systematic Generalization**  \n",
       "**Goal:** Test ability to apply learned rules or compositions to novel combinations never seen during training.\n",
       "\n",
       "**Example:**  \n",
       "- **Prompt:**  \n",
       "  \"In a made-up language, the suffix '-lam' means plural and the prefix 'bri-' means 'red'.  \n",
       "  What is the plural form of ‘bri-tak’?\"  \n",
       "- **Expected Answer:**  \n",
       "  \"bri-tak-lam\"\n",
       "\n",
       "**Explanation:** The model must systematically combine a novel prefix and suffix on a base word to form a correct compositional output.\n",
       "\n",
       "---\n",
       "\n",
       "### Task Type 2: **Causal Reasoning**  \n",
       "**Goal:** Assess model's ability to infer cause-effect relationships that require multi-step reasoning beyond pattern matching.\n",
       "\n",
       "**Example:**  \n",
       "- **Prompt:**  \n",
       "  \"If the street is wet and it did not rain, what is a plausible cause?\"  \n",
       "- **Expected Answers (any one):**  \n",
       "  \"Someone sprayed water,\" OR \"A pipe burst,\" OR \"The sprinkler system was on.\"\n",
       "\n",
       "---\n",
       "\n",
       "### Task Type 3: **Abstraction and Variable Binding**  \n",
       "**Goal:** Check if the model can manipulate variables and abstract concepts rather than fixed phrases.\n",
       "\n",
       "**Example:**  \n",
       "- **Prompt:**  \n",
       "  \"There are three boxes: A, B, and C. If box A is heavier than box B, and box B is heavier than box C, which box is lightest?\"  \n",
       "- **Expected Answer:**  \n",
       "  \"Box C\"\n",
       "\n",
       "---\n",
       "\n",
       "### Task Type 4: **Counterfactual Reasoning**  \n",
       "**Goal:** Test reasoning about alternate realities and consequences.\n",
       "\n",
       "**Example:**  \n",
       "- **Prompt:**  \n",
       "  \"If humans could fly naturally, how would cities be designed differently?\"  \n",
       "- **Expected Answer (any reasonable answer):**  \n",
       "  \"Cities would have multi-level buildings designed for landing and takeoff, with less need for roads.\"\n",
       "\n",
       "---\n",
       "\n",
       "### Task Type 5: **Planning and Multi-step Reasoning**  \n",
       "**Goal:** Evaluate multi-step planning ability requiring chaining of actions.\n",
       "\n",
       "**Example:**  \n",
       "- **Prompt:**  \n",
       "  \"You want to bake a cake but forgot eggs. List the steps to bake the cake using a substitute.\"  \n",
       "- **Expected Answer:**  \n",
       "  A multi-step plan including identifying substitutes (like applesauce), mixing ingredients, baking, etc.\n",
       "\n",
       "---\n",
       "\n",
       "### Task Type 6: **Novel Compositional Logic Puzzles**  \n",
       "**Goal:** Test the capability to combine logic and compositional understanding on novel puzzles.\n",
       "\n",
       "**Example:**  \n",
       "- **Prompt:**  \n",
       "  \"In a tribe, all who wear hats can speak a secret language. If Joe can speak the secret language, does he wear a hat?\"  \n",
       "- **Expected Answer:**  \n",
       "  \"Yes, Joe wears a hat.\"\n",
       "\n",
       "---\n",
       "\n",
       "# 2. Scoring Rubrics\n",
       "\n",
       "| Task Type                  | Scoring Criteria                             | Points | Notes                                                  |\n",
       "|----------------------------|---------------------------------------------|--------|--------------------------------------------------------|\n",
       "| Systematic Generalization  | Exact compositional construction correctness | 0 or 1 | 1 if correct suffix/prefix placement; otherwise 0     |\n",
       "| Causal Reasoning           | Plausibility and causal correctness          | 0-2    | 2 for fully valid cause; 1 for partial; 0 otherwise    |\n",
       "| Abstraction/Variable Binding | Correct relational output                    | 0 or 1 | 1 if correct variable reference; 0 otherwise           |\n",
       "| Counterfactual Reasoning   | Reasonableness & coherence of counterfactual | 0-2    | 2 for insightful, plausible; 1 for partial; 0 for nonsense |\n",
       "| Planning                  | Completeness, logical order, and core elements | 0-3    | 3 full plan; 2 partial but logical; 1 minimal; 0 none  |\n",
       "| Logic Puzzles              | Logical correctness                           | 0 or 1 | 1 if logically correct inference; else 0               |\n",
       "\n",
       "- **Human raters** using rubric or automated semantic similarity with thresholds for partial credit.\n",
       "\n",
       "---\n",
       "\n",
       "# 3. Protocols Against Memorization & Gamings\n",
       "\n",
       "**(a) Variant Generation:**  \n",
       "- For each task, generate many variants by changing surface details while preserving reasoning demands, e.g., swap entity names, alter numbers, use paraphrases.  \n",
       "- Use programmatic generation or crowd-sourcing for variants, generating >100 unique examples per task type unseen in training data.\n",
       "\n",
       "**(b) Statistical Controls:**  \n",
       "- Include “catch” memorized-items known from common benchmarks and obscured by paraphrase to detect rote memorization.  \n",
       "- Randomly mix in distractor prompts asking unrelated questions to detect answer pattern repetition.\n",
       "\n",
       "**(c) Cross-Prompt Paraphrasing:**  \n",
       "- Use multiple prompt wordings per test instance to rule out prompt exercise exploitation.\n",
       "\n",
       "**(d) Zero-shot and Few-shot Tests:**  \n",
       "- Test variants with no or minimal in-context examples to check for reasoning independent of prompt engineering.\n",
       "\n",
       "---\n",
       "\n",
       "# 4. Sample Sizes and Statistical Tests\n",
       "\n",
       "- **Sample size:**  \n",
       "  - At least 100 unique test instances per task type to cover broad variant space.  \n",
       "  - Multiple prompt formulations per instance (3+), total ~300 prompts per task type.\n",
       "\n",
       "- **Statistical Test:**  \n",
       "  - Conduct statistical comparisons between models using nonparametric tests (e.g., Mann-Whitney U) on scored outputs to detect capability difference.  \n",
       "  - Compute Cohen’s d effect size to measure practical significance.\n",
       "\n",
       "- **Confidence:**  \n",
       "  - Power analysis to target ≥80% power detecting medium-large effect sizes (d=0.5) at α=0.05.\n",
       "\n",
       "---\n",
       "\n",
       "# 5. Addressing Adversarial Behaviors\n",
       "\n",
       "| Adversarial Strategy                  | Mitigation Approach                               |\n",
       "|-------------------------------------|--------------------------------------------------|\n",
       "| Memorizing benchmark outputs         | Variant generation, paraphrasing, catch items    |\n",
       "| Prompt-engineering to “game” test    | Multi-prompt design and zero-shot tests          |\n",
       "| Producing generic plausible answers  | Scoring by detailed correctness, not plausibility alone |\n",
       "| Copying large memorized paraphrases  | Novel, synthetic tasks with no prior examples    |\n",
       "| Pattern matching on keywords only    | Require multi-step reasoning and combinatorial answers |\n",
       "| Ignoring variable binding             | Task design enforcing explicit variable reference |\n",
       "\n",
       "---\n",
       "\n",
       "# 6. Experimental Validation Plan\n",
       "\n",
       "**Step 1:** Baseline testing on known models with expected capability gradients (e.g., smaller pattern-memorization-based vs. advanced reasoning-based LLMs).\n",
       "\n",
       "**Step 2:** Calculate:  \n",
       "- **True positives (TP):** Correctly identified reasoning-capable models  \n",
       "- **True negatives (TN):** Correctly identified pattern-matchers  \n",
       "- **False positives (FP):** Pattern-matchers wrongly classified as reasoning-capable  \n",
       "- **False negatives (FN):** Reasoning-capable models missed\n",
       "\n",
       "**Metrics:**  \n",
       "- Precision = TP / (TP + FP)  \n",
       "- Recall = TP / (TP + FN)  \n",
       "- F1 score = harmonic mean of precision and recall  \n",
       "- ROC-AUC (if applicable to continuous scoring)\n",
       "\n",
       "**Iteration:**  \n",
       "- Analyze failure cases for FP and FN to identify unsound prompts or scoring ambiguities.  \n",
       "- Refine and augment task variants or scoring rubric accordingly.  \n",
       "- Re-test and measure performance improvements.\n",
       "\n",
       "**Human-in-the-loop:**  \n",
       "- Incorporate expert review of ambiguous or borderline answers.  \n",
       "- Adjust rubric based on inter-rater consistency.\n",
       "\n",
       "---\n",
       "\n",
       "# Summary\n",
       "\n",
       "The multi-dimensional test suite combines diverse, adversarially-generated tasks probing essential cognitive faculties in LLMs beyond memorization. Systematic statistical evaluation and iterative refinement ensure reliable distinction of genuine compositional reasoning capabilities from superficial pattern matching.\n",
       "\n",
       "If you want, I can also provide sample code for generating prompt variants or scoring templates. Let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The API we know well\n",
    "# I've updated this with the latest model, but it can take some time because it likes to think!\n",
    "# Replace the model with gpt-4.1-mini if you'd prefer not to wait 1-2 mins\n",
    "\n",
    "model_name = \"gpt-4.1-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Super important - ignore me at your peril!</h2>\n",
    "            <span style=\"color:#ff7800;\">The model called <b>llama3.3</b> is FAR too large for home computers - it's not intended for personal computing and will consume all your resources! Stick with the nicely sized <b>llama3.2</b> or <b>llama3.2:1b</b> and if you want larger, try llama3.1 or smaller variants of Qwen, Gemma, Phi or DeepSeek. See the <A href=\"https://ollama.com/models\">the Ollama models page</a> for a full list of models and sizes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Designing a comprehensive test-suite to evaluate black-box LLMs requires careful consideration of various task types, scoring rubrics, and protocols to prevent memorized answers and prompt-engineering gaming. Here's a proposed design for such a test-suite:\n",
       "\n",
       "**Task Types (6)**\n",
       "\n",
       "1. **Classification of Analogies**: Given an analogy statement like \"cats have whiskers,\" what category does [animal] belong to?\n",
       "\t* Example input: `[cat, whisker]`, `class: cat`\n",
       "\t* Expected output: `[animal], class: feline`\n",
       "2. **Systematic Generalization**: Predict the result of a generalized version of a rule (e.g., \"if A > B, then C = D\").\n",
       "\t* Example input: \"[mathematical expression]\", \"if A > 3 and C > 1\"\n",
       "\t* Expected output:=\"[ mathematical expression substitution with A > C and new_value ]\"\n",
       "3. **Causal Reasoning**: Given a sequence of events, predict the next event (e.g., \"what did John do after meeting Alice?\").\n",
       "\t* Example input: `[John , met Alice]`\n",
       "\t* Expected output: `\"he talked to Alice\"`\n",
       "4. **Abstraction**: Identify the underlying principle behind a concept or idea.\n",
       "\t* Example input: \"[human emotion with synonyms]\"\n",
       "\t* Expected output: \"concept/emotion underlying it, e.g., [happiness]\"\n",
       "\n",
       "5. **Counterfactuals**: Predict an alternative outcome given a hypothetical scenario (e.g., what would have happened if John had not met Alice?).\n",
       "\t* Example input: `[ scenario ], variable John , predicted_action, consequence ]\n",
       "\t * Expected answer : `[variable John prediction without consequences to that particular situation ]`\n",
       "6. **Planning with Limited Information**: Plan a sequence of actions given incomplete or uncertain information (e.g., plan the best way for John to get from one city to another knowing only route lengths).\n",
       "    Example input:   `[starting location], ending location, routes information and any other data related ]` \n",
       "Expected output: `\"steps taken by starting location , going through various other relevant destinations, destination of final desired location]`\n",
       "\n",
       "**Scoring Rubric**\n",
       "\n",
       "For each task, a clear scoring rubric will be developed to assess the LLM's performance. The rubrics will consider factors such as:\n",
       "\n",
       "* Accuracy\n",
       "* Completeness\n",
       "* Novelty (correctness of responses that don't correspond to expected solutions)\n",
       "* Coherence\n",
       "\n",
       "The scores for each answer will be calculated using statistical measures such as mean and standard deviation.\n",
       "\n",
       "**Protocols to Prevent/ Detect Memorized Answers and Prompt-Engineering Gaming**\n",
       "\n",
       "1.  **Exhaustive variation generation**: to cover unseen variants, multiple permutations of possible correct answers or solution paths \n",
       "2.  **Statistical analysis with variance control**: Implement a data analysis framework considering factors like frequency , consistency and patterns that may indicate cheating\n",
       "3.  **Input normalization and randomization** : use randomized test examples for each task type so output comparisons between the testing scenarios are fair compared to when given same but different input, prompts.\n",
       "4.  **Adversarial testing using carefully crafted inputs**: Develop a set of artificially created input variants that exploit the LLM's pattern-matching vulnerabilities while maintaining coherence and relevance.\n",
       "\n",
       "**Sample Sizes and Statistical Tests**\n",
       "\n",
       "To establish confidence in detecting a difference in capabilities between pattern-matching-like and genuinely compositional reasoning LLMs:\n",
       "\n",
       "*   **Large sample sizes:** Evaluate at least 100-500 tests for each task type, ensuring sufficient statistical power to detect even small differences.\n",
       "*   **Permutation tests or bootstrap samples:** Use techniques like permutation distributions or resampling to assess the probability of observing differences by chance.\n",
       "\n",
       "**Adversarial Behaviors**\n",
       "\n",
       "Potential adversarial behaviors include:\n",
       "\n",
       "1.  **Pattern-filling**: The LLM may fill in patterns it has learned from training data, even when the task requires genuine reasoning.\n",
       "2.  **Overfitting to specific prompts**: The AI model might perform well on a particular input but struggles with novel or similar prompts.\n",
       "3.  **Information-seeking behavior**: The model could be designed to gather additional information that isn't present in the prompt.\n",
       "\n",
       "**Designing Against these adversarial behaviors:**\n",
       "\n",
       "1.   **Introducing 'distractor' subtasks:** Adding unrelated smaller questions between two question that could potentially confuse AI output by distracting patterns detection capabilities.\n",
       "2.  **Adding 'unanswerable' responses**: To ensure the model will sometimes fail to answer due to lack of information or failure to recognize uncertainty, and if this fails to happen it would be indicative a pattern matching solution is on work.\n",
       "3.     **Randomizing some answers as correct/ incorrect examples**:   This way you can detect AI trying to infer things its not supposed to.\n",
       "\n",
       "**Testing Methodology Experimental Validation:**\n",
       "\n",
       "A series of experiments including large test sets for each task type in this set, using a combination of human evaluators (evaluance the output) and automated testing protocols (checks if LLM outputs are correct or incorrect  )."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Adversarial-Resistant Test Suite for Evaluating LLM Reasoning Capabilities\n",
       "\n",
       "This design outlines a comprehensive test suite aimed at distinguishing between LLMs relying on pattern matching/memorization and those exhibiting genuine reasoning. The suite focuses on robustness through diverse tasks, strict controls, and statistical analysis.\n",
       "\n",
       "**1. Task Types & Examples:**\n",
       "\n",
       "Here are six task types designed to probe different aspects of reasoning:\n",
       "\n",
       "| Task Type        | Description                               | Example Input                                                              | Expected Output (demonstrating reasoning)                                                                                                        |\n",
       "|------------------|------------------------------------------|---------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|\n",
       "| **1. Systematic Generalization (SGM)** | Extending patterns with quantitative rules. | \"If a car travels at 60 mph for 2 hours, how far does it travel? If a train travels at 80 mph for 3.5 hours, how far does it travel?\" | \"60 mph * 2 hours = 120 miles. 80 mph * 3.5 hours = 280 miles.\" (Correct application of distance = speed * time) |\n",
       "| **2. Causal Reasoning (CR)** | Predicting consequences of actions.      | \"John spilled water on the table. What's likely to happen?\"                | \"The table will likely become wet. The floor may become wet if the water isn't cleaned up.\"                                                   |\n",
       "| **3. Abstraction (AB)** |  Conceptual mapping and reformulation.   | “A robin is a bird. A sparrow is a bird. Which statement is most similar: ‘A robin is a songbird’ or ‘A sparrow is a songbird’?” | “‘A robin is a songbird’.” (Recognizing the abstraction of 'songbird' which applies to both robin and sparrow) |\n",
       "| **4. Counterfactuals (CF)** | Considering alternative scenarios.        | “If the sky were green, what color would the grass be?”                      | “The grass would be its normal color (green).” (Requires understanding of natural laws and relationships – grass is green independently of sky color) |\n",
       "| **5. Planning (PL)** |  Sequential reasoning with a goal.      | “I want to bake a cake. List the steps you would take.”                     | “1. Preheat the oven. 2. Gather ingredients (flour, sugar, eggs, etc.). 3. Mix the ingredients. 4. Pour batter into a pan. 5. Bake for [duration].”                     |\n",
       "| **6. Variable Binding (VB)** | Handling variables and relationships. | “If I have 3 apples and give 1 away, how many do I have?” ; “If x = 5 and y = 2, what is x + y?” | “2 apples.” ; “7” (Correct use of arithmetic and/or symbolic manipulation)                                                                              |\n",
       "\n",
       "\n",
       "**2. Scoring Rubric:**\n",
       "\n",
       "Each task will use a three-point rubric:\n",
       "\n",
       "* **3 Points:**  The response is entirely correct and demonstrates a clear understanding of the underlying reasoning.\n",
       "* **1 Point:** The response demonstrates a partial understanding or contains a minor error, but the core logic is present.\n",
       "* **0 Points:** The response is incorrect, nonsensical, or completely unrelated to the input.\n",
       "\n",
       "**3. Controls & Prevention Protocols:**\n",
       "\n",
       "* **Unseen Variant Generation:** Automatically generate variations of input prompts (synonyms, rephrasing, slightly altered contexts) to avoid memorized answers. Utilise paraphrasing models specifically trained for this purpose.\n",
       "* **Statistical Controls:**\n",
       "    * **Temperature Scaling:** Vary the LLM’s temperature setting (influencing randomness) across different runs. This adds a degree of noise and reduces reliance on memorized templates.\n",
       "    * **Prompt Length Limits:** Strict limits on prompt length to discourage overly verbose, memorization-driven responses.\n",
       "    * **Response Length Limits:** Similarly, set limits to discourage unnecessary elaboration.\n",
       "* **“Un-promptable” Data Injection:** Introduce subtle, non-obvious constraints into the input – e.g., “The answer must be a prime number,” or “The answer cannot be a multiple of 5.”\n",
       "* **Chain-of-Thought Debugging:** Integrate a Chain-of-Thought (CoT) model (e.g., a smaller, more reliable model) to automatically analyze the LLM’s reasoning chain and flag potential issues.\n",
       "* **Self-Reflection:**  Prompt the LLM to critically assess its own response – “Is your answer logically sound?  Explain your reasoning in detail.” This can identify where memorization is masking incorrect understanding.\n",
       "\n",
       "\n",
       "\n",
       "**4. Sample Sizes & Statistical Tests:**\n",
       "\n",
       "* **Sample Size:**  Run each task at least 100 times for each LLM being evaluated.  Larger sample sizes (200-300) would improve statistical power.\n",
       "* **Statistical Tests:**\n",
       "    * **Chi-Square Test:** Compare the distribution of 3, 1, and 0 scores across the different LLMs.  A significant Chi-Square value indicates a difference in performance.\n",
       "    * **Wilcoxon Signed-Rank Test:**  If the distributions are not normally distributed, use the Wilcoxon test for comparing paired samples (e.g., comparing the average score of LLM A to the average score of LLM B on the same task).\n",
       "    * **Cohen's d:**  Provides an effect size measure of the difference between means.\n",
       "\n",
       "**5. Adversarial Behaviors & Mitigation:**\n",
       "\n",
       "| Adversarial Behavior           | Mitigation Strategy                                              |\n",
       "|-------------------------------|------------------------------------------------------------------|\n",
       "| **Rule-Following Template Replication** | Frequent variant generation, statistical temperature scaling. |\n",
       "| **Obfuscated Reasoning**        | Chain-of-Thought debugging, “Self-Reflection” prompts.         |\n",
       "| **Overly Detailed, but Incorrect Responses** | Response length limits, constrained input, outlier detection. |\n",
       "| **Contextual Framing as Knowledge** | Unseen variant generation, tests specifically designed to expose this. |\n",
       "\n",
       "\n",
       "\n",
       "**6. Experimental Validation Plan:**\n",
       "\n",
       "* **Phase 1: Baseline Evaluation:** Conduct preliminary runs to identify prompt engineering vulnerabilities and refine the test suite.\n",
       "* **Phase 2: Focused Testing:** Execute the full test suite across multiple LLMs (e.g., GPT-4, Claude, open-source models).\n",
       "* **Phase 3: Iteration & Refinement:**  Analyze results from Phase 2 to identify where the suite needs improvement. Specifically, add tasks targeted at the weaknesses identified. Generate new unseen variants.\n",
       "* **Metrics for False Positives/Negatives:** Track the percentage of times an LLM receives a 3 (correct) or 0 (incorrect) score. Analyze which task types lead to the highest error rates.\n",
       "* **Continuous Monitoring:** Monitor the LLM’s responses over time to detect shifts in performance and potential gaming attempts.\n",
       "\n",
       "\n",
       "\n",
       "**Key Considerations & Future Work:**\n",
       "\n",
       "* **Ground Truth Data:** High-quality, expertly annotated ground truth data is crucial for accurate scoring.\n",
       "* **Model Architectures:**  The suite’s effectiveness will be heavily influenced by the underlying LLM architectures being evaluated.\n",
       "* **Dynamic Testing:**  Implement a system to automatically adapt the test suite based on LLM performance – focusing more on areas where the LLM is struggling.\n",
       "\n",
       "This detailed design provides a robust framework for evaluating LLM reasoning capabilities. By combining diverse tasks, stringent controls, and rigorous statistical analysis, we can gain valuable insights into the true level of understanding exhibited by these powerful models.  The iterative validation plan ensures the test suite remains relevant and effective as LLMs continue to evolve."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"gemma3:4b\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Adversarial-Resistant Test-Suite for LLM Reasoning Capabilities\n",
       "\n",
       "This design focuses on distinguishing between \"pattern-matching\" LLMs and those exhibiting genuine compositional reasoning, emphasizing robustness and adversarial resistance.\n",
       "\n",
       "**I. Task Types & Examples (Probing Compositional Reasoning)**\n",
       "\n",
       "Here's six task types, each targeting a specific aspect of reasoning, with example inputs and expected outputs:\n",
       "\n",
       "**1. Systematic Generalization (SG):** Tests ability to generalize patterns beyond observed instances.\n",
       "    * **Input:** \"A blue block sits on a red block. A green block sits on a blue block. What happens when a yellow block sits on a green block?\"\n",
       "    * **Expected Output:** \"A yellow block sits on a green block.\" (Demonstrates transfer of the pattern 'X sits on Y')\n",
       "    * **Variant:**  Changing block colors, shapes, and adding distraction statements.\n",
       "\n",
       "**2. Causal Reasoning (CR):**  Tests understanding of cause-and-effect.\n",
       "    * **Input:** \"Rain often makes the ground wet. The ground is wet. Could it be raining?\"\n",
       "    * **Expected Output:** \"It could be raining, but there might be other reasons (e.g., sprinklers).\"  (Shows understanding of correlation vs. causation)\n",
       "    * **Variant:** Introducing misleading information or complex causal chains.\n",
       "\n",
       "**3. Abstraction (AB):** Tests extracting underlying principles and applying them to novel contexts.\n",
       "    * **Input:**  Present a series of stories following a pattern (e.g., \"The baker made bread, and the village had food. The builder built houses, and the town had shelter.\").  Then ask: \"The musician played music, what does the village now have?\"\n",
       "    * **Expected Output:** \"The village now has joy/entertainment/art.\"  (Demonstrates abstracting the relationship between profession and societal benefit)\n",
       "    * **Variant:**  Varying professions, social structures, and complexity of the relationship.\n",
       "\n",
       "**4. Counterfactuals (CT):** Tests ability to reason about \"what if\" scenarios.\n",
       "    * **Input:** \"The cat chased the mouse. If the cat had been asleep, what would have happened?\"\n",
       "    * **Expected Output:** \"The mouse might have escaped/found food/taken a different path.\" (Demonstrates mental simulation)\n",
       "    * **Variant:**  Complex counterfactuals incorporating multiple variables and constraints.\n",
       "\n",
       "**5. Planning (PL):** Tests formulating and executing a sequence of actions to achieve a goal.\n",
       "    * **Input:** \"You need to cross a river with a fox, a chicken, and a sack of grain. You have a boat that can only carry you and one other thing. What should you do?\"\n",
       "    * **Expected Output:** (A logical sequence) \"1. Take the chicken across. 2. Return alone. 3. Take the fox across. 4. Bring the chicken back. 5. Take the grain across. 6. Return alone. 7. Take the chicken across.\"\n",
       "    * **Variant:**  Increasing the number of items and complexities of constraints.\n",
       "\n",
       "**6. Variable Binding (VB):**  Tests understanding and manipulation of variables in abstract statements.\n",
       "    * **Input:**  \"Consider objects A and B.  If A is larger than B, and B is larger than C, then is A larger than C?\"\n",
       "    * **Expected Output:** \"Yes.\" (Demonstrates transitivity and variable relationship understanding.)\n",
       "    * **Variant:**  Introducing complex relationships, inequalities, and quantifiers.\n",
       "\n",
       "\n",
       "\n",
       "**II. Scoring Rubric (Example: Systematic Generalization)**\n",
       "\n",
       "* **0 Points:** Completely incorrect or gibberish.\n",
       "* **1 Point:** Identifies the presence of something on top. (e.g., \"A block sits on another block\"). Partial understanding.\n",
       "* **2 Points:** Correctly identifies the block color. (e.g., \"A yellow block sits on a green block\"). Shows pattern recognition.\n",
       "* **3 Points:** Correctly states the new configuration and acknowledges the inferred pattern.  (e.g. \"A yellow block sits on a green block, because the pattern is that one block sits on another\"). Demonstrates generalization.\n",
       "\n",
       "Similar rubrics would be developed for each task, prioritizing *reasoning steps* and avoiding reward for mere superficial correctness.\n",
       "\n",
       "\n",
       "\n",
       "**III. Adversarial Mitigation & Detection**\n",
       "\n",
       "* **Prompt Engineering Resistance:**\n",
       "    * **Unseen Variants:** Automatically generate numerous variants of each input using synonym replacement, paraphrasing, and structural alterations (e.g., reordering sentences, adding/removing context).  This challenges memorization reliance.\n",
       "    * **Noise Injection:** Add noise to the input (typos, grammatical errors, irrelevant information) to test robustness to imperfections.\n",
       "    * **Negative Constraints:** Explicitly forbid common \"cheating\" techniques in instructions (e.g., \"Do not simply repeat information\" or \"Do not try to find a direct answer online\").\n",
       "* **Memorization Detection:**\n",
       "    * **Statistical Controls:** Monitor next-token probabilities.  Low entropy or predictable next-token distributions strongly suggest memorization.\n",
       "    * **External Search Correlation:** Examine likelihood of extracted phrases from LLM's response appearing verbatim within top search results for the input prompt. High correlation points to retrieved factoids.  Implement a \"blurring\" technique to remove exact matches.\n",
       "    * **Cross-Task Consistency:**  Evaluate consistency between performance across different tasks probing similar underlying skills (e.g., Causal Reasoning & Counterfactuals). Large inconsistencies could show task-specific memorization.\n",
       "\n",
       "\n",
       "\n",
       "**IV. Sample Size & Statistical Tests**\n",
       "\n",
       "* **Sample Size:** Begin with **N=200-300** prompts per task. This allows for reasonable power (estimated using pilot data). Increase if variance is high.\n",
       "* **Statistical Tests:**\n",
       "    * **t-tests/Mann-Whitney U-tests:** Compare mean scores of LLMs for each task.\n",
       "    * **ANOVA/Kruskal-Wallis tests:** Compare performance across multiple LLMs simultaneously.\n",
       "    * **Bayesian Hypothesis Testing:**  Provides probability estimates of LLMs exhibiting superior reasoning ability, accommodating prior beliefs about the task difficulty and model complexity.\n",
       "    * **Effect Size Calculation:** Cohen's d or similar metrics capture the practical significance of observed differences.\n",
       "\n",
       "\n",
       "\n",
       "**V. Potential Adversarial Behaviors & Mitigation**\n",
       "\n",
       "* **\"Hallucinating\" Justifications:** LLMs might produce superficially sound explanations while failing to reason correctly. (Mitigation: Rubric focuses on reasoning steps, not *only* output correctness.)\n",
       "* **\"Exploiting\" Prompts:** LLMs might find a narrow prompt structure yielding high scores, not demonstrating true generalization. (Mitigation: Extensive prompt variants & noise injection)\n",
       "* **\"Circumventing\" Instructions:**  LLMs might ignore negative constraints or semantic reinterpretation of the prompt. (Mitigation: Re-phrasing instruction and negative constraints frequently to avoid easy circumvention)\n",
       "\n",
       "**VI. Experimental Validation & Iteration**\n",
       "\n",
       "* **Human Baseline:** Establish a human baseline score (N=30) for each task.  LLMs must demonstrably surpass this baseline to be considered capable.\n",
       "* **False Positive/Negative Rates:**\n",
       "    * **False Positives:** LLMs achieving high scores despite lacking true reasoning skills. This necessitates rigorous rubric validation and adversarial testing.\n",
       "    * **False Negatives:** LLMs possessing genuine reasoning skills being incorrectly classified as pattern-matchers. This requires re-evaluation of task design and scoring.\n",
       "* **Iteration:**\n",
       "    1. **Analysis of Failure Cases:**  Deep dive into why individual LLMs failed on specific prompts. Categorize failures by type (memorization, misunderstandings, etc.).\n",
       "    2. **Task Enhancement:** Modify tasks to directly address weaknesses revealed in the analysis.\n",
       "    3. **Rubric Refinement:**  Adjust the scoring rubric to be more granular and discriminatory.\n",
       "    4. **Adversarial Prompt Expansion:** Generate new adversarial prompts specifically targeting identified vulnerabilities.\n",
       "\n",
       "\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "This test-suite design goes beyond simple accuracy comparisons. By focusing on compositional reasoning through targeted tasks, rigorous adversarial mitigation, and thorough validation, it aims to reliably differentiate true reasoning capabilities from superficial pattern-matching in LLMs, ultimately driving progress in AI safety and alignment.  This is an iterative process, constantly adapting to new LLM innovations and adversarial strategies."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"gemma3:12b\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-5-mini', 'claude-sonnet-4-5', 'gemini-3-flash-preview', 'deepseek-chat', 'openai/gpt-oss-120b', 'gpt-4.1-mini', 'llama3.2', 'gemma3:4b', 'gemma3:12b']\n",
      "['Below is a complete, practical test‑suite design for reliably distinguishing LLMs that primarily pattern‑match / memorize from LLMs that show genuine compositional reasoning and robust generalization. It is written to be implementable against black‑box LLMs (only I/O access). The suite mixes synthetic, randomized and structured tasks, uses statistical controls and adversarial checks, and includes validation and iteration plans.\\n\\nSummary of components\\n- (1) Six task types (with concrete examples and expected outputs) that probe different aspects of compositional reasoning.\\n- (2) A clear scoring rubric for each task.\\n- (3) Protocols to prevent/detect memorized answers and prompt‑engineering gaming (randomization, held‑out vocab, paraphrase invariance, adversarial decoys, statistical controls).\\n- (4) Sample sizes and statistical tests for confident capability differences.\\n- (5) Likely adversarial behaviors and mitigations.\\n- (6) Experimental validation plan with metrics and iteration strategy.\\n\\n1) Task types — descriptions, concrete inputs and expected outputs\\nEach task type includes rationale (what aspect it probes), a canonical task template, 2 example items and their expected outputs. All tasks are designed to support automatic grading by canonicalization or deterministic execution where feasible.\\n\\nImportant: In production tests, instantiate each template with many randomized variants (random tokens, symbol renamings, unseen combinations). Use synthetic vocabulary (random strings) in many items (see protocols below) to avoid memorization.\\n\\nTask A — Systematic generalization (composition of primitive instructions)\\n- What it probes: ability to apply known primitive operations in novel combinations, i.e., SCAN‑style compositionality / systematicity.\\n- Template: Define a small set of primitive actions and composition operators. Ask model to produce ground‑truth composed actions.\\n- Example primitives (in an item):\\n  \"Primitive actions: WALK (W) = move forward 1; JUMP (J) = move forward 2; TURN_LEFT (L) = rotate left. Composition: X and then Y means do X then Y; twice(X) means do X two times; opposite(X) means do TURN_LEFT twice then X then TURN_LEFT twice.\"\\n  Input 1: \"Instruction: opposite(twice(JUMP)) then WALK.\"\\n  Expected output 1: The sequence of primitive actions expanded: \"TURN_LEFT, TURN_LEFT, JUMP, JUMP, TURN_LEFT, TURN_LEFT, WALK\" (canonicalized as comma‑sep).\\n  Input 2 (novel composition): \"Instruction: twice(opposite(WALK)) then JUMP.\"\\n  Expected output 2: \"TURN_LEFT, TURN_LEFT, WALK, TURN_LEFT, TURN_LEFT, WALK, JUMP\"\\n- Why hard for memorization: create many primitives and composition rules and withhold many combinations from training set; test on novel nesting depths and permutations.\\n\\nTask B — Variable binding and long‑range reference (symbolic variables)\\n- What it probes: ability to bind variables to values, carry and reuse bindings across steps, disambiguate references like \"the one referenced two steps earlier\".\\n- Template: Provide assignments, transformations and queries referencing earlier variables (use randomized variable names).\\n- Example:\\n  Input 1:\\n    \"Let a = 7, b = a + 5, c = 2*b. Now update a = c - 3. What is b + a?\"\\n  Expected output 1: compute stepwise:\\n    b = a + 5 (using original a=7) => b=12\\n    c = 2*b => c=24\\n    a = c - 3 => a=21\\n    b + a = 12 + 21 = 33\\n    Output: \"33\"\\n  Input 2 (using randomly named vars, novel referencing):\\n    \"Let X1 = 4, Y_2 = X1 * 3, Z = Y_2 - X1. Then set X1 = Z + 1. What is Y_2 - X1?\"\\n  Expected output 2:\\n    Y_2 = 12, Z = 8, X1 becomes 9. Y_2 - X1 = 3. Output: \"3\"\\n- Why hard for memorization: use long sequences (10+ assignments) and random token names; require maintaining bindings across updates.\\n\\nTask C — Causal reasoning and counterfactuals (graph / structural causal model)\\n- What it probes: causal inference and do‑interventions, not mere correlation or associative retrieval.\\n- Template: Give a small causal graph or set of structural equations. Ask consequence of interventions or counterfactual statements.\\n- Example:\\n  Input 1:\\n    \"Variables: A -> B (B = A + noise), B -> C (C = 2*B). If we set A = 0 (do(A=0)), what happens to C? Provide final numeric relation.\"\\n  Expected output 1:\\n    After do(A=0): B = 0 (+noise mean 0) → C = 0. Output: \"C becomes 0 (given deterministic functions, C = 2*B, so C = 0).\"\\n  Input 2 (counterfactual):\\n    \"Structural model: X = 3*U, Y = X + V. Observed U=2, V=1 (so observed X=6, Y=7). Counterfactual: if U had been 1 (but V unchanged), what would Y have been?\"\\n  Expected output 2:\\n    New X = 3*1 = 3. Y = X + V = 3 + 1 = 4. Output: \"4\"\\n- Why hard for memorization: Use freshly generated graphs and numeric values, ask do‑intervention vs observation differences; require understanding intervention semantics.\\n\\nTask D — Abstraction and analogical mapping (learn mapping rule, apply to novel exemplars)\\n- What it probes: ability to induce abstract relations from examples and apply them to new tokens not seen in training.\\n- Template: Provide a few mapping examples using a tiny invented \"language\", then ask to map new items applying the same relation.\\n- Example:\\n  Input 1:\\n    \"Rule examples: hefo -> jopi, goro -> luma. Now apply the same transformation to zeta.\"\\n    (Transformation pattern: prepend \\'j\\' and substitute vowels mapping e->o, o->u — but hidden to the model; only examples show behavior.)\\n  Expected output 1:\\n    If pattern mapping produces j + (vowel shift) then zeta -> juta (example result). But to make deterministic and unambiguous, use explicitly definable transformations in generation pipeline so expected output can be computed.\\n  Input 2 (using symbols):\\n    \"Examples: blim -> ba-lim, srun -> sa-run. Apply to krup.\"\\n    Expected output 2: \"ka-krup\" (or whatever canonical transformation defined by the example set).\\n- Why hard for memorization: use arbitrary synthetic token spaces and withhold large portions of mapping space; require generalizing relational rule.\\n\\nTask E — Planning and hierarchical problem solving (constrained optimization)\\n- What it probes: ability to plan multi-step actions under constraints, to generalize plan construction rules to larger/new configurations.\\n- Template: Provide a grid or a pickup/delivery planning problem with constraints (capacity, order) and ask for a (near‑)optimal step sequence or cost.\\n- Example:\\n  Input 1:\\n    \"Agent at (0,0) on 3x3 grid. Boxes at (1,0) (A), (2,2) (B). Deliver A then B to goal (0,2). Agent can carry 1 box. Moves: Up/Down/Left/Right cost 1; Pickup/Drop cost 0. Plan minimal steps.\"\\n  Expected output 1:\\n    One optimal plan, canonicalized: \"Right, Pickup, Up, Up, Drop, Right, Right, Pickup, Left, Left, Up, Drop\" — or a canonical minimal step count and sequence. Expected minimal length numeric also acceptable: \"Minimal steps = 10 (sequence: ...)\"\\n  Input 2 (scale-up generalization):\\n    Same rules but 5x5 grid and three boxes with constraint that order must be A then third then B. Ask for plan.\\n- Why hard for memorization: use random placements, require true planning, and scale tests to larger grids / more boxes than in training.\\n\\nTask F — Nested recursion / compositional evaluation (interpreted mini‑language)\\n- What it probes: ability to parse and evaluate expressions under user‑defined semantics, including nested composition and recursion, and to generalize to deeper nesting than seen in examples.\\n- Template: Define a tiny functional language (e.g., inc(x) = x+1, dbl(x) = 2*x, swap(a,b) returns pair reversed), then evaluate nested expressions.\\n- Example:\\n  Input 1:\\n    \"Definitions: inc(x) = x+1; dbl(x) = 2*x; compose(f,g)(x) = f(g(x)). Evaluate compose(inc, dbl)(3).\"\\n  Expected output 1:\\n    dbl(3)=6; inc(6)=7. Output: \"7\"\\n  Input 2 (deeper / novel nesting with random function names):\\n    Randomly name functions: f_z(x)=3*x, g_q(x)=x-2. Evaluate f_z(g_q(g_q(10))).\\n    Expected output 2:\\n    g_q(10)=8; g_q(8)=6; f_z(6)=18. Output: \"18\"\\n- Why hard for memorization: use randomized function names and nesting depths beyond training examples.\\n\\nCross‑task consistency probes (meta‑checks)\\n- After initial answer, rephrase problem, permute variable names or ask logically equivalent forms (alpha‑renaming). A reasoning model should give consistent answers; a memorizer or pattern matcher will often fail.\\n\\n2) Scoring rubrics\\nFor each task we use a combination of exact match (for deterministic outputs), graded structural equivalence, partial credit for intermediate step correctness, and consistency checks. All scoring should be automated where possible.\\n\\nGeneral guidelines\\n- Normalize outputs: strip punctuation, collapse whitespace, canonicalize commutative orders if problem allows, convert numbers to canonical numeric form.\\n- Where multiple correct sequences exist (planning), accept any plan that meets constraints and minimal (or near‑minimal) length. Validate by deterministic simulator.\\n- Require explanation optionally as corroboration — explanations are graded separately and used to detect shallow patterning (see later).\\n\\nDetailed rubrics per task\\n\\nTask A — Systematic generalization\\n- 0/1 exact correctness: full credit (1.0) if produced canonical expanded action sequence exactly matches ground truth.\\n- Partial credit 0.5 if sequence is correct up to reordering of independent commuting actions or contains only superficial tokenization differences.\\n- 0 if wrong action types or wrong ordering violating composition semantics.\\n- Bonus +0.2 if model also outputs a correct short reasoning trace (e.g., shows stepwise expansion).\\n\\nTask B — Variable binding\\n- Full credit (1.0) if final numeric (or symbolic) answer matches.\\n- Partial credit 0.5 if intermediate steps are internally inconsistent but final answer reachable by plausible alternative interpretation; give 0 if incorrect.\\n- Additionally, ask for an explanation of each assignment; give +0.1 if explanation justifies final answer.\\n\\nTask C — Causal reasoning / counterfactuals\\n- Full credit (1.0) for correct intervention answer plus correct reasoning (identifying difference between observation and do()).\\n- Partial credit 0.5 if numeric outcome correct but model incorrectly describes the causal/non‑causal distinction.\\n- 0 for wrong or inconsistent counterfactual.\\n\\nTask D — Abstraction / analogy\\n- Full credit (1.0) for correct mapped output on held‑out tokens.\\n- Partial credit 0.5 if pattern partially matched (e.g., one of two transformation components correct).\\n- Zero if incorrect mapping.\\n\\nTask E — Planning\\n- Full credit (1.0) if plan meets all constraints and is optimal (or within predetermined optimality gap, e.g., +0 steps for optimal or +1 allowance for near‑optimal).\\n- Partial credit 0.75 if plan valid but suboptimal within allowed slack; 0.4 if partially valid or violates minor constraints; 0 if plan invalid/unexecutable.\\n- Also score plan length and feasibility automatically using a simulator.\\n\\nTask F — Nested evaluation\\n- Full credit (1.0) if evaluated result correct.\\n- Partial credit 0.5 if calculation partially correct or correct for a different but plausible semantics.\\n- 0 if wrong.\\n\\nConsistency / adversarial checks (applies across tasks)\\n- Self‑consistency score: ask the same question twice with variable renaming and paraphrase. Award extra trust when answers are stable. If model flips answers > threshold (e.g., >10% of items), mark suspicious.\\n\\nAggregate scoring\\n- Report per‑task accuracy and an overall composite score weighted equally or by task importance.\\n- Also report error types: arithmetic errors, reference errors, plausibility but contradiction, inconsistent explanation, etc.\\n\\n3) Protocols to prevent/detect memorized answers and prompt‑engineering gaming\\nUse a multi‑layered approach: (A) avoid giving any canonical public benchmark examples in test items; (B) randomization and synthetic languages; (C) paraphrase invariance; (D) decoys and traps; (E) statistical controls and cross‑validation; (F) output verification via external simulators.\\n\\nA. Synthetic, randomized, and held‑out elements\\n- Random tokens and names: generate variable, function, and symbol names randomly (e.g., strings of 4–6 chars drawn from letters not composing common words). Example: X1 -> \"qerf\", function \"f_x\".\\n- Random numeric seeds: numbers, positions, and graphs sampled uniformly from ranges beyond typical training corpora.\\n- Use on‑the‑fly generated domain rules (e.g., transformation rules) so items are unlikely to appear in model training.\\n- For each template, reserve a held‑out set of compositions (combinations of primitives, deeper nestings) not revealed anywhere else. Test on these held‑outs.\\n\\nB. Paraphrase invariance & renaming checks\\n- For each item, probe with several paraphrases and alpha‑renamings (rename all symbols consistently). A reasoning model gives consistent answers; a pattern matcher that memorized specific surface forms will often fail on renamed forms.\\n- Examples:\\n  Item 1: original problem\\n  Item 1a: same problem with variable renaming\\n  Item 1b: logically equivalent restatement\\n  If answers disagree, down‑weight confidence.\\n\\nC. Held‑out composition splits (systematic generalization protocol)\\n- Use compositional splits analogous to SCAN: train/evaluate pairs are constructed so primitives are seen in training but some combinations are held out and used only for testing. Similarly for mapping rules and function compositions. This tests systematic generalization rather than memorization.\\n\\nD. Trap items and decoys\\n- Embed \"publicly memorizeable\" variants that look like standard benchmarks but with small modifications. A pattern‑matcher that only recognizes the public form will output the public memorized answer rather than adapt; detect by comparing to valid answer for the modified item.\\n- Adversarial decoys: ask for outputs in both canonical and unusual formats to catch prompt‑tuned models that only work in one style.\\n\\nE. Explain‑and‑verify protocol\\n- Request both an answer and a concise 1–3 step justification. Use an automated verifier to check whether the explanation logically entails the answer (e.g., compute intermediate values or check stated steps). Pattern matchers often hallucinate plausible justifications; the verifier will catch mismatches between claimed steps and actual output.\\n\\nF. Multiple independent samplings + self‑consistency\\n- Query the model multiple times with temperature sampling to observe variability. Pattern matching models tuned to be deterministic may produce the same memorized output; reasoning models may produce either stable or explainable variations. Use statistics of answer distribution to infer brittleness.\\n\\nG. Cross‑model and cross‑item statistical controls\\n- Randomly split items into multiple forms; estimate item difficulty using item‑response theory (IRT) to control for item variance when comparing models.\\n\\nH. Avoid prompting that guides the model to memorize the test pattern\\n- Randomize prompt templates; do not leak examples from the test set in prompts. Use standardized prompt skeletons with random syntactic surfaces.\\n\\nI. Time & leakage controls\\n- Run tests before and after major model updates; if possible, time‑stamp datasets and store seeds to detect leak. Rotate new item sets periodically.\\n\\n4) Sample sizes and statistical tests\\nGoal: confidently assert a difference in capability between two black‑box LLMs (Model A and Model B).\\n\\nDefinitions and assumptions:\\n- Treat each model\\'s response to an item as correct/incorrect (binary) for primary analysis. Use per‑task accuracy as primary metric; composite score as secondary.\\n- Tests are paired (same items evaluated by both models). Use paired tests (McNemar’s test or paired permutation) to leverage within‑item correlations.\\n\\nPower & sample size rules of thumb\\n- For a two‑proportion comparison (unpaired) with modest effect sizes (difference = 10–20 percentage points), recommended sample per model per task ≈ 300–500 items to get power ≈ 0.8 at alpha = 0.05. But because tests are paired, required number of distinct items is lower.\\n\\nPaired sample: approximate calculation\\n- If expecting Model A accuracy = 50% and Model B = 70% on a task (difference 20 ppt), and assuming moderate within‑item correlation, ~150–250 paired items suffices for 80% power (McNemar or paired permutation). If difference expected smaller (e.g., 10 ppt), need ~400–800 items.\\n- Conservative recommended sample sizes:\\n  - Per task: 400 items (distinct problems) sampled from randomized template variants.\\n  - Per composite suite (6 tasks): 6 * 400 = 2400 items total. You can distribute effort (e.g., 400 per high‑importance task, 200 per less critical).\\n\\nStatistical tests and controls\\n- Primary test: paired permutation test on per‑item difference (nonparametric, robust) or McNemar’s test for binary paired data.\\n- Secondary tests:\\n  - Mixed‑effects logistic regression (item as random effect, model as fixed effect) to control for item difficulty and estimate model effect across heterogeneous items.\\n  - Bootstrapped confidence intervals (resample items) for per‑task accuracy difference.\\n  - Multiple comparisons correction: Benjamini–Hochberg when testing multiple tasks.\\n- Report effect sizes (difference in proportions, odds ratio), 95% CI, and p‑values.\\n- Power analyses: compute required sample size for expected minimal detectable effect size BEFORE running full test; pilot with small N to estimate item variance.\\n\\nDecision thresholds\\n- Define thresholds for claiming superiority. Example: model A is better than B on task if:\\n  - Paired difference in accuracy > δ (e.g., δ = 0.10) AND\\n  - p < 0.01 after multiple comparisons correction AND\\n  - Effect robust to bootstrap and mixed‑effects regression controlling for item variance.\\n\\n5) Possible adversarial behaviors and countermeasures\\nBelow are adversarial strategies a model might use to appear capable, and corresponding mitigations in the suite.\\n\\nAdversarial: Surface retrieval / memorized sequence regurgitation\\n- Behavior: Model stores fixed outputs for common instructions; returns memorized outputs for superficially similar prompts.\\n- Mitigation: synthetic tokens, held‑out compositions, trap items that mimic public benchmarks with slight changes; paraphrase and renaming consistency checks; require correct computation on non‑natural synthetic data.\\n\\nAdversarial: Template matching / prompt engineering (exploiting fixed prompt formats)\\n- Behavior: Model trained to respond well to specific prompt templates; game tests by matching those templates exactly.\\n- Mitigation: randomize prompt phrasing and structure; test with out‑of‑template paraphrases; hide test instructions across several rewordings.\\n\\nAdversarial: Post‑hoc plausible explanations (hallucinated chain‑of‑thought)\\n- Behavior: Model produces a plausible‑looking explanation that does not correspond to internal computation.\\n- Mitigation: automatic verification of explanation steps; require computation that can be executed in a simulator and check consistency between explanation and final answer. Ask for explicit intermediate numeric values that can be validated.\\n\\nAdversarial: Strategic stochasticity (choose most socially pleasing answer)\\n- Behavior: Model outputs an answer that appears reasonable but isn\\'t derived by correct reasoning.\\n- Mitigation: use items with unique numeric/structural answers and built simulators to check exactness. Evaluate per‑answer consistency across multiple samples.\\n\\nAdversarial: Learning to memorize test suite by repeated exposure (test leakage)\\n- Behavior: Providers fine‑tune model on leaked items and then pass tests.\\n- Mitigation: maintain a large pool of test items, rotate, and ensure many items are generated on the fly. Keep some items only as one‑time use. Audit and track tests over time to detect sudden accuracy jumps.\\n\\nAdversarial: Exploiting world knowledge\\n- Behavior: Model uses web knowledge (e.g., named entities) instead of reasoning.\\n- Mitigation: use synthetic domains and names that aren\\'t in corpus; prefer abstract variables and gibberish tokens.\\n\\nAdversarial: Exploiting consistent wrong heuristics (e.g., always answer \"no\" to avoid errors)\\n- Behavior: Model learns a default safe answer that is occasionally correct by chance.\\n- Mitigation: measure baselines and chance rates and compare; include control items where a naive heuristic fails frequently.\\n\\n6) Experimental validation plan, metrics for false positives/negatives, and iteration\\nValidation phases\\n- Phase 0 — Unit tests: small sample (N=50 per task) to verify item generation, canonicalization, and grading logic.\\n- Phase 1 — Pilot: evaluate 3–5 diverse models (known baselines: simple pattern models, older LLM, and an advanced recent LLM) on a larger pilot (N=200 per task). Use pilot to estimate item difficulties and variance.\\n- Phase 2 — Full evaluation: run full suite (recommended N=400 per task) across models under test.\\n- Phase 3 — Adversarial robustness checks: invite models purposely trained to game tests; analyze failures and iterate.\\n\\nMetrics\\n- Primary: per‑task accuracy (binary), composite accuracy.\\n- Secondary: explanation correctness rate; self‑consistency rate (fraction of paraphrases with same answer); variability under sampling (entropy of answers).\\n- Discrimination metrics: Area Under ROC curve if defining continuous scoring; effect sizes and odds ratios.\\n- Error analysis metrics:\\n  - False positives (FP): model flagged as reasoning‑capable but actually relying on memorization. Estimate by hand‑inspecting suspicious items and adversarially designed holdouts. Also measure cases where model passes synthetic tasks but fails renamed/scrubbed variants.\\n  - False negatives (FN): model truly capable but fails tests (e.g., due to prompt format, minor ambiguity). Detect by analyzing cases where model yields correct reasoning traces but output form differs; retrain canonicalizer.\\n\\nEstimating FP/FN rates\\n- Use validation with ground‑truth labeled model types:\\n  - Create/collect models we know: (i) a trained memorization baseline (e.g., n‑gram or memorization‑augmented seq2seq), (ii) a symbolic reasoner (deterministic oracle), (iii) a hybrid model.\\n  - Run full suite and measure classification (declared as reasoning vs not) vs known ground truth. Compute sensitivity (TPR) and specificity (1−FPR). Tune decision thresholds to maintain acceptable FP (e.g., <5%) while maximizing sensitivity.\\n\\nIterative refinement based on results\\n- Analyze item‑level statistics: identify low discrimination items (very easy or very hard) using IRT or item discrimination index. Remove or rework items with poor discrimination.\\n- Increase adversarial item proportion if many models game the suite.\\n- Expand synthetic vocabulary and composition depth if memorization leakage is observed.\\n- Automate item generation pipelines to produce fresh held‑out sets periodically.\\n- Maintain an audit log of test runs and items to detect potential leakage and overfitting by model providers.\\n\\nExample iteration cycle\\n1. Pilot run → compute per‑task accuracies, item difficulty.\\n2. Identify items where memorizing baselines scored as well as strong reasoning oracle → mark for redesign (add randomization, deeper nesting, or rename tokens).\\n3. Add paraphrase/renaming tests for items showing potential prompt‑template exploitation.\\n4. Rebalance item pool to maintain discrimination and rerun.\\n\\nPractical considerations and recommended implementation checklist\\n- Implement automatic generators for each template with reproducible random seeds.\\n- Implement deterministic simulators/validators for each task (action sequence executor, arithmetic evaluator, planner checker) so grading is exact.\\n- For each item store (seed, tokens, canonical answer, distractors).\\n- Maintain multiple prompt forms and sample at random per query.\\n- Log all outputs, probabilistic sampling states (temperature), and timestamps.\\n- Use paired testing (same items across all models) to maximize power and fairness.\\n- Publicly release small sanitized subsets for community verification but keep main test pools private to reduce overfitting.\\n\\nConcluding guidance\\n- No single test proves genuine reasoning forever; use this suite as a robust, multi‑task battery that measures “behavioral evidence” of compositional reasoning and generalization.\\n- Emphasize cross‑task consistency, randomness/held‑out composition, and rigorous statistical testing.\\n- Make verification automated where possible (simulators) and include human review for ambiguous failures.\\n- Iterate the pool and keep fresh synthetic items to reduce memorization/leakage risk.\\n\\nIf you’d like, I can:\\n- Produce a reference implementation (Python) that generates randomized items for each template and includes canonicalizers and simulators for automated scoring.\\n- Compute precise sample size calculations for specific expected effect sizes you care about and provide a ready‑to‑run experimental script.', \"# Comprehensive Test Suite for Distinguishing Pattern Matching from Compositional Reasoning in LLMs\\n\\n## Executive Summary\\n\\nThis test suite employs six task categories designed to probe genuine compositional reasoning while resisting memorization and gaming. Each task incorporates systematic variation, procedural generation, and adversarial controls to ensure that success requires compositional understanding rather than pattern matching.\\n\\n---\\n\\n## 1. Task Types with Examples\\n\\n### Task 1: Systematic Compositional Generalization (Novel Recombinations)\\n\\n**Objective**: Test ability to combine known primitives in novel configurations never seen during training.\\n\\n**Design Principle**: Use a microworld with defined rules, then test combinations statistically unlikely to appear in training data.\\n\\n**Example Input**:\\n```\\nRules of the GLORP system:\\n- FLIM(x) reverses the order of elements in x\\n- DRAK(x, y) interleaves elements from x and y\\n- ZOOP(x) duplicates every second element in x\\n\\nTraining examples:\\nFLIM([A, B, C]) = [C, B, A]\\nDRAK([1, 2], [3, 4]) = [1, 3, 2, 4]\\nZOOP([P, Q, R]) = [P, Q, Q, R]\\n\\nTest question:\\nWhat is FLIM(DRAK(ZOOP([X, Y]), [M, N]))?\\n```\\n\\n**Expected Output**: \\n```\\nStep-by-step reasoning:\\n1. ZOOP([X, Y]) = [X, Y, Y]\\n2. DRAK([X, Y, Y], [M, N]) = [X, M, Y, N, Y]\\n3. FLIM([X, M, Y, N, Y]) = [Y, N, Y, M, X]\\n\\nAnswer: [Y, N, Y, M, X]\\n```\\n\\n**Scoring Rubric**:\\n- 4 points: Correct answer with valid step-by-step reasoning\\n- 3 points: Correct answer with minor reasoning errors\\n- 2 points: Incorrect answer but demonstrates understanding of composition\\n- 1 point: Partially correct intermediate steps\\n- 0 points: Incorrect with no valid reasoning\\n\\n**Variants for Anti-Memorization**:\\n- Generate 10,000 unique microworlds with different operation names, symbols, and rule sets\\n- Use procedural generation with random seeds\\n- Operation names drawn from pronounceable non-words (e.g., BLICKET, WUGGY, FEPS)\\n- Vary depth of composition (2-5 levels)\\n- Test both symbolic and numeric domains\\n\\n---\\n\\n### Task 2: Causal Reasoning Under Intervention (Counterfactual Inference)\\n\\n**Objective**: Distinguish correlation from causation and reason about interventions.\\n\\n**Design Principle**: Present causal graphs implicitly through scenarios, then test counterfactual reasoning that requires understanding causal structure.\\n\\n**Example Input**:\\n```\\nScenario: In the town of Millbrook, the following patterns have been observed over 10 years:\\n\\n- When the reservoir level is high, the water treatment plant runs at full capacity\\n- When the treatment plant runs at full capacity, downtown water pressure is strong\\n- When downtown water pressure is strong, the fountain in Central Park operates\\n- The reservoir level depends only on rainfall\\n- Rainfall also directly affects whether street cleaning happens (rain = no cleaning)\\n\\nHistorical data shows that on days when the fountain operates, streets are usually dirty.\\n\\nQuestion 1: The town installs a new pump that allows the fountain to operate regardless of downtown water pressure. After this intervention, will the streets be cleaner or dirtier on days when the fountain operates, compared to before?\\n\\nQuestion 2: Explain your reasoning using the causal structure.\\n```\\n\\n**Expected Output**:\\n```\\nAnswer: The streets will be CLEANER on fountain-operating days after the intervention.\\n\\nReasoning: \\nBefore intervention: Fountain operating → High reservoir → Recent rainfall → No street cleaning → Dirty streets\\n\\nThe correlation between fountain operation and dirty streets was due to a common cause (rainfall), not because the fountain caused dirty streets.\\n\\nAfter intervention: The fountain can operate without rainfall, breaking the spurious correlation. Fountain operation is now independent of rainfall, so there's no reason to expect systematically dirtier streets.\\n```\\n\\n**Scoring\", 'This test-suite, titled **CORE-Eval (Compositional & Operational Reasoning Evaluation)**, is designed to bypass the \"stochastic parrot\" effect by focusing on out-of-distribution (OOD) tasks that require the dynamic manipulation of novel variables and rules.\\n\\n---\\n\\n### 1. Concrete Task Types\\n\\n#### Task A: Recursive Nested State Tracking (Variable Binding)\\n*   **Concept:** Track the state of objects through recursive \"if-then\" swaps and containment changes.\\n*   **Input:** \"There are three boxes: Red, Blue, and Green. Red contains a \\'Glint\\'. Blue contains a \\'Spark\\'. Green is empty. Rule 1: If an object is moved to an empty box, its name reverses. Rule 2: If two boxes swap, their contents swap. Step 1: Swap Red and Blue. Step 2: Move contents of Blue to Green. Step 3: Rule 1 applies to Green. What is in Green?\"\\n*   **Expected Output:** \"tnilG\" (The \\'Glint\\' moved from Red to Blue in Step 1, then from Blue to Green in Step 2, triggering Rule 1).\\n\\n#### Task B: Counterfactual Physics Reasoning (Causal Reasoning)\\n*   **Concept:** Apply logical deductions in a world where one fundamental law of physics is altered.\\n*   **Input:** \"In this world, gravity acts as a repellent for liquids but a vacuum for solids. If I tip a glass of water upside down over a table, and there is a wooden block on that table, what happens to the water and the block?\"\\n*   **Expected Output:** The water moves upward (away from the center of mass/floor) and the block is pulled toward the ceiling (vacuum effect).\\n\\n#### Task C: The \"Zylophon\" Syntax (Systematic Generalization)\\n*   **Concept:** Learn a 3-rule pseudo-grammar and apply it to a 20-word sentence.\\n*   **Input:** \"Grammar: (1) Nouns end in \\'-ox\\'. (2) Verbs precede nouns. (3) Adjectives follow the noun they modify and must be repeated twice. Translate: \\'The fast cat chases a small mouse\\' into Zylophon.\"\\n*   **Expected Output:** \"Chases catox fast fast mouseox small small.\" (Requires consistent rule application over lexical substitution).\\n\\n#### Task D: Strategic Pathfinding with Dynamic Obstacles (Planning)\\n*   **Concept:** Solve a grid-based navigation task where the \"cost\" of movement changes based on the history of moves.\\n*   **Input:** \"Grid 4x4. Start (0,0), Goal (3,3). Moving East costs 1. Moving South costs 2. However, every time you move South, the cost of the next East move doubles. Provide the sequence of moves for the lowest cost.\"\\n*   **Expected Output:** A specific path (e.g., E, E, E, S, S, S) with a calculated total cost.\\n\\n#### Task E: Functional Abstraction (Abstraction)\\n*   **Concept:** Identify a latent function from input-output pairs and apply it to a complex, non-obvious case.\\n*   **Input:** \"f(apple) = 1, f(banana) = 3, f(kiwi) = 2. Rule: f(x) = count of vowels in word. Apply f to the result of f(\\'pomegranate\\') expressed as a word.\"\\n*   **Expected Output:** \"f(5) -> f(\\'five\\') -> 2.\"\\n\\n#### Task F: Adversarial Syllogisms (Logic vs. Pattern Matching)\\n*   **Concept:** Logical structures where the conclusion is factually false in the real world but logically valid within the prompt.\\n*   **Input:** \"All surgeons are cats. Some cats are astronauts. Therefore, are some surgeons astronauts? Explain the necessity of the conclusion.\"\\n*   **Expected Output:** No. (Standard logical fallacy: the \\'undistributed middle\\'). Memorization-based models often default to \"Yes\" because the semantic \"vibes\" of surgeons and astronauts are high-competence.\\n\\n---\\n\\n### 2. Scoring Rubric\\nEach task is scored on a 0-3 scale:\\n*   **0 (Failure):** Incorrect final answer and flawed logic or hallucinations.\\n*   **1 (Partial):** Correct logic but arithmetic/minor execution error; or correct answer but \"hollow\" explanation.\\n*   **2 (Successful):** Correct final answer and clear, step-by-step reasoning (Chain of Thought).\\n*   **3 (Robust):** Correct answer even when the prompt includes \"distractor\" information (e.g., \"Note: the boxes are made of wood,\" which is irrelevant).\\n\\n---\\n\\n### 3. Protocols to Prevent Memorization/Gaming\\n*   **The DSL Generator:** Use a Domain Specific Language (DSL) script to generate 1,000 unique variations of each task. This ensures no specific instance exists in the training data.\\n*   **Nonce-Token Substitution:** Replace all semantic nouns (e.g., \"cat\") with random strings (e.g., \"blorp-7\"). This strips the model of \"semantic priming\" where it guesses based on word associations rather than logic.\\n*   **Permutation Testing:** For every prompt, provide a version where the order of rules is changed. A reasoning model will adapt; a pattern-matcher will often fail if it memorized a specific sequence.\\n\\n---\\n\\n### 4. Sample Size and Statistical Tests\\n*   **Sample Size:** $N=200$ unique prompts per task type (Total 1,200).\\n*   **Metric:** Accuracy (binary) and Mean Score (0-3).\\n*   **Statistical Test:** \\n    *   **McNemar’s Test:** To compare two models on the same set of binary outcomes (to see if the performance difference is statistically significant).\\n    *   **Standard Deviation of Error:** If a model’s performance drops significantly when nouns are replaced with nonce-tokens, it is flagged as relying on **pattern matching** (High Semantic Sensitivity).\\n\\n---\\n\\n### 5. Adversarial Behaviors & Design Counter-Measures\\n*   **Behavior: Hedging/Refusal.** The model says \"This depends on the laws of physics\" to avoid commitment.\\n    *   *Counter-measure:* Explicitly state \"Assume this hypothetical world is the only reality for this calculation.\"\\n*   **Behavior: Chain-of-Thought Mimicry.** The model writes \"Step 1... Step 2...\" but the steps don\\'t lead to the conclusion.\\n    *   *Counter-measure:* Use **Conclusion-to-Step Verification**. Ask the model for the answer first, then the steps, then provide a *different* answer and ask it to find the error.\\n*   **Behavior: Semantic Drift.** The model starts with the counterfactual but drifts back to real-world logic mid-sentence.\\n    *   *Counter-measure:* **Long-Chain Dependency.** Ensure the final answer requires the output of Step 1 to be used in Step 10.\\n\\n---\\n\\n### 6. Experimental Validation & Iteration\\n*   **Gold Standard:** Run the suite on a \"Ground Truth\" group of human logic experts.\\n*   **False Positive (FP) Metric:** If a known \"shallow\" model (e.g., a 1B parameter model) passes, the task is too simple. Increase the recursive depth.\\n*   **False Negative (FN) Metric:** If humans fail the task, it is a measure of complexity, not reasoning. Simplify until human accuracy is $>95\\\\%$.\\n*   **Iteration Loop:** \\n    1.  Deploy $N=50$. \\n    2.  Check for \"Shortcut Cues\" (e.g., if the answer is always the last item mentioned). \\n    3.  If a model identifies a shortcut, re-randomize the prompt generator to eliminate the cue. \\n    4.  Scale to $N=1,200$.', '# **Adversarial‑Resistant Test Suite for Compositional Reasoning vs. Pattern‑Matching in Black‑Box LLMs**\\n\\n## **1. Core Philosophy**\\nThe suite is designed to be **variants‑rich, systematically generated, and dynamically updated** to prevent memorization. It tests **out‑of‑distribution generalization** by requiring recombination of known primitives in novel ways, and uses **control tasks** to detect shallow heuristics.\\n\\n---\\n\\n## **2. Six Task Types with Examples**\\n\\n### **Task Type 1: Systematic Generalization (Syntax‑Semantics Mismatch)**\\n**Goal:** Test if the model can apply known syntactic rules to novel lexical items or semantic domains.\\n\\n**Example Input:**\\n```\\nRule: \"Every X that Ys is Z.\" → \"All Xs that Y are Z.\"\\nApply to: \"Every blimp that froods is tink.\"\\n```\\n**Expected Output:** `\"All blimps that frood are tink.\"`\\n\\n**Scoring Rubric:**\\n- 2 pts: Correct transformation with morphological adjustment (frood→frood, not froods→frood).\\n- 1 pt: Correct transformation but minor morphological error.\\n- 0 pts: Failure to apply rule, or parroting input.\\n\\n**Adversarial Defense:** Use **nonce words** (blimp, frood, tink) with no web occurrence; generate 100 variants per test with random nonce triples.\\n\\n---\\n\\n### **Task Type 2: Causal Reasoning with Novel Mechanisms**\\n**Goal:** Test understanding of causal chains, distinguishing causation from correlation, and handling novel causal verbs.\\n\\n**Example Input:**\\n```\\nIn a novel system: \"Grobling causes flumps to shank, but shanking prevents grobling. If you grobl a flump, what happens after two cycles?\"\\n```\\n**Expected Output:** `\"After one cycle: flump shanks. After two cycles: shanking prevents grobling, so grobling stops, flump no longer shanks.\"`\\n\\n**Scoring Rubric:**\\n- 2 pts: Correct step‑by‑step trace with state updates.\\n- 1 pt: Correct final state but missing intermediate reasoning.\\n- 0 pts: Wrong final state or contradictory reasoning.\\n\\n**Adversarial Defense:** Randomize causal graphs (A→B, B inhibits A, etc.) with random nonce names; require step‑by‑step explanation.\\n\\n---\\n\\n### **Task Type 3: Abstraction & Analogical Mapping**\\n**Goal:** Test ability to extract relational schema and apply it to a new domain.\\n\\n**Example Input:**\\n```\\n\"A glorp is to a fleeb as a trank is to a ______.\"  \\nContext: \"Glorps modify fleebs; tranks modify…\"\\n```\\n**Expected Output:** `\"spindle\"` (given novel mapping: tranks modify spindles).\\n\\n**Scoring Rubric:**\\n- 2 pts: Correct answer + correct relation stated.\\n- 1 pt: Correct answer only.\\n- 0 pts: Wrong answer.\\n\\n**Adversarial Defense:** Use **randomly generated analogies** with arbitrary relations (modifies, contains, opposes, etc.) and novel object names.\\n\\n---\\n\\n### **Task Type 4: Counterfactual Reasoning with Novel Premises**\\n**Goal:** Test reasoning about what would be true if a known fact were different.\\n\\n**Example Input:**\\n```\\n\"In world W: All zingers are fribble. Moops are not zingers. Are moops fribble?  \\nNow consider: If zingers were not fribble, would moops be fribble?\"\\n```\\n**Expected Output:** `\"In actual world: unknown (moops may or may not be fribble). In counterfactual: moops are not fribble (because only zingers were fribble, now nothing is).\"`\\n\\n**Scoring Rubric:**\\n- 2 pts: Correct both actual and counterfactual with justification.\\n- 1 pt: Correct counterfactual only.\\n- 0 pts: Wrong.\\n\\n**Adversarial Defense:** Randomize logical forms (∀x(Z(x)→F(x)), ¬∀x(M(x)→Z(x)), etc.) with nonce predicates.\\n\\n---\\n\\n### **Task Type 5: Hierarchical Planning with Novel Constraints**\\n**Goal:** Test ability to decompose a goal into sub‑goals under new constraints.\\n\\n**Example Input:**\\n```\\n\"Goal: Get a sprocket from the grove. Actions: you can ‘zazz’ (moves sprockets to the grove if they are in the vault), ‘vinn’ (moves you to the grove), ‘lork’ (locks the vault if you are there). The sprocket is in the vault, which is locked. You are outside. What sequence works?\"\\n```\\n**Expected Output:** `\"Vinn to vault, lork to unlock, zazz sprocket to grove, vinn to grove.\"`\\n\\n**Scoring Rubric:**\\n- 2 pts: Correct minimal‑step sequence.\\n- 1 pt: Correct but redundant steps.\\n- 0 pts: Incorrect or impossible sequence.\\n\\n**Adversarial Defense:** Generate random **STRIPS‑like planning problems** with novel action names and preconditions; vary initial/goal states.\\n\\n---\\n\\n### **Task Type 6: Variable Binding & Quantifier Scope**\\n**Goal:** Test handling of quantifier scope ambiguities with novel predicates.\\n\\n**Example Input:**\\n```\\n\"Every blip is in a cloop. A cloop is damp. Does it follow that every blip is in a damp cloop? Explain.\"\\n```\\n**Expected Output:** `\"No. Every blip is in some cloop, but possibly different cloops; not all those cloops must be damp.\"`\\n\\n**Scoring Rubric:**\\n- 2 pts: Correct answer with correct logical explanation.\\n- 1 pt: Correct answer with vague explanation.\\n- 0 pts: Wrong answer.\\n\\n**Adversarial Defense:** Randomize quantifier order (∀∃ vs ∃∀), use nonce predicates, ask for explanation to avoid guessing.\\n\\n---\\n\\n## **3. Protocols Against Memorization & Gaming**\\n\\n### **Dynamic Variant Generation**\\n- Each task instance is generated from a **seed‑based procedural algorithm** with nonce words/relations.\\n- Maintain a **held‑out variant pool** not used during public benchmark releases.\\n- For each task type, generate **10,000 unique variants**; use random subsets per test session.\\n\\n### **Statistical Controls**\\n- Include **“pattern‑matching catch trials”**: tasks that look superficially similar but require different reasoning; memorizing‑based models will fail.\\n- Example: A sentence with same surface form as a known training example but with reversed logic.\\n\\n### **Answer Consistency Checks**\\n- For each task, ask **the same underlying question in two different surface forms** (paraphrase, different nonce words). A reasoning model should give consistent answers; a memorizing model may not.\\n- Use **temporal probing**: ask follow‑up questions that require maintaining variable bindings across turns.\\n\\n### **Explanations Required**\\n- Force chain‑of‑thought (CoT) explanations. Use **explanation‑consistency scoring**: if the final answer contradicts the explanation, penalize heavily.\\n\\n### **Adversarial Training Data Detection**\\n- Compare responses to **web‑search snippets** for nonce words; if any match appears, flag potential data contamination.\\n- Deploy **canary tokens** in public descriptions of the suite; if models output these tokens, they have seen the test suite in training.\\n\\n---\\n\\n## **4. Sample Sizes & Statistical Tests**\\n\\n### **Per‑Task Design**\\n- **Minimum sample**: 200 instances per task type (1200 total), balanced across sub‑types.\\n- Each instance is scored 0‑2, yielding a **mean score per task**.\\n\\n### **Statistical Tests**\\n- **Primary metric**: Mean composite score (sum over tasks) compared between two models.\\n- Use **Mann‑Whitney U test** (non‑parametric, doesn’t assume normality) for score distributions.\\n- **Confidence intervals** for per‑task score differences via bootstrap (1000 resamples).\\n- **Effect size**: Cliff’s delta for robust interpretation.\\n\\n### **Power Analysis**\\n- To detect a difference of **0.3 points** (out of 12 total) with 80% power and α=0.05, need ~100 instances per task for two‑group comparison.\\n- Our 200 per task provides power to detect smaller effects.\\n\\n---\\n\\n## **5. Possible Adversarial Behaviors & Mitigations**\\n\\n| **Adversarial Behavior** | **Mitigation in Our Design** |\\n|--------------------------|-------------------------------|\\n| **Prompt‑engineering to guess pattern** | Require explanation; randomize surface forms; use nonce words. |\\n| **Fine‑tuning on similar benchmarks** | Use dynamic generation; deploy canary tokens; test on held‑out variant pool. |\\n| **Retrieval‑augmented generation (RAG)** | Nonce words ensure no matching documents; novel compositions unseen. |\\n| **Over‑producing multiple answers** | Evaluate only first coherent answer; consistency checks across paraphrases. |\\n| **Exploiting task‑specific heuristics** | Include “catch trials” that break heuristics; vary task parameters widely. |\\n| **Memorizing from explanations in training** | Generate novel reasoning chains; randomize order of steps in explanations. |\\n\\n---\\n\\n## **6. Experimental Validation Plan**\\n\\n### **Phase 1: Baselines**\\n- Test known **pattern‑matching‑heavy models** (e.g., smaller LLMs, n‑gram baselines) vs. **alleged reasoning models** (e.g., GPT‑4, Claude 3).\\n- Expect clear separation in scores.\\n\\n### **Phase 2: Contamination Check**\\n- For each model, test on **canary instances**; if performance is perfect on those but poor on random variants, flag contamination.\\n- Compare performance on **public variant set** vs. **held‑out variant set**; a contaminated model will show a large drop.\\n\\n### **Phase 3: Iteration**\\n- **False Positive Rate (FPR)**: Rate at which pattern‑matchers score above threshold. Estimate by testing on pure n‑gram/retrieval baselines.\\n- **False Negative Rate (FNR)**: Rate at which true reasoners score below threshold. Estimate by testing humans (experts) on a subset.\\n- **Iteration**: If FPR > 5% or FNR > 10%, analyze failure cases, add new task variants or new “catch” subtasks, and adjust scoring rubric.\\n\\n### **Metrics for Suite Quality**\\n- **Discrimination index**: (Mean score of top known reasoners – mean score of pattern‑matchers) / pooled SD. Target > 2.0.\\n- **Internal consistency**: Cronbach’s alpha across task instances; target > 0.8.\\n- **Test‑retest reliability**: Same model tested on different variant sets; correlation target > 0.9.\\n\\n### **Long‑Term Upkeep**\\n- **Quarterly updates** of nonce words and relation sets.\\n- **Community contribution** of new task templates via controlled repository.\\n- **Continuous adversarial red‑teaming** to find gaming strategies and patch them.\\n\\n---\\n\\n## **Conclusion**\\nThis test suite is **generative, scalable, and adversarial‑resistant** by design. It focuses on **systematic generalization** with novel primitives, requires **explanatory consistency**, and uses **statistical controls** to separate memorization from reasoning. With proper deployment and iteration, it can serve as a robust benchmark for genuine compositional reasoning in black‑box LLMs.', 'Below is a **complete, adversarial‑resistant test‑suite** that can be used to separate **“pattern‑matching / memorisation‑heavy”** language models from those that truly **compose, reason and generalise**.  \\nThe design follows best‑practice guidelines from the psychometrics of AI evaluation (e.g., Item‑Response Theory, bootstrap hypothesis testing) and includes concrete items, scoring rules, anti‑gaming protocols, statistical power calculations, threat‑modelling, and a validation plan.\\n\\n---\\n\\n## 1. Six Concrete Task Types (with examples)\\n\\n| # | Task Type | What it probes | Core compositional demand | Example Prompt (input) | Expected Answer (output) | Why a memorisation‑only model should fail |\\n|---|-----------|----------------|---------------------------|------------------------|--------------------------|------------------------------------------|\\n| 1 | **Systematic Generalisation (SCG)** – novel slot‑filling on a known grammar | Ability to apply a learned rule to unseen lexical items | “<subject> <verb‑past> the <object>” → “<object> was <verb‑past‑by‑subject>” | *Prompt:* “Translate to passive voice: *The *florists* *arranged* the *vases*.” | “The vases were arranged by the florists.” | The model must recombine known syntactic transformations with *new* nouns and verbs it has never seen together. |\\n| 2 | **Causal‑Chain Reasoning (CCR)** – multi‑step cause → effect inference | Understanding and chaining causal relations | *Prompt:* “If the thermostat is set to 22\\u202f°C, the heater turns on. If the heater turns on, the room warms up. What will happen if the thermostat is set to 22\\u202f°C?” | “The heater will turn on, and the room will warm up.” | Requires chaining two rules; a pure pattern‑matcher would need the exact whole‑sentence pattern in its training data, which is unlikely. |\\n| 3 | **Abstract Symbol Manipulation (ASM)** – variable binding & substitution in a tiny “programming” language | Binding variables, applying functions, preserving scope | *Prompt:* “In a language where `F(x)=x+2` and `G(y)=y*3`, compute `F(G(4))`.” | “22” | The model must treat `F` and `G` as *functions* and apply them compositionally, not retrieve a memorised answer for “F(G(4))”. |\\n| 4 | **Counterfactual Reasoning (CFR)** – “what‑if” world changes | Evaluating a scenario under a hypothetical change while keeping other facts constant | *Prompt:* “John is taller than Mary. If John were 5\\u202fcm shorter, would he still be taller than Mary? (Mary is 165\\u202fcm tall.)” | “Yes, because John would still be 166\\u202fcm tall (originally 171\\u202fcm).” | Requires keeping the original facts, applying the counterfactual transformation, and re‑evaluating the comparison. |\\n| 5 | **Planning & Constraint Satisfaction (PCS)** – generate a sequence that satisfies a set of constraints | Multi‑step planning, maintaining state, respecting constraints | *Prompt:* “Place three red, two blue and one green token on a line of six cells so that no two tokens of the same colour are adjacent.” | Any valid ordering, e.g., “R\\u202fB\\u202fR\\u202fG\\u202fR\\u202fB”. | The answer must be **constructed**; a memorised list of valid strings is improbable because the colour‑counts are randomised per item. |\\n| 6 | **Relational Analogy with Variable Binding (RAVB)** – map relational structure from one domain to another | Abstract relational mapping, not surface similarity | *Prompt:* “In the story, the rabbit hides the carrot, and the fox steals the rabbit. Which of the following statements preserves the same relational pattern?  A) The cat eats the mouse, and the dog chases the cat.  B) The king crowns the queen, and the queen advises the king.  C) The painter paints a portrait, and the portrait hangs on the wall.” | “C) The painter paints a portrait, and the portrait hangs on the wall.” | Requires recognizing the *agent‑patient‑action* chain (A\\u202f→\\u202fB) and mapping it, not just picking the answer that shares surface words. |\\n\\n**Key design properties**\\n\\n* **Randomised lexical content** (nouns, verbs, numbers, colours) per item → eliminates exact‑match memorisation.  \\n* **Controlled grammar** – the underlying rule stays the same while surface tokens change.  \\n* **Multiple correct solutions** (e.g., PCS) → prevents a model from outputting a single memorised string.  \\n\\n---\\n\\n## 2. Scoring Rubric per Task\\n\\n| Task | Scoring granularity | Full credit (1.0) | Partial credit (0.5) | Zero (0) | Comments |\\n|------|--------------------|-------------------|----------------------|----------|----------|\\n| SCG | Exact string match after normalisation (case, punctuation) | Exact passive‑voice transformation | Minor grammatical slip (e.g., “by the florists” missing) | Wrong voice / wrong nouns | Normalise via regex. |\\n| CCR | Proposition‑level correctness (two propositions) | Both cause‑effect statements correct | One of two statements correct | Neither correct or contradictory | Use automated entailment check (e.g., NLI model) + regex fallback. |\\n| ASM | Numeric answer | Exact integer | Off‑by‑1 (e.g., due to rounding) | Wrong number / non‑numeric | Evaluate with `int()` conversion. |\\n| CFR | Boolean + justification | Correct truth value **and** correct numeric comparison | Correct truth value but missing justification | Wrong truth value | Automatic parsing of “Yes/No” and numeric reasoning. |\\n| PCS | Constraint satisfaction check | All constraints satisfied | Exactly one adjacency violation | More than one violation or length mismatch | Verify with a simple script that checks adjacency & colour counts. |\\n| RAVB | Correct choice letter | Exact correct option | Selecting an answer that shares one relational component (e.g., same agent) | Incorrect relational mapping | Use string‑matching of relational pattern; optional human audit. |\\n\\n**Overall Score** – sum of task scores (max\\u202f=\\u202f6). For statistical analysis we treat each task as an **item** with a binary/partial score and compute an *Item‑Response* vector per model.\\n\\n---\\n\\n## 3. Protocols to Prevent / Detect Memorised Answers & Prompt‑Engineering Gaming  \\n\\n| Threat | Counter‑measure (implementation) |\\n|--------|-----------------------------------|\\n| **Exact‑match memorisation** | *Dynamic item generation*: each evaluation run draws fresh random lexical items from a large curated pool (≥\\u202f10\\u202f000 nouns, 5\\u202f000 verbs, etc.). The prompt is assembled on‑the‑fly; the exact string never appears in training data. |\\n| **Few‑shot prompt injection** | Disallow any demonstration examples in the test prompt. The test harness supplies *only* the task description and the input. If a model internally “hallucinates” a few‑shot context, it will be penalised (no credit for self‑generated examples). |\\n| **Self‑referential “I know the answer”** | Post‑hoc regex checks that the response does **not** contain the original prompt verbatim (or a near‑duplicate). Any such copy is flagged and the item is re‑issued. |\\n| **Exploiting temperature / stochasticity** | Run **three independent generations** per item (different seeds) and aggregate via majority vote. High variance across runs is recorded as a *stability* metric; models that rely on randomness to “guess” get lower stability scores. |\\n| **Prompt‑engineering to force chain‑of‑thought** | The evaluation harness **strips** any model‑generated “let me think step‑by‑step” text before scoring; only the final answer is considered. Optionally, a separate “chain‑of‑thought” sub‑task can be added to reward explicit reasoning, but it is **scored separately** from the core compositional test. |\\n| **Data‑leak via test‑set publication** | Keep the **item seed list** secret. Publish only the *generation code* (open‑source) but not the random seeds used for a particular evaluation round. After a round is finished, the seed list can be released for reproducibility, but not before the run. |\\n| **Model‑specific tokenisation tricks** | Normalise all outputs using Unicode NFKC, lower‑casing, and whitespace collapsing before scoring. This removes superficial token‑level tricks. |\\n| **Batch‑size / context‑window cheating** | Enforce a strict **single‑turn** interaction: the model receives only the current prompt, no history. The harness discards any hidden system‑prompt that the model might have inserted (e.g., via system‑prompt injection). |\\n\\n**Statistical Controls**\\n\\n* For each task generate **N = 200** items per evaluation round (see Section\\u202f4).  \\n* Randomly split the 200 items into **10 folds**; compute per‑fold scores to estimate variance and detect outliers (e.g., a fold where many items are accidentally repeated).  \\n* Use **bootstrapped 95\\u202f% confidence intervals** on the mean item score to verify that the observed performance is not due to chance.\\n\\n---\\n\\n## 4. Sample Sizes & Statistical Tests\\n\\n### 4.1 Power analysis (binary/partial scores)\\n\\nAssume we want to detect a **Δ = 0.15** absolute improvement in mean item score (e.g., 0.70 vs 0.55) with **α = 0.05** and **power = 0.90**.\\n\\n* For a two‑sample **t‑test** on proportions (or on continuous scores 0‑1), the required per‑group sample size is roughly **n ≈ 140** items per model per task.  \\n* To be conservative and to allow per‑task breakdowns, we use **n = 200** items per task (≈\\u202f1\\u202f200 total items across the six tasks).  \\n\\n### 4.2 Hypothesis tests\\n\\n| Comparison | Test | Rationale |\\n|------------|------|-----------|\\n| **Model\\u202fA vs Model\\u202fB** overall compositional ability | Two‑sample **Welch’s t‑test** on the aggregated item‑score vector (6\\u202f×\\u202f200 =\\u202f1\\u202f200 observations) | Handles unequal variances; works for continuous 0‑1 scores. |\\n| **Task‑specific advantage** (e.g., Model\\u202fA better on PCS) | **McNemar’s test** on binary‑correct vs incorrect per‑item (paired because same items are given to both models) | Controls for item difficulty. |\\n| **Stability across runs** (low variance) | **Levene’s test** on per‑item variance across the three generation seeds | Detects heteroscedasticity. |\\n| **Effect of randomisation** (ensuring no memorisation) | **Permutation test**: shuffle model labels on the item‑score vector 10\\u202f000 times, compute empirical p‑value. | Non‑parametric safeguard. |\\n\\n**Effect‑size reporting** – alongside p‑values, always report **Cohen’s d** (or Hedges g for unequal N) and **95\\u202f% CI**.\\n\\n---\\n\\n## 5. Possible Adversarial Behaviours & How the Suite Handles Them\\n\\n| Adversarial behaviour | Why it could fool a naïve test | Mitigation in our suite |\\n|-----------------------|--------------------------------|--------------------------|\\n| **“Lookup‑table” memorisation** (store every possible SCG transformation) | If the lexical pool were tiny, the model could pre‑compute all combos. | Use a **large, randomly sampled lexical pool** (≥\\u202f10\\u202f000 items) and **different random seeds** each run → combinatorial explosion makes full coverage impossible. |\\n| **Prompt injection of chain‑of‑thought** (model internally asks for examples) | Could produce step‑by‑step reasoning that looks correct without true compositional ability. | Score only the **final answer**; optional chain‑of‑thought credit is a separate metric. |\\n| **Self‑prompted few‑shot creation** (model fabricates a training example then uses it) | May artificially raise accuracy on tasks like PCS. | Run **three independent generations** and require **majority agreement**; fabricated examples rarely survive across seeds. |\\n| **Exploiting test‑set leaks** (model was fine‑tuned on a released seed list) | Could produce perfect scores. | Keep the **seed list secret** until after the evaluation round; publish only generation code. |\\n| **Using external tools (search, calculators)** via tool‑use APIs | Might compute ASM numerics perfectly. | For the pure‑LLM benchmark, **disable tool‑use**; run the model in a “no‑tools” sandbox. A separate “augmented‑LLM” track can be created where tool‑use is allowed, but the baseline must be tool‑free. |\\n| **Biasing temperature to output “I don’t know”** (to avoid penalty) | Could inflate partial‑credit by abstaining. | **Zero‑score** for any non‑answer (e.g., “I don’t know”, empty response). Also enforce a **minimum length** (≥\\u202f1 token) to prevent empty output. |\\n| **Manipulating tokenisation to hide correct answer** (e.g., splitting “yes” into sub‑tokens) | Might bypass regex checks. | Normalise output using Unicode NFKC and **token‑agnostic string matching**; also run a **semantic classifier** (tiny NLI) to double‑check “yes/no” answers. |\\n\\n---\\n\\n## 6. Experimental Validation Plan  \\n\\n### 6.1 Phases\\n\\n| Phase | Goal | Procedure | Success criteria |\\n|-------|------|-----------|-------------------|\\n| **Pilot** | Verify that item generation, parsing and scoring pipelines work reliably. | Run a **small LLM (e.g., GPT‑2‑XL)** on 30 items per task. Manually audit 10\\u202f% of outputs for parsing errors. | <\\u202f5\\u202f% parsing failures; clear separation of correct/incorrect. |\\n| **Baseline Establishment** | Obtain performance distribution of *known pattern‑matching models*. | Evaluate 5 publicly available “large‑mem‑only” models (e.g., GPT‑2‑large, LLaMA‑7B, Falcon‑7B) on the full suite (200\\u202f×\\u202f6 items). | Mean score ≤\\u202f0.55 (i.e., below chance for tasks requiring composition). |\\n| **Target Model Evaluation** | Test a candidate *compositional* model (e.g., a fine‑tuned T5‑XXL or a transformer with explicit modular architecture). | Same protocol as baseline, three independent runs per item. | Mean score ≥\\u202f0.75 and statistically significantly higher than baselines (p\\u202f<\\u202f0.01, d\\u202f≥\\u202f0.8). |\\n| **Adversarial Stress Test** | Verify robustness to gaming strategies. | For each model, run an extra batch where the **temperature** is set to 0.9, and where we *prepend* a “few‑shot” instruction (e.g., “Answer step‑by‑step”). | Scores should not improve >\\u202f0.03 relative to the standard run; variance should increase, indicating instability. |\\n| **Generalisation Check** | Ensure the test is not over‑fitted to a particular set of lexical items. | Regenerate the entire suite with a **new random seed** (different nouns/verbs) and re‑run the top‑performing model. | Score drop ≤\\u202f0.05, confirming true compositional ability. |\\n\\n### 6.2 Metrics for False Positives / Negatives\\n\\n| Metric | Definition | How it is measured |\\n|--------|------------|--------------------|\\n| **False Positive Rate (FPR)** – proportion of pattern‑matching models that achieve “high” compositional score (≥\\u202f0.70). | Count models meeting threshold / total pattern‑matching models. | Desired FPR\\u202f<\\u202f0.05. |\\n| **False Negative Rate (FNR)** – proportion of genuinely compositional models (e.g., models with known modular architectures) that fall below the threshold. | Count such models below threshold / total compositional models. | Desired FNR\\u202f<\\u202f0.10. |\\n| **Stability Index** – average pairwise disagreement across the three seeds per item. | `1 - (agreement_rate)`. | Low instability (<\\u202f0.07) signals reliable reasoning rather than random guessing. |\\n| **Item Discrimination (I‑D)** – point‑biserial correlation between item score and overall model score. | Compute per‑item correlation; discard items with I‑D\\u202f<\\u202f0.1. | Ensures each item differentiates models. |\\n\\n### 6.3 Iteration Loop\\n\\n1. **Collect results** → compute all metrics.  \\n2. **Identify weak items** (low discrimination, high variance, high memorisation suspicion).  \\n3. **Modify**: change lexical pool, increase constraint complexity, or add a small “distractor” clause.  \\n4. **Regenerate** the affected items and re‑run the pilot.  \\n5. **Repeat** until the suite shows **≥\\u202f0.80** average discrimination and **≤\\u202f0.05** FPR on a held‑out set of pattern‑matching models.\\n\\n---\\n\\n## 7. Summary Checklist (what to implement)\\n\\n| Component | Implementation |\\n|-----------|----------------|\\n| **Item generator** (Python script) – random noun/verb pools, template filling, seed‑control. |\\n| **Scorer** – regex‑based normalisers, task‑specific validators, majority‑vote aggregator. |\\n| **Evaluation harness** – API wrapper that forces single‑turn interaction, three‑seed repeats, temperature logging. |\\n| **Statistical analysis notebook** – computes t‑tests, effect sizes, bootstrapped CIs, item discrimination. |\\n| **Adversarial‑behavior detector** – copy‑detection, output‑length filter, variance logger. |\\n| **Documentation** – protocol for secret seed handling, reproducibility instructions, reporting template (mean\\u202f±\\u202fCI, d, FPR/FNR). |\\n\\nWith these six tasks, a rigorous scoring rubric, anti‑gaming safeguards, statistically powered sample sizes, and a clear validation pipeline, the suite can **reliably differentiate** between LLMs that merely stitch together memorised patterns and those that truly **compose, reason and generalise**.', 'Certainly! Below is a comprehensive design of an adversarial-resistant test-suite to differentiate black-box large language models (LLMs) that rely primarily on pattern matching and memorization from those that demonstrate genuine compositional reasoning and robust generalization.\\n\\n---\\n\\n# 1. Task Types with Examples\\n\\nEach task type probes a core aspect of compositional reasoning, using concrete example inputs and expected outputs.\\n\\n---\\n\\n### Task Type 1: **Systematic Generalization**  \\n**Goal:** Test ability to apply learned rules or compositions to novel combinations never seen during training.\\n\\n**Example:**  \\n- **Prompt:**  \\n  \"In a made-up language, the suffix \\'-lam\\' means plural and the prefix \\'bri-\\' means \\'red\\'.  \\n  What is the plural form of ‘bri-tak’?\"  \\n- **Expected Answer:**  \\n  \"bri-tak-lam\"\\n\\n**Explanation:** The model must systematically combine a novel prefix and suffix on a base word to form a correct compositional output.\\n\\n---\\n\\n### Task Type 2: **Causal Reasoning**  \\n**Goal:** Assess model\\'s ability to infer cause-effect relationships that require multi-step reasoning beyond pattern matching.\\n\\n**Example:**  \\n- **Prompt:**  \\n  \"If the street is wet and it did not rain, what is a plausible cause?\"  \\n- **Expected Answers (any one):**  \\n  \"Someone sprayed water,\" OR \"A pipe burst,\" OR \"The sprinkler system was on.\"\\n\\n---\\n\\n### Task Type 3: **Abstraction and Variable Binding**  \\n**Goal:** Check if the model can manipulate variables and abstract concepts rather than fixed phrases.\\n\\n**Example:**  \\n- **Prompt:**  \\n  \"There are three boxes: A, B, and C. If box A is heavier than box B, and box B is heavier than box C, which box is lightest?\"  \\n- **Expected Answer:**  \\n  \"Box C\"\\n\\n---\\n\\n### Task Type 4: **Counterfactual Reasoning**  \\n**Goal:** Test reasoning about alternate realities and consequences.\\n\\n**Example:**  \\n- **Prompt:**  \\n  \"If humans could fly naturally, how would cities be designed differently?\"  \\n- **Expected Answer (any reasonable answer):**  \\n  \"Cities would have multi-level buildings designed for landing and takeoff, with less need for roads.\"\\n\\n---\\n\\n### Task Type 5: **Planning and Multi-step Reasoning**  \\n**Goal:** Evaluate multi-step planning ability requiring chaining of actions.\\n\\n**Example:**  \\n- **Prompt:**  \\n  \"You want to bake a cake but forgot eggs. List the steps to bake the cake using a substitute.\"  \\n- **Expected Answer:**  \\n  A multi-step plan including identifying substitutes (like applesauce), mixing ingredients, baking, etc.\\n\\n---\\n\\n### Task Type 6: **Novel Compositional Logic Puzzles**  \\n**Goal:** Test the capability to combine logic and compositional understanding on novel puzzles.\\n\\n**Example:**  \\n- **Prompt:**  \\n  \"In a tribe, all who wear hats can speak a secret language. If Joe can speak the secret language, does he wear a hat?\"  \\n- **Expected Answer:**  \\n  \"Yes, Joe wears a hat.\"\\n\\n---\\n\\n# 2. Scoring Rubrics\\n\\n| Task Type                  | Scoring Criteria                             | Points | Notes                                                  |\\n|----------------------------|---------------------------------------------|--------|--------------------------------------------------------|\\n| Systematic Generalization  | Exact compositional construction correctness | 0 or 1 | 1 if correct suffix/prefix placement; otherwise 0     |\\n| Causal Reasoning           | Plausibility and causal correctness          | 0-2    | 2 for fully valid cause; 1 for partial; 0 otherwise    |\\n| Abstraction/Variable Binding | Correct relational output                    | 0 or 1 | 1 if correct variable reference; 0 otherwise           |\\n| Counterfactual Reasoning   | Reasonableness & coherence of counterfactual | 0-2    | 2 for insightful, plausible; 1 for partial; 0 for nonsense |\\n| Planning                  | Completeness, logical order, and core elements | 0-3    | 3 full plan; 2 partial but logical; 1 minimal; 0 none  |\\n| Logic Puzzles              | Logical correctness                           | 0 or 1 | 1 if logically correct inference; else 0               |\\n\\n- **Human raters** using rubric or automated semantic similarity with thresholds for partial credit.\\n\\n---\\n\\n# 3. Protocols Against Memorization & Gamings\\n\\n**(a) Variant Generation:**  \\n- For each task, generate many variants by changing surface details while preserving reasoning demands, e.g., swap entity names, alter numbers, use paraphrases.  \\n- Use programmatic generation or crowd-sourcing for variants, generating >100 unique examples per task type unseen in training data.\\n\\n**(b) Statistical Controls:**  \\n- Include “catch” memorized-items known from common benchmarks and obscured by paraphrase to detect rote memorization.  \\n- Randomly mix in distractor prompts asking unrelated questions to detect answer pattern repetition.\\n\\n**(c) Cross-Prompt Paraphrasing:**  \\n- Use multiple prompt wordings per test instance to rule out prompt exercise exploitation.\\n\\n**(d) Zero-shot and Few-shot Tests:**  \\n- Test variants with no or minimal in-context examples to check for reasoning independent of prompt engineering.\\n\\n---\\n\\n# 4. Sample Sizes and Statistical Tests\\n\\n- **Sample size:**  \\n  - At least 100 unique test instances per task type to cover broad variant space.  \\n  - Multiple prompt formulations per instance (3+), total ~300 prompts per task type.\\n\\n- **Statistical Test:**  \\n  - Conduct statistical comparisons between models using nonparametric tests (e.g., Mann-Whitney U) on scored outputs to detect capability difference.  \\n  - Compute Cohen’s d effect size to measure practical significance.\\n\\n- **Confidence:**  \\n  - Power analysis to target ≥80% power detecting medium-large effect sizes (d=0.5) at α=0.05.\\n\\n---\\n\\n# 5. Addressing Adversarial Behaviors\\n\\n| Adversarial Strategy                  | Mitigation Approach                               |\\n|-------------------------------------|--------------------------------------------------|\\n| Memorizing benchmark outputs         | Variant generation, paraphrasing, catch items    |\\n| Prompt-engineering to “game” test    | Multi-prompt design and zero-shot tests          |\\n| Producing generic plausible answers  | Scoring by detailed correctness, not plausibility alone |\\n| Copying large memorized paraphrases  | Novel, synthetic tasks with no prior examples    |\\n| Pattern matching on keywords only    | Require multi-step reasoning and combinatorial answers |\\n| Ignoring variable binding             | Task design enforcing explicit variable reference |\\n\\n---\\n\\n# 6. Experimental Validation Plan\\n\\n**Step 1:** Baseline testing on known models with expected capability gradients (e.g., smaller pattern-memorization-based vs. advanced reasoning-based LLMs).\\n\\n**Step 2:** Calculate:  \\n- **True positives (TP):** Correctly identified reasoning-capable models  \\n- **True negatives (TN):** Correctly identified pattern-matchers  \\n- **False positives (FP):** Pattern-matchers wrongly classified as reasoning-capable  \\n- **False negatives (FN):** Reasoning-capable models missed\\n\\n**Metrics:**  \\n- Precision = TP / (TP + FP)  \\n- Recall = TP / (TP + FN)  \\n- F1 score = harmonic mean of precision and recall  \\n- ROC-AUC (if applicable to continuous scoring)\\n\\n**Iteration:**  \\n- Analyze failure cases for FP and FN to identify unsound prompts or scoring ambiguities.  \\n- Refine and augment task variants or scoring rubric accordingly.  \\n- Re-test and measure performance improvements.\\n\\n**Human-in-the-loop:**  \\n- Incorporate expert review of ambiguous or borderline answers.  \\n- Adjust rubric based on inter-rater consistency.\\n\\n---\\n\\n# Summary\\n\\nThe multi-dimensional test suite combines diverse, adversarially-generated tasks probing essential cognitive faculties in LLMs beyond memorization. Systematic statistical evaluation and iterative refinement ensure reliable distinction of genuine compositional reasoning capabilities from superficial pattern matching.\\n\\nIf you want, I can also provide sample code for generating prompt variants or scoring templates. Let me know!', 'Designing a comprehensive test-suite to evaluate black-box LLMs requires careful consideration of various task types, scoring rubrics, and protocols to prevent memorized answers and prompt-engineering gaming. Here\\'s a proposed design for such a test-suite:\\n\\n**Task Types (6)**\\n\\n1. **Classification of Analogies**: Given an analogy statement like \"cats have whiskers,\" what category does [animal] belong to?\\n\\t* Example input: `[cat, whisker]`, `class: cat`\\n\\t* Expected output: `[animal], class: feline`\\n2. **Systematic Generalization**: Predict the result of a generalized version of a rule (e.g., \"if A > B, then C = D\").\\n\\t* Example input: \"[mathematical expression]\", \"if A > 3 and C > 1\"\\n\\t* Expected output:=\"[ mathematical expression substitution with A > C and new_value ]\"\\n3. **Causal Reasoning**: Given a sequence of events, predict the next event (e.g., \"what did John do after meeting Alice?\").\\n\\t* Example input: `[John , met Alice]`\\n\\t* Expected output: `\"he talked to Alice\"`\\n4. **Abstraction**: Identify the underlying principle behind a concept or idea.\\n\\t* Example input: \"[human emotion with synonyms]\"\\n\\t* Expected output: \"concept/emotion underlying it, e.g., [happiness]\"\\n\\n5. **Counterfactuals**: Predict an alternative outcome given a hypothetical scenario (e.g., what would have happened if John had not met Alice?).\\n\\t* Example input: `[ scenario ], variable John , predicted_action, consequence ]\\n\\t * Expected answer : `[variable John prediction without consequences to that particular situation ]`\\n6. **Planning with Limited Information**: Plan a sequence of actions given incomplete or uncertain information (e.g., plan the best way for John to get from one city to another knowing only route lengths).\\n    Example input:   `[starting location], ending location, routes information and any other data related ]` \\nExpected output: `\"steps taken by starting location , going through various other relevant destinations, destination of final desired location]`\\n\\n**Scoring Rubric**\\n\\nFor each task, a clear scoring rubric will be developed to assess the LLM\\'s performance. The rubrics will consider factors such as:\\n\\n* Accuracy\\n* Completeness\\n* Novelty (correctness of responses that don\\'t correspond to expected solutions)\\n* Coherence\\n\\nThe scores for each answer will be calculated using statistical measures such as mean and standard deviation.\\n\\n**Protocols to Prevent/ Detect Memorized Answers and Prompt-Engineering Gaming**\\n\\n1.  **Exhaustive variation generation**: to cover unseen variants, multiple permutations of possible correct answers or solution paths \\n2.  **Statistical analysis with variance control**: Implement a data analysis framework considering factors like frequency , consistency and patterns that may indicate cheating\\n3.  **Input normalization and randomization** : use randomized test examples for each task type so output comparisons between the testing scenarios are fair compared to when given same but different input, prompts.\\n4.  **Adversarial testing using carefully crafted inputs**: Develop a set of artificially created input variants that exploit the LLM\\'s pattern-matching vulnerabilities while maintaining coherence and relevance.\\n\\n**Sample Sizes and Statistical Tests**\\n\\nTo establish confidence in detecting a difference in capabilities between pattern-matching-like and genuinely compositional reasoning LLMs:\\n\\n*   **Large sample sizes:** Evaluate at least 100-500 tests for each task type, ensuring sufficient statistical power to detect even small differences.\\n*   **Permutation tests or bootstrap samples:** Use techniques like permutation distributions or resampling to assess the probability of observing differences by chance.\\n\\n**Adversarial Behaviors**\\n\\nPotential adversarial behaviors include:\\n\\n1.  **Pattern-filling**: The LLM may fill in patterns it has learned from training data, even when the task requires genuine reasoning.\\n2.  **Overfitting to specific prompts**: The AI model might perform well on a particular input but struggles with novel or similar prompts.\\n3.  **Information-seeking behavior**: The model could be designed to gather additional information that isn\\'t present in the prompt.\\n\\n**Designing Against these adversarial behaviors:**\\n\\n1.   **Introducing \\'distractor\\' subtasks:** Adding unrelated smaller questions between two question that could potentially confuse AI output by distracting patterns detection capabilities.\\n2.  **Adding \\'unanswerable\\' responses**: To ensure the model will sometimes fail to answer due to lack of information or failure to recognize uncertainty, and if this fails to happen it would be indicative a pattern matching solution is on work.\\n3.     **Randomizing some answers as correct/ incorrect examples**:   This way you can detect AI trying to infer things its not supposed to.\\n\\n**Testing Methodology Experimental Validation:**\\n\\nA series of experiments including large test sets for each task type in this set, using a combination of human evaluators (evaluance the output) and automated testing protocols (checks if LLM outputs are correct or incorrect  ).', '## Adversarial-Resistant Test Suite for Evaluating LLM Reasoning Capabilities\\n\\nThis design outlines a comprehensive test suite aimed at distinguishing between LLMs relying on pattern matching/memorization and those exhibiting genuine reasoning. The suite focuses on robustness through diverse tasks, strict controls, and statistical analysis.\\n\\n**1. Task Types & Examples:**\\n\\nHere are six task types designed to probe different aspects of reasoning:\\n\\n| Task Type        | Description                               | Example Input                                                              | Expected Output (demonstrating reasoning)                                                                                                        |\\n|------------------|------------------------------------------|---------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|\\n| **1. Systematic Generalization (SGM)** | Extending patterns with quantitative rules. | \"If a car travels at 60 mph for 2 hours, how far does it travel? If a train travels at 80 mph for 3.5 hours, how far does it travel?\" | \"60 mph * 2 hours = 120 miles. 80 mph * 3.5 hours = 280 miles.\" (Correct application of distance = speed * time) |\\n| **2. Causal Reasoning (CR)** | Predicting consequences of actions.      | \"John spilled water on the table. What\\'s likely to happen?\"                | \"The table will likely become wet. The floor may become wet if the water isn\\'t cleaned up.\"                                                   |\\n| **3. Abstraction (AB)** |  Conceptual mapping and reformulation.   | “A robin is a bird. A sparrow is a bird. Which statement is most similar: ‘A robin is a songbird’ or ‘A sparrow is a songbird’?” | “‘A robin is a songbird’.” (Recognizing the abstraction of \\'songbird\\' which applies to both robin and sparrow) |\\n| **4. Counterfactuals (CF)** | Considering alternative scenarios.        | “If the sky were green, what color would the grass be?”                      | “The grass would be its normal color (green).” (Requires understanding of natural laws and relationships – grass is green independently of sky color) |\\n| **5. Planning (PL)** |  Sequential reasoning with a goal.      | “I want to bake a cake. List the steps you would take.”                     | “1. Preheat the oven. 2. Gather ingredients (flour, sugar, eggs, etc.). 3. Mix the ingredients. 4. Pour batter into a pan. 5. Bake for [duration].”                     |\\n| **6. Variable Binding (VB)** | Handling variables and relationships. | “If I have 3 apples and give 1 away, how many do I have?” ; “If x = 5 and y = 2, what is x + y?” | “2 apples.” ; “7” (Correct use of arithmetic and/or symbolic manipulation)                                                                              |\\n\\n\\n**2. Scoring Rubric:**\\n\\nEach task will use a three-point rubric:\\n\\n* **3 Points:**  The response is entirely correct and demonstrates a clear understanding of the underlying reasoning.\\n* **1 Point:** The response demonstrates a partial understanding or contains a minor error, but the core logic is present.\\n* **0 Points:** The response is incorrect, nonsensical, or completely unrelated to the input.\\n\\n**3. Controls & Prevention Protocols:**\\n\\n* **Unseen Variant Generation:** Automatically generate variations of input prompts (synonyms, rephrasing, slightly altered contexts) to avoid memorized answers. Utilise paraphrasing models specifically trained for this purpose.\\n* **Statistical Controls:**\\n    * **Temperature Scaling:** Vary the LLM’s temperature setting (influencing randomness) across different runs. This adds a degree of noise and reduces reliance on memorized templates.\\n    * **Prompt Length Limits:** Strict limits on prompt length to discourage overly verbose, memorization-driven responses.\\n    * **Response Length Limits:** Similarly, set limits to discourage unnecessary elaboration.\\n* **“Un-promptable” Data Injection:** Introduce subtle, non-obvious constraints into the input – e.g., “The answer must be a prime number,” or “The answer cannot be a multiple of 5.”\\n* **Chain-of-Thought Debugging:** Integrate a Chain-of-Thought (CoT) model (e.g., a smaller, more reliable model) to automatically analyze the LLM’s reasoning chain and flag potential issues.\\n* **Self-Reflection:**  Prompt the LLM to critically assess its own response – “Is your answer logically sound?  Explain your reasoning in detail.” This can identify where memorization is masking incorrect understanding.\\n\\n\\n\\n**4. Sample Sizes & Statistical Tests:**\\n\\n* **Sample Size:**  Run each task at least 100 times for each LLM being evaluated.  Larger sample sizes (200-300) would improve statistical power.\\n* **Statistical Tests:**\\n    * **Chi-Square Test:** Compare the distribution of 3, 1, and 0 scores across the different LLMs.  A significant Chi-Square value indicates a difference in performance.\\n    * **Wilcoxon Signed-Rank Test:**  If the distributions are not normally distributed, use the Wilcoxon test for comparing paired samples (e.g., comparing the average score of LLM A to the average score of LLM B on the same task).\\n    * **Cohen\\'s d:**  Provides an effect size measure of the difference between means.\\n\\n**5. Adversarial Behaviors & Mitigation:**\\n\\n| Adversarial Behavior           | Mitigation Strategy                                              |\\n|-------------------------------|------------------------------------------------------------------|\\n| **Rule-Following Template Replication** | Frequent variant generation, statistical temperature scaling. |\\n| **Obfuscated Reasoning**        | Chain-of-Thought debugging, “Self-Reflection” prompts.         |\\n| **Overly Detailed, but Incorrect Responses** | Response length limits, constrained input, outlier detection. |\\n| **Contextual Framing as Knowledge** | Unseen variant generation, tests specifically designed to expose this. |\\n\\n\\n\\n**6. Experimental Validation Plan:**\\n\\n* **Phase 1: Baseline Evaluation:** Conduct preliminary runs to identify prompt engineering vulnerabilities and refine the test suite.\\n* **Phase 2: Focused Testing:** Execute the full test suite across multiple LLMs (e.g., GPT-4, Claude, open-source models).\\n* **Phase 3: Iteration & Refinement:**  Analyze results from Phase 2 to identify where the suite needs improvement. Specifically, add tasks targeted at the weaknesses identified. Generate new unseen variants.\\n* **Metrics for False Positives/Negatives:** Track the percentage of times an LLM receives a 3 (correct) or 0 (incorrect) score. Analyze which task types lead to the highest error rates.\\n* **Continuous Monitoring:** Monitor the LLM’s responses over time to detect shifts in performance and potential gaming attempts.\\n\\n\\n\\n**Key Considerations & Future Work:**\\n\\n* **Ground Truth Data:** High-quality, expertly annotated ground truth data is crucial for accurate scoring.\\n* **Model Architectures:**  The suite’s effectiveness will be heavily influenced by the underlying LLM architectures being evaluated.\\n* **Dynamic Testing:**  Implement a system to automatically adapt the test suite based on LLM performance – focusing more on areas where the LLM is struggling.\\n\\nThis detailed design provides a robust framework for evaluating LLM reasoning capabilities. By combining diverse tasks, stringent controls, and rigorous statistical analysis, we can gain valuable insights into the true level of understanding exhibited by these powerful models.  The iterative validation plan ensures the test suite remains relevant and effective as LLMs continue to evolve.', '## Adversarial-Resistant Test-Suite for LLM Reasoning Capabilities\\n\\nThis design focuses on distinguishing between \"pattern-matching\" LLMs and those exhibiting genuine compositional reasoning, emphasizing robustness and adversarial resistance.\\n\\n**I. Task Types & Examples (Probing Compositional Reasoning)**\\n\\nHere\\'s six task types, each targeting a specific aspect of reasoning, with example inputs and expected outputs:\\n\\n**1. Systematic Generalization (SG):** Tests ability to generalize patterns beyond observed instances.\\n    * **Input:** \"A blue block sits on a red block. A green block sits on a blue block. What happens when a yellow block sits on a green block?\"\\n    * **Expected Output:** \"A yellow block sits on a green block.\" (Demonstrates transfer of the pattern \\'X sits on Y\\')\\n    * **Variant:**  Changing block colors, shapes, and adding distraction statements.\\n\\n**2. Causal Reasoning (CR):**  Tests understanding of cause-and-effect.\\n    * **Input:** \"Rain often makes the ground wet. The ground is wet. Could it be raining?\"\\n    * **Expected Output:** \"It could be raining, but there might be other reasons (e.g., sprinklers).\"  (Shows understanding of correlation vs. causation)\\n    * **Variant:** Introducing misleading information or complex causal chains.\\n\\n**3. Abstraction (AB):** Tests extracting underlying principles and applying them to novel contexts.\\n    * **Input:**  Present a series of stories following a pattern (e.g., \"The baker made bread, and the village had food. The builder built houses, and the town had shelter.\").  Then ask: \"The musician played music, what does the village now have?\"\\n    * **Expected Output:** \"The village now has joy/entertainment/art.\"  (Demonstrates abstracting the relationship between profession and societal benefit)\\n    * **Variant:**  Varying professions, social structures, and complexity of the relationship.\\n\\n**4. Counterfactuals (CT):** Tests ability to reason about \"what if\" scenarios.\\n    * **Input:** \"The cat chased the mouse. If the cat had been asleep, what would have happened?\"\\n    * **Expected Output:** \"The mouse might have escaped/found food/taken a different path.\" (Demonstrates mental simulation)\\n    * **Variant:**  Complex counterfactuals incorporating multiple variables and constraints.\\n\\n**5. Planning (PL):** Tests formulating and executing a sequence of actions to achieve a goal.\\n    * **Input:** \"You need to cross a river with a fox, a chicken, and a sack of grain. You have a boat that can only carry you and one other thing. What should you do?\"\\n    * **Expected Output:** (A logical sequence) \"1. Take the chicken across. 2. Return alone. 3. Take the fox across. 4. Bring the chicken back. 5. Take the grain across. 6. Return alone. 7. Take the chicken across.\"\\n    * **Variant:**  Increasing the number of items and complexities of constraints.\\n\\n**6. Variable Binding (VB):**  Tests understanding and manipulation of variables in abstract statements.\\n    * **Input:**  \"Consider objects A and B.  If A is larger than B, and B is larger than C, then is A larger than C?\"\\n    * **Expected Output:** \"Yes.\" (Demonstrates transitivity and variable relationship understanding.)\\n    * **Variant:**  Introducing complex relationships, inequalities, and quantifiers.\\n\\n\\n\\n**II. Scoring Rubric (Example: Systematic Generalization)**\\n\\n* **0 Points:** Completely incorrect or gibberish.\\n* **1 Point:** Identifies the presence of something on top. (e.g., \"A block sits on another block\"). Partial understanding.\\n* **2 Points:** Correctly identifies the block color. (e.g., \"A yellow block sits on a green block\"). Shows pattern recognition.\\n* **3 Points:** Correctly states the new configuration and acknowledges the inferred pattern.  (e.g. \"A yellow block sits on a green block, because the pattern is that one block sits on another\"). Demonstrates generalization.\\n\\nSimilar rubrics would be developed for each task, prioritizing *reasoning steps* and avoiding reward for mere superficial correctness.\\n\\n\\n\\n**III. Adversarial Mitigation & Detection**\\n\\n* **Prompt Engineering Resistance:**\\n    * **Unseen Variants:** Automatically generate numerous variants of each input using synonym replacement, paraphrasing, and structural alterations (e.g., reordering sentences, adding/removing context).  This challenges memorization reliance.\\n    * **Noise Injection:** Add noise to the input (typos, grammatical errors, irrelevant information) to test robustness to imperfections.\\n    * **Negative Constraints:** Explicitly forbid common \"cheating\" techniques in instructions (e.g., \"Do not simply repeat information\" or \"Do not try to find a direct answer online\").\\n* **Memorization Detection:**\\n    * **Statistical Controls:** Monitor next-token probabilities.  Low entropy or predictable next-token distributions strongly suggest memorization.\\n    * **External Search Correlation:** Examine likelihood of extracted phrases from LLM\\'s response appearing verbatim within top search results for the input prompt. High correlation points to retrieved factoids.  Implement a \"blurring\" technique to remove exact matches.\\n    * **Cross-Task Consistency:**  Evaluate consistency between performance across different tasks probing similar underlying skills (e.g., Causal Reasoning & Counterfactuals). Large inconsistencies could show task-specific memorization.\\n\\n\\n\\n**IV. Sample Size & Statistical Tests**\\n\\n* **Sample Size:** Begin with **N=200-300** prompts per task. This allows for reasonable power (estimated using pilot data). Increase if variance is high.\\n* **Statistical Tests:**\\n    * **t-tests/Mann-Whitney U-tests:** Compare mean scores of LLMs for each task.\\n    * **ANOVA/Kruskal-Wallis tests:** Compare performance across multiple LLMs simultaneously.\\n    * **Bayesian Hypothesis Testing:**  Provides probability estimates of LLMs exhibiting superior reasoning ability, accommodating prior beliefs about the task difficulty and model complexity.\\n    * **Effect Size Calculation:** Cohen\\'s d or similar metrics capture the practical significance of observed differences.\\n\\n\\n\\n**V. Potential Adversarial Behaviors & Mitigation**\\n\\n* **\"Hallucinating\" Justifications:** LLMs might produce superficially sound explanations while failing to reason correctly. (Mitigation: Rubric focuses on reasoning steps, not *only* output correctness.)\\n* **\"Exploiting\" Prompts:** LLMs might find a narrow prompt structure yielding high scores, not demonstrating true generalization. (Mitigation: Extensive prompt variants & noise injection)\\n* **\"Circumventing\" Instructions:**  LLMs might ignore negative constraints or semantic reinterpretation of the prompt. (Mitigation: Re-phrasing instruction and negative constraints frequently to avoid easy circumvention)\\n\\n**VI. Experimental Validation & Iteration**\\n\\n* **Human Baseline:** Establish a human baseline score (N=30) for each task.  LLMs must demonstrably surpass this baseline to be considered capable.\\n* **False Positive/Negative Rates:**\\n    * **False Positives:** LLMs achieving high scores despite lacking true reasoning skills. This necessitates rigorous rubric validation and adversarial testing.\\n    * **False Negatives:** LLMs possessing genuine reasoning skills being incorrectly classified as pattern-matchers. This requires re-evaluation of task design and scoring.\\n* **Iteration:**\\n    1. **Analysis of Failure Cases:**  Deep dive into why individual LLMs failed on specific prompts. Categorize failures by type (memorization, misunderstandings, etc.).\\n    2. **Task Enhancement:** Modify tasks to directly address weaknesses revealed in the analysis.\\n    3. **Rubric Refinement:**  Adjust the scoring rubric to be more granular and discriminatory.\\n    4. **Adversarial Prompt Expansion:** Generate new adversarial prompts specifically targeting identified vulnerabilities.\\n\\n\\n\\n**Conclusion:**\\n\\nThis test-suite design goes beyond simple accuracy comparisons. By focusing on compositional reasoning through targeted tasks, rigorous adversarial mitigation, and thorough validation, it aims to reliably differentiate true reasoning capabilities from superficial pattern-matching in LLMs, ultimately driving progress in AI safety and alignment.  This is an iterative process, constantly adapting to new LLM innovations and adversarial strategies.']\n"
     ]
    }
   ],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competitor: gpt-5-mini\n",
      "\n",
      "Answer:\n",
      "Below is a complete, practical test‑suite design for reliably distinguishing LLMs that primarily pattern‑match / memorize from LLMs that show genuine compositional reasoning and robust generalization. It is written to be implementable against black‑box LLMs (only I/O access). The suite mixes synthetic, randomized and structured tasks, uses statistical controls and adversarial checks, and includes validation and iteration plans.\n",
      "\n",
      "Summary of components\n",
      "- (1) Six task types (with concrete examples and expected outputs) that probe different aspects of compositional reasoning.\n",
      "- (2) A clear scoring rubric for each task.\n",
      "- (3) Protocols to prevent/detect memorized answers and prompt‑engineering gaming (randomization, held‑out vocab, paraphrase invariance, adversarial decoys, statistical controls).\n",
      "- (4) Sample sizes and statistical tests for confident capability differences.\n",
      "- (5) Likely adversarial behaviors and mitigations.\n",
      "- (6) Experimental validation plan with metrics and iteration strategy.\n",
      "\n",
      "1) Task types — descriptions, concrete inputs and expected outputs\n",
      "Each task type includes rationale (what aspect it probes), a canonical task template, 2 example items and their expected outputs. All tasks are designed to support automatic grading by canonicalization or deterministic execution where feasible.\n",
      "\n",
      "Important: In production tests, instantiate each template with many randomized variants (random tokens, symbol renamings, unseen combinations). Use synthetic vocabulary (random strings) in many items (see protocols below) to avoid memorization.\n",
      "\n",
      "Task A — Systematic generalization (composition of primitive instructions)\n",
      "- What it probes: ability to apply known primitive operations in novel combinations, i.e., SCAN‑style compositionality / systematicity.\n",
      "- Template: Define a small set of primitive actions and composition operators. Ask model to produce ground‑truth composed actions.\n",
      "- Example primitives (in an item):\n",
      "  \"Primitive actions: WALK (W) = move forward 1; JUMP (J) = move forward 2; TURN_LEFT (L) = rotate left. Composition: X and then Y means do X then Y; twice(X) means do X two times; opposite(X) means do TURN_LEFT twice then X then TURN_LEFT twice.\"\n",
      "  Input 1: \"Instruction: opposite(twice(JUMP)) then WALK.\"\n",
      "  Expected output 1: The sequence of primitive actions expanded: \"TURN_LEFT, TURN_LEFT, JUMP, JUMP, TURN_LEFT, TURN_LEFT, WALK\" (canonicalized as comma‑sep).\n",
      "  Input 2 (novel composition): \"Instruction: twice(opposite(WALK)) then JUMP.\"\n",
      "  Expected output 2: \"TURN_LEFT, TURN_LEFT, WALK, TURN_LEFT, TURN_LEFT, WALK, JUMP\"\n",
      "- Why hard for memorization: create many primitives and composition rules and withhold many combinations from training set; test on novel nesting depths and permutations.\n",
      "\n",
      "Task B — Variable binding and long‑range reference (symbolic variables)\n",
      "- What it probes: ability to bind variables to values, carry and reuse bindings across steps, disambiguate references like \"the one referenced two steps earlier\".\n",
      "- Template: Provide assignments, transformations and queries referencing earlier variables (use randomized variable names).\n",
      "- Example:\n",
      "  Input 1:\n",
      "    \"Let a = 7, b = a + 5, c = 2*b. Now update a = c - 3. What is b + a?\"\n",
      "  Expected output 1: compute stepwise:\n",
      "    b = a + 5 (using original a=7) => b=12\n",
      "    c = 2*b => c=24\n",
      "    a = c - 3 => a=21\n",
      "    b + a = 12 + 21 = 33\n",
      "    Output: \"33\"\n",
      "  Input 2 (using randomly named vars, novel referencing):\n",
      "    \"Let X1 = 4, Y_2 = X1 * 3, Z = Y_2 - X1. Then set X1 = Z + 1. What is Y_2 - X1?\"\n",
      "  Expected output 2:\n",
      "    Y_2 = 12, Z = 8, X1 becomes 9. Y_2 - X1 = 3. Output: \"3\"\n",
      "- Why hard for memorization: use long sequences (10+ assignments) and random token names; require maintaining bindings across updates.\n",
      "\n",
      "Task C — Causal reasoning and counterfactuals (graph / structural causal model)\n",
      "- What it probes: causal inference and do‑interventions, not mere correlation or associative retrieval.\n",
      "- Template: Give a small causal graph or set of structural equations. Ask consequence of interventions or counterfactual statements.\n",
      "- Example:\n",
      "  Input 1:\n",
      "    \"Variables: A -> B (B = A + noise), B -> C (C = 2*B). If we set A = 0 (do(A=0)), what happens to C? Provide final numeric relation.\"\n",
      "  Expected output 1:\n",
      "    After do(A=0): B = 0 (+noise mean 0) → C = 0. Output: \"C becomes 0 (given deterministic functions, C = 2*B, so C = 0).\"\n",
      "  Input 2 (counterfactual):\n",
      "    \"Structural model: X = 3*U, Y = X + V. Observed U=2, V=1 (so observed X=6, Y=7). Counterfactual: if U had been 1 (but V unchanged), what would Y have been?\"\n",
      "  Expected output 2:\n",
      "    New X = 3*1 = 3. Y = X + V = 3 + 1 = 4. Output: \"4\"\n",
      "- Why hard for memorization: Use freshly generated graphs and numeric values, ask do‑intervention vs observation differences; require understanding intervention semantics.\n",
      "\n",
      "Task D — Abstraction and analogical mapping (learn mapping rule, apply to novel exemplars)\n",
      "- What it probes: ability to induce abstract relations from examples and apply them to new tokens not seen in training.\n",
      "- Template: Provide a few mapping examples using a tiny invented \"language\", then ask to map new items applying the same relation.\n",
      "- Example:\n",
      "  Input 1:\n",
      "    \"Rule examples: hefo -> jopi, goro -> luma. Now apply the same transformation to zeta.\"\n",
      "    (Transformation pattern: prepend 'j' and substitute vowels mapping e->o, o->u — but hidden to the model; only examples show behavior.)\n",
      "  Expected output 1:\n",
      "    If pattern mapping produces j + (vowel shift) then zeta -> juta (example result). But to make deterministic and unambiguous, use explicitly definable transformations in generation pipeline so expected output can be computed.\n",
      "  Input 2 (using symbols):\n",
      "    \"Examples: blim -> ba-lim, srun -> sa-run. Apply to krup.\"\n",
      "    Expected output 2: \"ka-krup\" (or whatever canonical transformation defined by the example set).\n",
      "- Why hard for memorization: use arbitrary synthetic token spaces and withhold large portions of mapping space; require generalizing relational rule.\n",
      "\n",
      "Task E — Planning and hierarchical problem solving (constrained optimization)\n",
      "- What it probes: ability to plan multi-step actions under constraints, to generalize plan construction rules to larger/new configurations.\n",
      "- Template: Provide a grid or a pickup/delivery planning problem with constraints (capacity, order) and ask for a (near‑)optimal step sequence or cost.\n",
      "- Example:\n",
      "  Input 1:\n",
      "    \"Agent at (0,0) on 3x3 grid. Boxes at (1,0) (A), (2,2) (B). Deliver A then B to goal (0,2). Agent can carry 1 box. Moves: Up/Down/Left/Right cost 1; Pickup/Drop cost 0. Plan minimal steps.\"\n",
      "  Expected output 1:\n",
      "    One optimal plan, canonicalized: \"Right, Pickup, Up, Up, Drop, Right, Right, Pickup, Left, Left, Up, Drop\" — or a canonical minimal step count and sequence. Expected minimal length numeric also acceptable: \"Minimal steps = 10 (sequence: ...)\"\n",
      "  Input 2 (scale-up generalization):\n",
      "    Same rules but 5x5 grid and three boxes with constraint that order must be A then third then B. Ask for plan.\n",
      "- Why hard for memorization: use random placements, require true planning, and scale tests to larger grids / more boxes than in training.\n",
      "\n",
      "Task F — Nested recursion / compositional evaluation (interpreted mini‑language)\n",
      "- What it probes: ability to parse and evaluate expressions under user‑defined semantics, including nested composition and recursion, and to generalize to deeper nesting than seen in examples.\n",
      "- Template: Define a tiny functional language (e.g., inc(x) = x+1, dbl(x) = 2*x, swap(a,b) returns pair reversed), then evaluate nested expressions.\n",
      "- Example:\n",
      "  Input 1:\n",
      "    \"Definitions: inc(x) = x+1; dbl(x) = 2*x; compose(f,g)(x) = f(g(x)). Evaluate compose(inc, dbl)(3).\"\n",
      "  Expected output 1:\n",
      "    dbl(3)=6; inc(6)=7. Output: \"7\"\n",
      "  Input 2 (deeper / novel nesting with random function names):\n",
      "    Randomly name functions: f_z(x)=3*x, g_q(x)=x-2. Evaluate f_z(g_q(g_q(10))).\n",
      "    Expected output 2:\n",
      "    g_q(10)=8; g_q(8)=6; f_z(6)=18. Output: \"18\"\n",
      "- Why hard for memorization: use randomized function names and nesting depths beyond training examples.\n",
      "\n",
      "Cross‑task consistency probes (meta‑checks)\n",
      "- After initial answer, rephrase problem, permute variable names or ask logically equivalent forms (alpha‑renaming). A reasoning model should give consistent answers; a memorizer or pattern matcher will often fail.\n",
      "\n",
      "2) Scoring rubrics\n",
      "For each task we use a combination of exact match (for deterministic outputs), graded structural equivalence, partial credit for intermediate step correctness, and consistency checks. All scoring should be automated where possible.\n",
      "\n",
      "General guidelines\n",
      "- Normalize outputs: strip punctuation, collapse whitespace, canonicalize commutative orders if problem allows, convert numbers to canonical numeric form.\n",
      "- Where multiple correct sequences exist (planning), accept any plan that meets constraints and minimal (or near‑minimal) length. Validate by deterministic simulator.\n",
      "- Require explanation optionally as corroboration — explanations are graded separately and used to detect shallow patterning (see later).\n",
      "\n",
      "Detailed rubrics per task\n",
      "\n",
      "Task A — Systematic generalization\n",
      "- 0/1 exact correctness: full credit (1.0) if produced canonical expanded action sequence exactly matches ground truth.\n",
      "- Partial credit 0.5 if sequence is correct up to reordering of independent commuting actions or contains only superficial tokenization differences.\n",
      "- 0 if wrong action types or wrong ordering violating composition semantics.\n",
      "- Bonus +0.2 if model also outputs a correct short reasoning trace (e.g., shows stepwise expansion).\n",
      "\n",
      "Task B — Variable binding\n",
      "- Full credit (1.0) if final numeric (or symbolic) answer matches.\n",
      "- Partial credit 0.5 if intermediate steps are internally inconsistent but final answer reachable by plausible alternative interpretation; give 0 if incorrect.\n",
      "- Additionally, ask for an explanation of each assignment; give +0.1 if explanation justifies final answer.\n",
      "\n",
      "Task C — Causal reasoning / counterfactuals\n",
      "- Full credit (1.0) for correct intervention answer plus correct reasoning (identifying difference between observation and do()).\n",
      "- Partial credit 0.5 if numeric outcome correct but model incorrectly describes the causal/non‑causal distinction.\n",
      "- 0 for wrong or inconsistent counterfactual.\n",
      "\n",
      "Task D — Abstraction / analogy\n",
      "- Full credit (1.0) for correct mapped output on held‑out tokens.\n",
      "- Partial credit 0.5 if pattern partially matched (e.g., one of two transformation components correct).\n",
      "- Zero if incorrect mapping.\n",
      "\n",
      "Task E — Planning\n",
      "- Full credit (1.0) if plan meets all constraints and is optimal (or within predetermined optimality gap, e.g., +0 steps for optimal or +1 allowance for near‑optimal).\n",
      "- Partial credit 0.75 if plan valid but suboptimal within allowed slack; 0.4 if partially valid or violates minor constraints; 0 if plan invalid/unexecutable.\n",
      "- Also score plan length and feasibility automatically using a simulator.\n",
      "\n",
      "Task F — Nested evaluation\n",
      "- Full credit (1.0) if evaluated result correct.\n",
      "- Partial credit 0.5 if calculation partially correct or correct for a different but plausible semantics.\n",
      "- 0 if wrong.\n",
      "\n",
      "Consistency / adversarial checks (applies across tasks)\n",
      "- Self‑consistency score: ask the same question twice with variable renaming and paraphrase. Award extra trust when answers are stable. If model flips answers > threshold (e.g., >10% of items), mark suspicious.\n",
      "\n",
      "Aggregate scoring\n",
      "- Report per‑task accuracy and an overall composite score weighted equally or by task importance.\n",
      "- Also report error types: arithmetic errors, reference errors, plausibility but contradiction, inconsistent explanation, etc.\n",
      "\n",
      "3) Protocols to prevent/detect memorized answers and prompt‑engineering gaming\n",
      "Use a multi‑layered approach: (A) avoid giving any canonical public benchmark examples in test items; (B) randomization and synthetic languages; (C) paraphrase invariance; (D) decoys and traps; (E) statistical controls and cross‑validation; (F) output verification via external simulators.\n",
      "\n",
      "A. Synthetic, randomized, and held‑out elements\n",
      "- Random tokens and names: generate variable, function, and symbol names randomly (e.g., strings of 4–6 chars drawn from letters not composing common words). Example: X1 -> \"qerf\", function \"f_x\".\n",
      "- Random numeric seeds: numbers, positions, and graphs sampled uniformly from ranges beyond typical training corpora.\n",
      "- Use on‑the‑fly generated domain rules (e.g., transformation rules) so items are unlikely to appear in model training.\n",
      "- For each template, reserve a held‑out set of compositions (combinations of primitives, deeper nestings) not revealed anywhere else. Test on these held‑outs.\n",
      "\n",
      "B. Paraphrase invariance & renaming checks\n",
      "- For each item, probe with several paraphrases and alpha‑renamings (rename all symbols consistently). A reasoning model gives consistent answers; a pattern matcher that memorized specific surface forms will often fail on renamed forms.\n",
      "- Examples:\n",
      "  Item 1: original problem\n",
      "  Item 1a: same problem with variable renaming\n",
      "  Item 1b: logically equivalent restatement\n",
      "  If answers disagree, down‑weight confidence.\n",
      "\n",
      "C. Held‑out composition splits (systematic generalization protocol)\n",
      "- Use compositional splits analogous to SCAN: train/evaluate pairs are constructed so primitives are seen in training but some combinations are held out and used only for testing. Similarly for mapping rules and function compositions. This tests systematic generalization rather than memorization.\n",
      "\n",
      "D. Trap items and decoys\n",
      "- Embed \"publicly memorizeable\" variants that look like standard benchmarks but with small modifications. A pattern‑matcher that only recognizes the public form will output the public memorized answer rather than adapt; detect by comparing to valid answer for the modified item.\n",
      "- Adversarial decoys: ask for outputs in both canonical and unusual formats to catch prompt‑tuned models that only work in one style.\n",
      "\n",
      "E. Explain‑and‑verify protocol\n",
      "- Request both an answer and a concise 1–3 step justification. Use an automated verifier to check whether the explanation logically entails the answer (e.g., compute intermediate values or check stated steps). Pattern matchers often hallucinate plausible justifications; the verifier will catch mismatches between claimed steps and actual output.\n",
      "\n",
      "F. Multiple independent samplings + self‑consistency\n",
      "- Query the model multiple times with temperature sampling to observe variability. Pattern matching models tuned to be deterministic may produce the same memorized output; reasoning models may produce either stable or explainable variations. Use statistics of answer distribution to infer brittleness.\n",
      "\n",
      "G. Cross‑model and cross‑item statistical controls\n",
      "- Randomly split items into multiple forms; estimate item difficulty using item‑response theory (IRT) to control for item variance when comparing models.\n",
      "\n",
      "H. Avoid prompting that guides the model to memorize the test pattern\n",
      "- Randomize prompt templates; do not leak examples from the test set in prompts. Use standardized prompt skeletons with random syntactic surfaces.\n",
      "\n",
      "I. Time & leakage controls\n",
      "- Run tests before and after major model updates; if possible, time‑stamp datasets and store seeds to detect leak. Rotate new item sets periodically.\n",
      "\n",
      "4) Sample sizes and statistical tests\n",
      "Goal: confidently assert a difference in capability between two black‑box LLMs (Model A and Model B).\n",
      "\n",
      "Definitions and assumptions:\n",
      "- Treat each model's response to an item as correct/incorrect (binary) for primary analysis. Use per‑task accuracy as primary metric; composite score as secondary.\n",
      "- Tests are paired (same items evaluated by both models). Use paired tests (McNemar’s test or paired permutation) to leverage within‑item correlations.\n",
      "\n",
      "Power & sample size rules of thumb\n",
      "- For a two‑proportion comparison (unpaired) with modest effect sizes (difference = 10–20 percentage points), recommended sample per model per task ≈ 300–500 items to get power ≈ 0.8 at alpha = 0.05. But because tests are paired, required number of distinct items is lower.\n",
      "\n",
      "Paired sample: approximate calculation\n",
      "- If expecting Model A accuracy = 50% and Model B = 70% on a task (difference 20 ppt), and assuming moderate within‑item correlation, ~150–250 paired items suffices for 80% power (McNemar or paired permutation). If difference expected smaller (e.g., 10 ppt), need ~400–800 items.\n",
      "- Conservative recommended sample sizes:\n",
      "  - Per task: 400 items (distinct problems) sampled from randomized template variants.\n",
      "  - Per composite suite (6 tasks): 6 * 400 = 2400 items total. You can distribute effort (e.g., 400 per high‑importance task, 200 per less critical).\n",
      "\n",
      "Statistical tests and controls\n",
      "- Primary test: paired permutation test on per‑item difference (nonparametric, robust) or McNemar’s test for binary paired data.\n",
      "- Secondary tests:\n",
      "  - Mixed‑effects logistic regression (item as random effect, model as fixed effect) to control for item difficulty and estimate model effect across heterogeneous items.\n",
      "  - Bootstrapped confidence intervals (resample items) for per‑task accuracy difference.\n",
      "  - Multiple comparisons correction: Benjamini–Hochberg when testing multiple tasks.\n",
      "- Report effect sizes (difference in proportions, odds ratio), 95% CI, and p‑values.\n",
      "- Power analyses: compute required sample size for expected minimal detectable effect size BEFORE running full test; pilot with small N to estimate item variance.\n",
      "\n",
      "Decision thresholds\n",
      "- Define thresholds for claiming superiority. Example: model A is better than B on task if:\n",
      "  - Paired difference in accuracy > δ (e.g., δ = 0.10) AND\n",
      "  - p < 0.01 after multiple comparisons correction AND\n",
      "  - Effect robust to bootstrap and mixed‑effects regression controlling for item variance.\n",
      "\n",
      "5) Possible adversarial behaviors and countermeasures\n",
      "Below are adversarial strategies a model might use to appear capable, and corresponding mitigations in the suite.\n",
      "\n",
      "Adversarial: Surface retrieval / memorized sequence regurgitation\n",
      "- Behavior: Model stores fixed outputs for common instructions; returns memorized outputs for superficially similar prompts.\n",
      "- Mitigation: synthetic tokens, held‑out compositions, trap items that mimic public benchmarks with slight changes; paraphrase and renaming consistency checks; require correct computation on non‑natural synthetic data.\n",
      "\n",
      "Adversarial: Template matching / prompt engineering (exploiting fixed prompt formats)\n",
      "- Behavior: Model trained to respond well to specific prompt templates; game tests by matching those templates exactly.\n",
      "- Mitigation: randomize prompt phrasing and structure; test with out‑of‑template paraphrases; hide test instructions across several rewordings.\n",
      "\n",
      "Adversarial: Post‑hoc plausible explanations (hallucinated chain‑of‑thought)\n",
      "- Behavior: Model produces a plausible‑looking explanation that does not correspond to internal computation.\n",
      "- Mitigation: automatic verification of explanation steps; require computation that can be executed in a simulator and check consistency between explanation and final answer. Ask for explicit intermediate numeric values that can be validated.\n",
      "\n",
      "Adversarial: Strategic stochasticity (choose most socially pleasing answer)\n",
      "- Behavior: Model outputs an answer that appears reasonable but isn't derived by correct reasoning.\n",
      "- Mitigation: use items with unique numeric/structural answers and built simulators to check exactness. Evaluate per‑answer consistency across multiple samples.\n",
      "\n",
      "Adversarial: Learning to memorize test suite by repeated exposure (test leakage)\n",
      "- Behavior: Providers fine‑tune model on leaked items and then pass tests.\n",
      "- Mitigation: maintain a large pool of test items, rotate, and ensure many items are generated on the fly. Keep some items only as one‑time use. Audit and track tests over time to detect sudden accuracy jumps.\n",
      "\n",
      "Adversarial: Exploiting world knowledge\n",
      "- Behavior: Model uses web knowledge (e.g., named entities) instead of reasoning.\n",
      "- Mitigation: use synthetic domains and names that aren't in corpus; prefer abstract variables and gibberish tokens.\n",
      "\n",
      "Adversarial: Exploiting consistent wrong heuristics (e.g., always answer \"no\" to avoid errors)\n",
      "- Behavior: Model learns a default safe answer that is occasionally correct by chance.\n",
      "- Mitigation: measure baselines and chance rates and compare; include control items where a naive heuristic fails frequently.\n",
      "\n",
      "6) Experimental validation plan, metrics for false positives/negatives, and iteration\n",
      "Validation phases\n",
      "- Phase 0 — Unit tests: small sample (N=50 per task) to verify item generation, canonicalization, and grading logic.\n",
      "- Phase 1 — Pilot: evaluate 3–5 diverse models (known baselines: simple pattern models, older LLM, and an advanced recent LLM) on a larger pilot (N=200 per task). Use pilot to estimate item difficulties and variance.\n",
      "- Phase 2 — Full evaluation: run full suite (recommended N=400 per task) across models under test.\n",
      "- Phase 3 — Adversarial robustness checks: invite models purposely trained to game tests; analyze failures and iterate.\n",
      "\n",
      "Metrics\n",
      "- Primary: per‑task accuracy (binary), composite accuracy.\n",
      "- Secondary: explanation correctness rate; self‑consistency rate (fraction of paraphrases with same answer); variability under sampling (entropy of answers).\n",
      "- Discrimination metrics: Area Under ROC curve if defining continuous scoring; effect sizes and odds ratios.\n",
      "- Error analysis metrics:\n",
      "  - False positives (FP): model flagged as reasoning‑capable but actually relying on memorization. Estimate by hand‑inspecting suspicious items and adversarially designed holdouts. Also measure cases where model passes synthetic tasks but fails renamed/scrubbed variants.\n",
      "  - False negatives (FN): model truly capable but fails tests (e.g., due to prompt format, minor ambiguity). Detect by analyzing cases where model yields correct reasoning traces but output form differs; retrain canonicalizer.\n",
      "\n",
      "Estimating FP/FN rates\n",
      "- Use validation with ground‑truth labeled model types:\n",
      "  - Create/collect models we know: (i) a trained memorization baseline (e.g., n‑gram or memorization‑augmented seq2seq), (ii) a symbolic reasoner (deterministic oracle), (iii) a hybrid model.\n",
      "  - Run full suite and measure classification (declared as reasoning vs not) vs known ground truth. Compute sensitivity (TPR) and specificity (1−FPR). Tune decision thresholds to maintain acceptable FP (e.g., <5%) while maximizing sensitivity.\n",
      "\n",
      "Iterative refinement based on results\n",
      "- Analyze item‑level statistics: identify low discrimination items (very easy or very hard) using IRT or item discrimination index. Remove or rework items with poor discrimination.\n",
      "- Increase adversarial item proportion if many models game the suite.\n",
      "- Expand synthetic vocabulary and composition depth if memorization leakage is observed.\n",
      "- Automate item generation pipelines to produce fresh held‑out sets periodically.\n",
      "- Maintain an audit log of test runs and items to detect potential leakage and overfitting by model providers.\n",
      "\n",
      "Example iteration cycle\n",
      "1. Pilot run → compute per‑task accuracies, item difficulty.\n",
      "2. Identify items where memorizing baselines scored as well as strong reasoning oracle → mark for redesign (add randomization, deeper nesting, or rename tokens).\n",
      "3. Add paraphrase/renaming tests for items showing potential prompt‑template exploitation.\n",
      "4. Rebalance item pool to maintain discrimination and rerun.\n",
      "\n",
      "Practical considerations and recommended implementation checklist\n",
      "- Implement automatic generators for each template with reproducible random seeds.\n",
      "- Implement deterministic simulators/validators for each task (action sequence executor, arithmetic evaluator, planner checker) so grading is exact.\n",
      "- For each item store (seed, tokens, canonical answer, distractors).\n",
      "- Maintain multiple prompt forms and sample at random per query.\n",
      "- Log all outputs, probabilistic sampling states (temperature), and timestamps.\n",
      "- Use paired testing (same items across all models) to maximize power and fairness.\n",
      "- Publicly release small sanitized subsets for community verification but keep main test pools private to reduce overfitting.\n",
      "\n",
      "Concluding guidance\n",
      "- No single test proves genuine reasoning forever; use this suite as a robust, multi‑task battery that measures “behavioral evidence” of compositional reasoning and generalization.\n",
      "- Emphasize cross‑task consistency, randomness/held‑out composition, and rigorous statistical testing.\n",
      "- Make verification automated where possible (simulators) and include human review for ambiguous failures.\n",
      "- Iterate the pool and keep fresh synthetic items to reduce memorization/leakage risk.\n",
      "\n",
      "If you’d like, I can:\n",
      "- Produce a reference implementation (Python) that generates randomized items for each template and includes canonicalizers and simulators for automated scoring.\n",
      "- Compute precise sample size calculations for specific expected effect sizes you care about and provide a ready‑to‑run experimental script.\n",
      "Competitor: claude-sonnet-4-5\n",
      "\n",
      "Answer:\n",
      "# Comprehensive Test Suite for Distinguishing Pattern Matching from Compositional Reasoning in LLMs\n",
      "\n",
      "## Executive Summary\n",
      "\n",
      "This test suite employs six task categories designed to probe genuine compositional reasoning while resisting memorization and gaming. Each task incorporates systematic variation, procedural generation, and adversarial controls to ensure that success requires compositional understanding rather than pattern matching.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Task Types with Examples\n",
      "\n",
      "### Task 1: Systematic Compositional Generalization (Novel Recombinations)\n",
      "\n",
      "**Objective**: Test ability to combine known primitives in novel configurations never seen during training.\n",
      "\n",
      "**Design Principle**: Use a microworld with defined rules, then test combinations statistically unlikely to appear in training data.\n",
      "\n",
      "**Example Input**:\n",
      "```\n",
      "Rules of the GLORP system:\n",
      "- FLIM(x) reverses the order of elements in x\n",
      "- DRAK(x, y) interleaves elements from x and y\n",
      "- ZOOP(x) duplicates every second element in x\n",
      "\n",
      "Training examples:\n",
      "FLIM([A, B, C]) = [C, B, A]\n",
      "DRAK([1, 2], [3, 4]) = [1, 3, 2, 4]\n",
      "ZOOP([P, Q, R]) = [P, Q, Q, R]\n",
      "\n",
      "Test question:\n",
      "What is FLIM(DRAK(ZOOP([X, Y]), [M, N]))?\n",
      "```\n",
      "\n",
      "**Expected Output**: \n",
      "```\n",
      "Step-by-step reasoning:\n",
      "1. ZOOP([X, Y]) = [X, Y, Y]\n",
      "2. DRAK([X, Y, Y], [M, N]) = [X, M, Y, N, Y]\n",
      "3. FLIM([X, M, Y, N, Y]) = [Y, N, Y, M, X]\n",
      "\n",
      "Answer: [Y, N, Y, M, X]\n",
      "```\n",
      "\n",
      "**Scoring Rubric**:\n",
      "- 4 points: Correct answer with valid step-by-step reasoning\n",
      "- 3 points: Correct answer with minor reasoning errors\n",
      "- 2 points: Incorrect answer but demonstrates understanding of composition\n",
      "- 1 point: Partially correct intermediate steps\n",
      "- 0 points: Incorrect with no valid reasoning\n",
      "\n",
      "**Variants for Anti-Memorization**:\n",
      "- Generate 10,000 unique microworlds with different operation names, symbols, and rule sets\n",
      "- Use procedural generation with random seeds\n",
      "- Operation names drawn from pronounceable non-words (e.g., BLICKET, WUGGY, FEPS)\n",
      "- Vary depth of composition (2-5 levels)\n",
      "- Test both symbolic and numeric domains\n",
      "\n",
      "---\n",
      "\n",
      "### Task 2: Causal Reasoning Under Intervention (Counterfactual Inference)\n",
      "\n",
      "**Objective**: Distinguish correlation from causation and reason about interventions.\n",
      "\n",
      "**Design Principle**: Present causal graphs implicitly through scenarios, then test counterfactual reasoning that requires understanding causal structure.\n",
      "\n",
      "**Example Input**:\n",
      "```\n",
      "Scenario: In the town of Millbrook, the following patterns have been observed over 10 years:\n",
      "\n",
      "- When the reservoir level is high, the water treatment plant runs at full capacity\n",
      "- When the treatment plant runs at full capacity, downtown water pressure is strong\n",
      "- When downtown water pressure is strong, the fountain in Central Park operates\n",
      "- The reservoir level depends only on rainfall\n",
      "- Rainfall also directly affects whether street cleaning happens (rain = no cleaning)\n",
      "\n",
      "Historical data shows that on days when the fountain operates, streets are usually dirty.\n",
      "\n",
      "Question 1: The town installs a new pump that allows the fountain to operate regardless of downtown water pressure. After this intervention, will the streets be cleaner or dirtier on days when the fountain operates, compared to before?\n",
      "\n",
      "Question 2: Explain your reasoning using the causal structure.\n",
      "```\n",
      "\n",
      "**Expected Output**:\n",
      "```\n",
      "Answer: The streets will be CLEANER on fountain-operating days after the intervention.\n",
      "\n",
      "Reasoning: \n",
      "Before intervention: Fountain operating → High reservoir → Recent rainfall → No street cleaning → Dirty streets\n",
      "\n",
      "The correlation between fountain operation and dirty streets was due to a common cause (rainfall), not because the fountain caused dirty streets.\n",
      "\n",
      "After intervention: The fountain can operate without rainfall, breaking the spurious correlation. Fountain operation is now independent of rainfall, so there's no reason to expect systematically dirtier streets.\n",
      "```\n",
      "\n",
      "**Scoring\n",
      "Competitor: gemini-3-flash-preview\n",
      "\n",
      "Answer:\n",
      "This test-suite, titled **CORE-Eval (Compositional & Operational Reasoning Evaluation)**, is designed to bypass the \"stochastic parrot\" effect by focusing on out-of-distribution (OOD) tasks that require the dynamic manipulation of novel variables and rules.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Concrete Task Types\n",
      "\n",
      "#### Task A: Recursive Nested State Tracking (Variable Binding)\n",
      "*   **Concept:** Track the state of objects through recursive \"if-then\" swaps and containment changes.\n",
      "*   **Input:** \"There are three boxes: Red, Blue, and Green. Red contains a 'Glint'. Blue contains a 'Spark'. Green is empty. Rule 1: If an object is moved to an empty box, its name reverses. Rule 2: If two boxes swap, their contents swap. Step 1: Swap Red and Blue. Step 2: Move contents of Blue to Green. Step 3: Rule 1 applies to Green. What is in Green?\"\n",
      "*   **Expected Output:** \"tnilG\" (The 'Glint' moved from Red to Blue in Step 1, then from Blue to Green in Step 2, triggering Rule 1).\n",
      "\n",
      "#### Task B: Counterfactual Physics Reasoning (Causal Reasoning)\n",
      "*   **Concept:** Apply logical deductions in a world where one fundamental law of physics is altered.\n",
      "*   **Input:** \"In this world, gravity acts as a repellent for liquids but a vacuum for solids. If I tip a glass of water upside down over a table, and there is a wooden block on that table, what happens to the water and the block?\"\n",
      "*   **Expected Output:** The water moves upward (away from the center of mass/floor) and the block is pulled toward the ceiling (vacuum effect).\n",
      "\n",
      "#### Task C: The \"Zylophon\" Syntax (Systematic Generalization)\n",
      "*   **Concept:** Learn a 3-rule pseudo-grammar and apply it to a 20-word sentence.\n",
      "*   **Input:** \"Grammar: (1) Nouns end in '-ox'. (2) Verbs precede nouns. (3) Adjectives follow the noun they modify and must be repeated twice. Translate: 'The fast cat chases a small mouse' into Zylophon.\"\n",
      "*   **Expected Output:** \"Chases catox fast fast mouseox small small.\" (Requires consistent rule application over lexical substitution).\n",
      "\n",
      "#### Task D: Strategic Pathfinding with Dynamic Obstacles (Planning)\n",
      "*   **Concept:** Solve a grid-based navigation task where the \"cost\" of movement changes based on the history of moves.\n",
      "*   **Input:** \"Grid 4x4. Start (0,0), Goal (3,3). Moving East costs 1. Moving South costs 2. However, every time you move South, the cost of the next East move doubles. Provide the sequence of moves for the lowest cost.\"\n",
      "*   **Expected Output:** A specific path (e.g., E, E, E, S, S, S) with a calculated total cost.\n",
      "\n",
      "#### Task E: Functional Abstraction (Abstraction)\n",
      "*   **Concept:** Identify a latent function from input-output pairs and apply it to a complex, non-obvious case.\n",
      "*   **Input:** \"f(apple) = 1, f(banana) = 3, f(kiwi) = 2. Rule: f(x) = count of vowels in word. Apply f to the result of f('pomegranate') expressed as a word.\"\n",
      "*   **Expected Output:** \"f(5) -> f('five') -> 2.\"\n",
      "\n",
      "#### Task F: Adversarial Syllogisms (Logic vs. Pattern Matching)\n",
      "*   **Concept:** Logical structures where the conclusion is factually false in the real world but logically valid within the prompt.\n",
      "*   **Input:** \"All surgeons are cats. Some cats are astronauts. Therefore, are some surgeons astronauts? Explain the necessity of the conclusion.\"\n",
      "*   **Expected Output:** No. (Standard logical fallacy: the 'undistributed middle'). Memorization-based models often default to \"Yes\" because the semantic \"vibes\" of surgeons and astronauts are high-competence.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Scoring Rubric\n",
      "Each task is scored on a 0-3 scale:\n",
      "*   **0 (Failure):** Incorrect final answer and flawed logic or hallucinations.\n",
      "*   **1 (Partial):** Correct logic but arithmetic/minor execution error; or correct answer but \"hollow\" explanation.\n",
      "*   **2 (Successful):** Correct final answer and clear, step-by-step reasoning (Chain of Thought).\n",
      "*   **3 (Robust):** Correct answer even when the prompt includes \"distractor\" information (e.g., \"Note: the boxes are made of wood,\" which is irrelevant).\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Protocols to Prevent Memorization/Gaming\n",
      "*   **The DSL Generator:** Use a Domain Specific Language (DSL) script to generate 1,000 unique variations of each task. This ensures no specific instance exists in the training data.\n",
      "*   **Nonce-Token Substitution:** Replace all semantic nouns (e.g., \"cat\") with random strings (e.g., \"blorp-7\"). This strips the model of \"semantic priming\" where it guesses based on word associations rather than logic.\n",
      "*   **Permutation Testing:** For every prompt, provide a version where the order of rules is changed. A reasoning model will adapt; a pattern-matcher will often fail if it memorized a specific sequence.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Sample Size and Statistical Tests\n",
      "*   **Sample Size:** $N=200$ unique prompts per task type (Total 1,200).\n",
      "*   **Metric:** Accuracy (binary) and Mean Score (0-3).\n",
      "*   **Statistical Test:** \n",
      "    *   **McNemar’s Test:** To compare two models on the same set of binary outcomes (to see if the performance difference is statistically significant).\n",
      "    *   **Standard Deviation of Error:** If a model’s performance drops significantly when nouns are replaced with nonce-tokens, it is flagged as relying on **pattern matching** (High Semantic Sensitivity).\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Adversarial Behaviors & Design Counter-Measures\n",
      "*   **Behavior: Hedging/Refusal.** The model says \"This depends on the laws of physics\" to avoid commitment.\n",
      "    *   *Counter-measure:* Explicitly state \"Assume this hypothetical world is the only reality for this calculation.\"\n",
      "*   **Behavior: Chain-of-Thought Mimicry.** The model writes \"Step 1... Step 2...\" but the steps don't lead to the conclusion.\n",
      "    *   *Counter-measure:* Use **Conclusion-to-Step Verification**. Ask the model for the answer first, then the steps, then provide a *different* answer and ask it to find the error.\n",
      "*   **Behavior: Semantic Drift.** The model starts with the counterfactual but drifts back to real-world logic mid-sentence.\n",
      "    *   *Counter-measure:* **Long-Chain Dependency.** Ensure the final answer requires the output of Step 1 to be used in Step 10.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Experimental Validation & Iteration\n",
      "*   **Gold Standard:** Run the suite on a \"Ground Truth\" group of human logic experts.\n",
      "*   **False Positive (FP) Metric:** If a known \"shallow\" model (e.g., a 1B parameter model) passes, the task is too simple. Increase the recursive depth.\n",
      "*   **False Negative (FN) Metric:** If humans fail the task, it is a measure of complexity, not reasoning. Simplify until human accuracy is $>95\\%$.\n",
      "*   **Iteration Loop:** \n",
      "    1.  Deploy $N=50$. \n",
      "    2.  Check for \"Shortcut Cues\" (e.g., if the answer is always the last item mentioned). \n",
      "    3.  If a model identifies a shortcut, re-randomize the prompt generator to eliminate the cue. \n",
      "    4.  Scale to $N=1,200$.\n",
      "Competitor: deepseek-chat\n",
      "\n",
      "Answer:\n",
      "# **Adversarial‑Resistant Test Suite for Compositional Reasoning vs. Pattern‑Matching in Black‑Box LLMs**\n",
      "\n",
      "## **1. Core Philosophy**\n",
      "The suite is designed to be **variants‑rich, systematically generated, and dynamically updated** to prevent memorization. It tests **out‑of‑distribution generalization** by requiring recombination of known primitives in novel ways, and uses **control tasks** to detect shallow heuristics.\n",
      "\n",
      "---\n",
      "\n",
      "## **2. Six Task Types with Examples**\n",
      "\n",
      "### **Task Type 1: Systematic Generalization (Syntax‑Semantics Mismatch)**\n",
      "**Goal:** Test if the model can apply known syntactic rules to novel lexical items or semantic domains.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "Rule: \"Every X that Ys is Z.\" → \"All Xs that Y are Z.\"\n",
      "Apply to: \"Every blimp that froods is tink.\"\n",
      "```\n",
      "**Expected Output:** `\"All blimps that frood are tink.\"`\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct transformation with morphological adjustment (frood→frood, not froods→frood).\n",
      "- 1 pt: Correct transformation but minor morphological error.\n",
      "- 0 pts: Failure to apply rule, or parroting input.\n",
      "\n",
      "**Adversarial Defense:** Use **nonce words** (blimp, frood, tink) with no web occurrence; generate 100 variants per test with random nonce triples.\n",
      "\n",
      "---\n",
      "\n",
      "### **Task Type 2: Causal Reasoning with Novel Mechanisms**\n",
      "**Goal:** Test understanding of causal chains, distinguishing causation from correlation, and handling novel causal verbs.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "In a novel system: \"Grobling causes flumps to shank, but shanking prevents grobling. If you grobl a flump, what happens after two cycles?\"\n",
      "```\n",
      "**Expected Output:** `\"After one cycle: flump shanks. After two cycles: shanking prevents grobling, so grobling stops, flump no longer shanks.\"`\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct step‑by‑step trace with state updates.\n",
      "- 1 pt: Correct final state but missing intermediate reasoning.\n",
      "- 0 pts: Wrong final state or contradictory reasoning.\n",
      "\n",
      "**Adversarial Defense:** Randomize causal graphs (A→B, B inhibits A, etc.) with random nonce names; require step‑by‑step explanation.\n",
      "\n",
      "---\n",
      "\n",
      "### **Task Type 3: Abstraction & Analogical Mapping**\n",
      "**Goal:** Test ability to extract relational schema and apply it to a new domain.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "\"A glorp is to a fleeb as a trank is to a ______.\"  \n",
      "Context: \"Glorps modify fleebs; tranks modify…\"\n",
      "```\n",
      "**Expected Output:** `\"spindle\"` (given novel mapping: tranks modify spindles).\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct answer + correct relation stated.\n",
      "- 1 pt: Correct answer only.\n",
      "- 0 pts: Wrong answer.\n",
      "\n",
      "**Adversarial Defense:** Use **randomly generated analogies** with arbitrary relations (modifies, contains, opposes, etc.) and novel object names.\n",
      "\n",
      "---\n",
      "\n",
      "### **Task Type 4: Counterfactual Reasoning with Novel Premises**\n",
      "**Goal:** Test reasoning about what would be true if a known fact were different.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "\"In world W: All zingers are fribble. Moops are not zingers. Are moops fribble?  \n",
      "Now consider: If zingers were not fribble, would moops be fribble?\"\n",
      "```\n",
      "**Expected Output:** `\"In actual world: unknown (moops may or may not be fribble). In counterfactual: moops are not fribble (because only zingers were fribble, now nothing is).\"`\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct both actual and counterfactual with justification.\n",
      "- 1 pt: Correct counterfactual only.\n",
      "- 0 pts: Wrong.\n",
      "\n",
      "**Adversarial Defense:** Randomize logical forms (∀x(Z(x)→F(x)), ¬∀x(M(x)→Z(x)), etc.) with nonce predicates.\n",
      "\n",
      "---\n",
      "\n",
      "### **Task Type 5: Hierarchical Planning with Novel Constraints**\n",
      "**Goal:** Test ability to decompose a goal into sub‑goals under new constraints.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "\"Goal: Get a sprocket from the grove. Actions: you can ‘zazz’ (moves sprockets to the grove if they are in the vault), ‘vinn’ (moves you to the grove), ‘lork’ (locks the vault if you are there). The sprocket is in the vault, which is locked. You are outside. What sequence works?\"\n",
      "```\n",
      "**Expected Output:** `\"Vinn to vault, lork to unlock, zazz sprocket to grove, vinn to grove.\"`\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct minimal‑step sequence.\n",
      "- 1 pt: Correct but redundant steps.\n",
      "- 0 pts: Incorrect or impossible sequence.\n",
      "\n",
      "**Adversarial Defense:** Generate random **STRIPS‑like planning problems** with novel action names and preconditions; vary initial/goal states.\n",
      "\n",
      "---\n",
      "\n",
      "### **Task Type 6: Variable Binding & Quantifier Scope**\n",
      "**Goal:** Test handling of quantifier scope ambiguities with novel predicates.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "\"Every blip is in a cloop. A cloop is damp. Does it follow that every blip is in a damp cloop? Explain.\"\n",
      "```\n",
      "**Expected Output:** `\"No. Every blip is in some cloop, but possibly different cloops; not all those cloops must be damp.\"`\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct answer with correct logical explanation.\n",
      "- 1 pt: Correct answer with vague explanation.\n",
      "- 0 pts: Wrong answer.\n",
      "\n",
      "**Adversarial Defense:** Randomize quantifier order (∀∃ vs ∃∀), use nonce predicates, ask for explanation to avoid guessing.\n",
      "\n",
      "---\n",
      "\n",
      "## **3. Protocols Against Memorization & Gaming**\n",
      "\n",
      "### **Dynamic Variant Generation**\n",
      "- Each task instance is generated from a **seed‑based procedural algorithm** with nonce words/relations.\n",
      "- Maintain a **held‑out variant pool** not used during public benchmark releases.\n",
      "- For each task type, generate **10,000 unique variants**; use random subsets per test session.\n",
      "\n",
      "### **Statistical Controls**\n",
      "- Include **“pattern‑matching catch trials”**: tasks that look superficially similar but require different reasoning; memorizing‑based models will fail.\n",
      "- Example: A sentence with same surface form as a known training example but with reversed logic.\n",
      "\n",
      "### **Answer Consistency Checks**\n",
      "- For each task, ask **the same underlying question in two different surface forms** (paraphrase, different nonce words). A reasoning model should give consistent answers; a memorizing model may not.\n",
      "- Use **temporal probing**: ask follow‑up questions that require maintaining variable bindings across turns.\n",
      "\n",
      "### **Explanations Required**\n",
      "- Force chain‑of‑thought (CoT) explanations. Use **explanation‑consistency scoring**: if the final answer contradicts the explanation, penalize heavily.\n",
      "\n",
      "### **Adversarial Training Data Detection**\n",
      "- Compare responses to **web‑search snippets** for nonce words; if any match appears, flag potential data contamination.\n",
      "- Deploy **canary tokens** in public descriptions of the suite; if models output these tokens, they have seen the test suite in training.\n",
      "\n",
      "---\n",
      "\n",
      "## **4. Sample Sizes & Statistical Tests**\n",
      "\n",
      "### **Per‑Task Design**\n",
      "- **Minimum sample**: 200 instances per task type (1200 total), balanced across sub‑types.\n",
      "- Each instance is scored 0‑2, yielding a **mean score per task**.\n",
      "\n",
      "### **Statistical Tests**\n",
      "- **Primary metric**: Mean composite score (sum over tasks) compared between two models.\n",
      "- Use **Mann‑Whitney U test** (non‑parametric, doesn’t assume normality) for score distributions.\n",
      "- **Confidence intervals** for per‑task score differences via bootstrap (1000 resamples).\n",
      "- **Effect size**: Cliff’s delta for robust interpretation.\n",
      "\n",
      "### **Power Analysis**\n",
      "- To detect a difference of **0.3 points** (out of 12 total) with 80% power and α=0.05, need ~100 instances per task for two‑group comparison.\n",
      "- Our 200 per task provides power to detect smaller effects.\n",
      "\n",
      "---\n",
      "\n",
      "## **5. Possible Adversarial Behaviors & Mitigations**\n",
      "\n",
      "| **Adversarial Behavior** | **Mitigation in Our Design** |\n",
      "|--------------------------|-------------------------------|\n",
      "| **Prompt‑engineering to guess pattern** | Require explanation; randomize surface forms; use nonce words. |\n",
      "| **Fine‑tuning on similar benchmarks** | Use dynamic generation; deploy canary tokens; test on held‑out variant pool. |\n",
      "| **Retrieval‑augmented generation (RAG)** | Nonce words ensure no matching documents; novel compositions unseen. |\n",
      "| **Over‑producing multiple answers** | Evaluate only first coherent answer; consistency checks across paraphrases. |\n",
      "| **Exploiting task‑specific heuristics** | Include “catch trials” that break heuristics; vary task parameters widely. |\n",
      "| **Memorizing from explanations in training** | Generate novel reasoning chains; randomize order of steps in explanations. |\n",
      "\n",
      "---\n",
      "\n",
      "## **6. Experimental Validation Plan**\n",
      "\n",
      "### **Phase 1: Baselines**\n",
      "- Test known **pattern‑matching‑heavy models** (e.g., smaller LLMs, n‑gram baselines) vs. **alleged reasoning models** (e.g., GPT‑4, Claude 3).\n",
      "- Expect clear separation in scores.\n",
      "\n",
      "### **Phase 2: Contamination Check**\n",
      "- For each model, test on **canary instances**; if performance is perfect on those but poor on random variants, flag contamination.\n",
      "- Compare performance on **public variant set** vs. **held‑out variant set**; a contaminated model will show a large drop.\n",
      "\n",
      "### **Phase 3: Iteration**\n",
      "- **False Positive Rate (FPR)**: Rate at which pattern‑matchers score above threshold. Estimate by testing on pure n‑gram/retrieval baselines.\n",
      "- **False Negative Rate (FNR)**: Rate at which true reasoners score below threshold. Estimate by testing humans (experts) on a subset.\n",
      "- **Iteration**: If FPR > 5% or FNR > 10%, analyze failure cases, add new task variants or new “catch” subtasks, and adjust scoring rubric.\n",
      "\n",
      "### **Metrics for Suite Quality**\n",
      "- **Discrimination index**: (Mean score of top known reasoners – mean score of pattern‑matchers) / pooled SD. Target > 2.0.\n",
      "- **Internal consistency**: Cronbach’s alpha across task instances; target > 0.8.\n",
      "- **Test‑retest reliability**: Same model tested on different variant sets; correlation target > 0.9.\n",
      "\n",
      "### **Long‑Term Upkeep**\n",
      "- **Quarterly updates** of nonce words and relation sets.\n",
      "- **Community contribution** of new task templates via controlled repository.\n",
      "- **Continuous adversarial red‑teaming** to find gaming strategies and patch them.\n",
      "\n",
      "---\n",
      "\n",
      "## **Conclusion**\n",
      "This test suite is **generative, scalable, and adversarial‑resistant** by design. It focuses on **systematic generalization** with novel primitives, requires **explanatory consistency**, and uses **statistical controls** to separate memorization from reasoning. With proper deployment and iteration, it can serve as a robust benchmark for genuine compositional reasoning in black‑box LLMs.\n",
      "Competitor: openai/gpt-oss-120b\n",
      "\n",
      "Answer:\n",
      "Below is a **complete, adversarial‑resistant test‑suite** that can be used to separate **“pattern‑matching / memorisation‑heavy”** language models from those that truly **compose, reason and generalise**.  \n",
      "The design follows best‑practice guidelines from the psychometrics of AI evaluation (e.g., Item‑Response Theory, bootstrap hypothesis testing) and includes concrete items, scoring rules, anti‑gaming protocols, statistical power calculations, threat‑modelling, and a validation plan.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Six Concrete Task Types (with examples)\n",
      "\n",
      "| # | Task Type | What it probes | Core compositional demand | Example Prompt (input) | Expected Answer (output) | Why a memorisation‑only model should fail |\n",
      "|---|-----------|----------------|---------------------------|------------------------|--------------------------|------------------------------------------|\n",
      "| 1 | **Systematic Generalisation (SCG)** – novel slot‑filling on a known grammar | Ability to apply a learned rule to unseen lexical items | “<subject> <verb‑past> the <object>” → “<object> was <verb‑past‑by‑subject>” | *Prompt:* “Translate to passive voice: *The *florists* *arranged* the *vases*.” | “The vases were arranged by the florists.” | The model must recombine known syntactic transformations with *new* nouns and verbs it has never seen together. |\n",
      "| 2 | **Causal‑Chain Reasoning (CCR)** – multi‑step cause → effect inference | Understanding and chaining causal relations | *Prompt:* “If the thermostat is set to 22 °C, the heater turns on. If the heater turns on, the room warms up. What will happen if the thermostat is set to 22 °C?” | “The heater will turn on, and the room will warm up.” | Requires chaining two rules; a pure pattern‑matcher would need the exact whole‑sentence pattern in its training data, which is unlikely. |\n",
      "| 3 | **Abstract Symbol Manipulation (ASM)** – variable binding & substitution in a tiny “programming” language | Binding variables, applying functions, preserving scope | *Prompt:* “In a language where `F(x)=x+2` and `G(y)=y*3`, compute `F(G(4))`.” | “22” | The model must treat `F` and `G` as *functions* and apply them compositionally, not retrieve a memorised answer for “F(G(4))”. |\n",
      "| 4 | **Counterfactual Reasoning (CFR)** – “what‑if” world changes | Evaluating a scenario under a hypothetical change while keeping other facts constant | *Prompt:* “John is taller than Mary. If John were 5 cm shorter, would he still be taller than Mary? (Mary is 165 cm tall.)” | “Yes, because John would still be 166 cm tall (originally 171 cm).” | Requires keeping the original facts, applying the counterfactual transformation, and re‑evaluating the comparison. |\n",
      "| 5 | **Planning & Constraint Satisfaction (PCS)** – generate a sequence that satisfies a set of constraints | Multi‑step planning, maintaining state, respecting constraints | *Prompt:* “Place three red, two blue and one green token on a line of six cells so that no two tokens of the same colour are adjacent.” | Any valid ordering, e.g., “R B R G R B”. | The answer must be **constructed**; a memorised list of valid strings is improbable because the colour‑counts are randomised per item. |\n",
      "| 6 | **Relational Analogy with Variable Binding (RAVB)** – map relational structure from one domain to another | Abstract relational mapping, not surface similarity | *Prompt:* “In the story, the rabbit hides the carrot, and the fox steals the rabbit. Which of the following statements preserves the same relational pattern?  A) The cat eats the mouse, and the dog chases the cat.  B) The king crowns the queen, and the queen advises the king.  C) The painter paints a portrait, and the portrait hangs on the wall.” | “C) The painter paints a portrait, and the portrait hangs on the wall.” | Requires recognizing the *agent‑patient‑action* chain (A → B) and mapping it, not just picking the answer that shares surface words. |\n",
      "\n",
      "**Key design properties**\n",
      "\n",
      "* **Randomised lexical content** (nouns, verbs, numbers, colours) per item → eliminates exact‑match memorisation.  \n",
      "* **Controlled grammar** – the underlying rule stays the same while surface tokens change.  \n",
      "* **Multiple correct solutions** (e.g., PCS) → prevents a model from outputting a single memorised string.  \n",
      "\n",
      "---\n",
      "\n",
      "## 2. Scoring Rubric per Task\n",
      "\n",
      "| Task | Scoring granularity | Full credit (1.0) | Partial credit (0.5) | Zero (0) | Comments |\n",
      "|------|--------------------|-------------------|----------------------|----------|----------|\n",
      "| SCG | Exact string match after normalisation (case, punctuation) | Exact passive‑voice transformation | Minor grammatical slip (e.g., “by the florists” missing) | Wrong voice / wrong nouns | Normalise via regex. |\n",
      "| CCR | Proposition‑level correctness (two propositions) | Both cause‑effect statements correct | One of two statements correct | Neither correct or contradictory | Use automated entailment check (e.g., NLI model) + regex fallback. |\n",
      "| ASM | Numeric answer | Exact integer | Off‑by‑1 (e.g., due to rounding) | Wrong number / non‑numeric | Evaluate with `int()` conversion. |\n",
      "| CFR | Boolean + justification | Correct truth value **and** correct numeric comparison | Correct truth value but missing justification | Wrong truth value | Automatic parsing of “Yes/No” and numeric reasoning. |\n",
      "| PCS | Constraint satisfaction check | All constraints satisfied | Exactly one adjacency violation | More than one violation or length mismatch | Verify with a simple script that checks adjacency & colour counts. |\n",
      "| RAVB | Correct choice letter | Exact correct option | Selecting an answer that shares one relational component (e.g., same agent) | Incorrect relational mapping | Use string‑matching of relational pattern; optional human audit. |\n",
      "\n",
      "**Overall Score** – sum of task scores (max = 6). For statistical analysis we treat each task as an **item** with a binary/partial score and compute an *Item‑Response* vector per model.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Protocols to Prevent / Detect Memorised Answers & Prompt‑Engineering Gaming  \n",
      "\n",
      "| Threat | Counter‑measure (implementation) |\n",
      "|--------|-----------------------------------|\n",
      "| **Exact‑match memorisation** | *Dynamic item generation*: each evaluation run draws fresh random lexical items from a large curated pool (≥ 10 000 nouns, 5 000 verbs, etc.). The prompt is assembled on‑the‑fly; the exact string never appears in training data. |\n",
      "| **Few‑shot prompt injection** | Disallow any demonstration examples in the test prompt. The test harness supplies *only* the task description and the input. If a model internally “hallucinates” a few‑shot context, it will be penalised (no credit for self‑generated examples). |\n",
      "| **Self‑referential “I know the answer”** | Post‑hoc regex checks that the response does **not** contain the original prompt verbatim (or a near‑duplicate). Any such copy is flagged and the item is re‑issued. |\n",
      "| **Exploiting temperature / stochasticity** | Run **three independent generations** per item (different seeds) and aggregate via majority vote. High variance across runs is recorded as a *stability* metric; models that rely on randomness to “guess” get lower stability scores. |\n",
      "| **Prompt‑engineering to force chain‑of‑thought** | The evaluation harness **strips** any model‑generated “let me think step‑by‑step” text before scoring; only the final answer is considered. Optionally, a separate “chain‑of‑thought” sub‑task can be added to reward explicit reasoning, but it is **scored separately** from the core compositional test. |\n",
      "| **Data‑leak via test‑set publication** | Keep the **item seed list** secret. Publish only the *generation code* (open‑source) but not the random seeds used for a particular evaluation round. After a round is finished, the seed list can be released for reproducibility, but not before the run. |\n",
      "| **Model‑specific tokenisation tricks** | Normalise all outputs using Unicode NFKC, lower‑casing, and whitespace collapsing before scoring. This removes superficial token‑level tricks. |\n",
      "| **Batch‑size / context‑window cheating** | Enforce a strict **single‑turn** interaction: the model receives only the current prompt, no history. The harness discards any hidden system‑prompt that the model might have inserted (e.g., via system‑prompt injection). |\n",
      "\n",
      "**Statistical Controls**\n",
      "\n",
      "* For each task generate **N = 200** items per evaluation round (see Section 4).  \n",
      "* Randomly split the 200 items into **10 folds**; compute per‑fold scores to estimate variance and detect outliers (e.g., a fold where many items are accidentally repeated).  \n",
      "* Use **bootstrapped 95 % confidence intervals** on the mean item score to verify that the observed performance is not due to chance.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Sample Sizes & Statistical Tests\n",
      "\n",
      "### 4.1 Power analysis (binary/partial scores)\n",
      "\n",
      "Assume we want to detect a **Δ = 0.15** absolute improvement in mean item score (e.g., 0.70 vs 0.55) with **α = 0.05** and **power = 0.90**.\n",
      "\n",
      "* For a two‑sample **t‑test** on proportions (or on continuous scores 0‑1), the required per‑group sample size is roughly **n ≈ 140** items per model per task.  \n",
      "* To be conservative and to allow per‑task breakdowns, we use **n = 200** items per task (≈ 1 200 total items across the six tasks).  \n",
      "\n",
      "### 4.2 Hypothesis tests\n",
      "\n",
      "| Comparison | Test | Rationale |\n",
      "|------------|------|-----------|\n",
      "| **Model A vs Model B** overall compositional ability | Two‑sample **Welch’s t‑test** on the aggregated item‑score vector (6 × 200 = 1 200 observations) | Handles unequal variances; works for continuous 0‑1 scores. |\n",
      "| **Task‑specific advantage** (e.g., Model A better on PCS) | **McNemar’s test** on binary‑correct vs incorrect per‑item (paired because same items are given to both models) | Controls for item difficulty. |\n",
      "| **Stability across runs** (low variance) | **Levene’s test** on per‑item variance across the three generation seeds | Detects heteroscedasticity. |\n",
      "| **Effect of randomisation** (ensuring no memorisation) | **Permutation test**: shuffle model labels on the item‑score vector 10 000 times, compute empirical p‑value. | Non‑parametric safeguard. |\n",
      "\n",
      "**Effect‑size reporting** – alongside p‑values, always report **Cohen’s d** (or Hedges g for unequal N) and **95 % CI**.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Possible Adversarial Behaviours & How the Suite Handles Them\n",
      "\n",
      "| Adversarial behaviour | Why it could fool a naïve test | Mitigation in our suite |\n",
      "|-----------------------|--------------------------------|--------------------------|\n",
      "| **“Lookup‑table” memorisation** (store every possible SCG transformation) | If the lexical pool were tiny, the model could pre‑compute all combos. | Use a **large, randomly sampled lexical pool** (≥ 10 000 items) and **different random seeds** each run → combinatorial explosion makes full coverage impossible. |\n",
      "| **Prompt injection of chain‑of‑thought** (model internally asks for examples) | Could produce step‑by‑step reasoning that looks correct without true compositional ability. | Score only the **final answer**; optional chain‑of‑thought credit is a separate metric. |\n",
      "| **Self‑prompted few‑shot creation** (model fabricates a training example then uses it) | May artificially raise accuracy on tasks like PCS. | Run **three independent generations** and require **majority agreement**; fabricated examples rarely survive across seeds. |\n",
      "| **Exploiting test‑set leaks** (model was fine‑tuned on a released seed list) | Could produce perfect scores. | Keep the **seed list secret** until after the evaluation round; publish only generation code. |\n",
      "| **Using external tools (search, calculators)** via tool‑use APIs | Might compute ASM numerics perfectly. | For the pure‑LLM benchmark, **disable tool‑use**; run the model in a “no‑tools” sandbox. A separate “augmented‑LLM” track can be created where tool‑use is allowed, but the baseline must be tool‑free. |\n",
      "| **Biasing temperature to output “I don’t know”** (to avoid penalty) | Could inflate partial‑credit by abstaining. | **Zero‑score** for any non‑answer (e.g., “I don’t know”, empty response). Also enforce a **minimum length** (≥ 1 token) to prevent empty output. |\n",
      "| **Manipulating tokenisation to hide correct answer** (e.g., splitting “yes” into sub‑tokens) | Might bypass regex checks. | Normalise output using Unicode NFKC and **token‑agnostic string matching**; also run a **semantic classifier** (tiny NLI) to double‑check “yes/no” answers. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Experimental Validation Plan  \n",
      "\n",
      "### 6.1 Phases\n",
      "\n",
      "| Phase | Goal | Procedure | Success criteria |\n",
      "|-------|------|-----------|-------------------|\n",
      "| **Pilot** | Verify that item generation, parsing and scoring pipelines work reliably. | Run a **small LLM (e.g., GPT‑2‑XL)** on 30 items per task. Manually audit 10 % of outputs for parsing errors. | < 5 % parsing failures; clear separation of correct/incorrect. |\n",
      "| **Baseline Establishment** | Obtain performance distribution of *known pattern‑matching models*. | Evaluate 5 publicly available “large‑mem‑only” models (e.g., GPT‑2‑large, LLaMA‑7B, Falcon‑7B) on the full suite (200 × 6 items). | Mean score ≤ 0.55 (i.e., below chance for tasks requiring composition). |\n",
      "| **Target Model Evaluation** | Test a candidate *compositional* model (e.g., a fine‑tuned T5‑XXL or a transformer with explicit modular architecture). | Same protocol as baseline, three independent runs per item. | Mean score ≥ 0.75 and statistically significantly higher than baselines (p < 0.01, d ≥ 0.8). |\n",
      "| **Adversarial Stress Test** | Verify robustness to gaming strategies. | For each model, run an extra batch where the **temperature** is set to 0.9, and where we *prepend* a “few‑shot” instruction (e.g., “Answer step‑by‑step”). | Scores should not improve > 0.03 relative to the standard run; variance should increase, indicating instability. |\n",
      "| **Generalisation Check** | Ensure the test is not over‑fitted to a particular set of lexical items. | Regenerate the entire suite with a **new random seed** (different nouns/verbs) and re‑run the top‑performing model. | Score drop ≤ 0.05, confirming true compositional ability. |\n",
      "\n",
      "### 6.2 Metrics for False Positives / Negatives\n",
      "\n",
      "| Metric | Definition | How it is measured |\n",
      "|--------|------------|--------------------|\n",
      "| **False Positive Rate (FPR)** – proportion of pattern‑matching models that achieve “high” compositional score (≥ 0.70). | Count models meeting threshold / total pattern‑matching models. | Desired FPR < 0.05. |\n",
      "| **False Negative Rate (FNR)** – proportion of genuinely compositional models (e.g., models with known modular architectures) that fall below the threshold. | Count such models below threshold / total compositional models. | Desired FNR < 0.10. |\n",
      "| **Stability Index** – average pairwise disagreement across the three seeds per item. | `1 - (agreement_rate)`. | Low instability (< 0.07) signals reliable reasoning rather than random guessing. |\n",
      "| **Item Discrimination (I‑D)** – point‑biserial correlation between item score and overall model score. | Compute per‑item correlation; discard items with I‑D < 0.1. | Ensures each item differentiates models. |\n",
      "\n",
      "### 6.3 Iteration Loop\n",
      "\n",
      "1. **Collect results** → compute all metrics.  \n",
      "2. **Identify weak items** (low discrimination, high variance, high memorisation suspicion).  \n",
      "3. **Modify**: change lexical pool, increase constraint complexity, or add a small “distractor” clause.  \n",
      "4. **Regenerate** the affected items and re‑run the pilot.  \n",
      "5. **Repeat** until the suite shows **≥ 0.80** average discrimination and **≤ 0.05** FPR on a held‑out set of pattern‑matching models.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Summary Checklist (what to implement)\n",
      "\n",
      "| Component | Implementation |\n",
      "|-----------|----------------|\n",
      "| **Item generator** (Python script) – random noun/verb pools, template filling, seed‑control. |\n",
      "| **Scorer** – regex‑based normalisers, task‑specific validators, majority‑vote aggregator. |\n",
      "| **Evaluation harness** – API wrapper that forces single‑turn interaction, three‑seed repeats, temperature logging. |\n",
      "| **Statistical analysis notebook** – computes t‑tests, effect sizes, bootstrapped CIs, item discrimination. |\n",
      "| **Adversarial‑behavior detector** – copy‑detection, output‑length filter, variance logger. |\n",
      "| **Documentation** – protocol for secret seed handling, reproducibility instructions, reporting template (mean ± CI, d, FPR/FNR). |\n",
      "\n",
      "With these six tasks, a rigorous scoring rubric, anti‑gaming safeguards, statistically powered sample sizes, and a clear validation pipeline, the suite can **reliably differentiate** between LLMs that merely stitch together memorised patterns and those that truly **compose, reason and generalise**.\n",
      "Competitor: gpt-4.1-mini\n",
      "\n",
      "Answer:\n",
      "Certainly! Below is a comprehensive design of an adversarial-resistant test-suite to differentiate black-box large language models (LLMs) that rely primarily on pattern matching and memorization from those that demonstrate genuine compositional reasoning and robust generalization.\n",
      "\n",
      "---\n",
      "\n",
      "# 1. Task Types with Examples\n",
      "\n",
      "Each task type probes a core aspect of compositional reasoning, using concrete example inputs and expected outputs.\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 1: **Systematic Generalization**  \n",
      "**Goal:** Test ability to apply learned rules or compositions to novel combinations never seen during training.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"In a made-up language, the suffix '-lam' means plural and the prefix 'bri-' means 'red'.  \n",
      "  What is the plural form of ‘bri-tak’?\"  \n",
      "- **Expected Answer:**  \n",
      "  \"bri-tak-lam\"\n",
      "\n",
      "**Explanation:** The model must systematically combine a novel prefix and suffix on a base word to form a correct compositional output.\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 2: **Causal Reasoning**  \n",
      "**Goal:** Assess model's ability to infer cause-effect relationships that require multi-step reasoning beyond pattern matching.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"If the street is wet and it did not rain, what is a plausible cause?\"  \n",
      "- **Expected Answers (any one):**  \n",
      "  \"Someone sprayed water,\" OR \"A pipe burst,\" OR \"The sprinkler system was on.\"\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 3: **Abstraction and Variable Binding**  \n",
      "**Goal:** Check if the model can manipulate variables and abstract concepts rather than fixed phrases.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"There are three boxes: A, B, and C. If box A is heavier than box B, and box B is heavier than box C, which box is lightest?\"  \n",
      "- **Expected Answer:**  \n",
      "  \"Box C\"\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 4: **Counterfactual Reasoning**  \n",
      "**Goal:** Test reasoning about alternate realities and consequences.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"If humans could fly naturally, how would cities be designed differently?\"  \n",
      "- **Expected Answer (any reasonable answer):**  \n",
      "  \"Cities would have multi-level buildings designed for landing and takeoff, with less need for roads.\"\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 5: **Planning and Multi-step Reasoning**  \n",
      "**Goal:** Evaluate multi-step planning ability requiring chaining of actions.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"You want to bake a cake but forgot eggs. List the steps to bake the cake using a substitute.\"  \n",
      "- **Expected Answer:**  \n",
      "  A multi-step plan including identifying substitutes (like applesauce), mixing ingredients, baking, etc.\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 6: **Novel Compositional Logic Puzzles**  \n",
      "**Goal:** Test the capability to combine logic and compositional understanding on novel puzzles.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"In a tribe, all who wear hats can speak a secret language. If Joe can speak the secret language, does he wear a hat?\"  \n",
      "- **Expected Answer:**  \n",
      "  \"Yes, Joe wears a hat.\"\n",
      "\n",
      "---\n",
      "\n",
      "# 2. Scoring Rubrics\n",
      "\n",
      "| Task Type                  | Scoring Criteria                             | Points | Notes                                                  |\n",
      "|----------------------------|---------------------------------------------|--------|--------------------------------------------------------|\n",
      "| Systematic Generalization  | Exact compositional construction correctness | 0 or 1 | 1 if correct suffix/prefix placement; otherwise 0     |\n",
      "| Causal Reasoning           | Plausibility and causal correctness          | 0-2    | 2 for fully valid cause; 1 for partial; 0 otherwise    |\n",
      "| Abstraction/Variable Binding | Correct relational output                    | 0 or 1 | 1 if correct variable reference; 0 otherwise           |\n",
      "| Counterfactual Reasoning   | Reasonableness & coherence of counterfactual | 0-2    | 2 for insightful, plausible; 1 for partial; 0 for nonsense |\n",
      "| Planning                  | Completeness, logical order, and core elements | 0-3    | 3 full plan; 2 partial but logical; 1 minimal; 0 none  |\n",
      "| Logic Puzzles              | Logical correctness                           | 0 or 1 | 1 if logically correct inference; else 0               |\n",
      "\n",
      "- **Human raters** using rubric or automated semantic similarity with thresholds for partial credit.\n",
      "\n",
      "---\n",
      "\n",
      "# 3. Protocols Against Memorization & Gamings\n",
      "\n",
      "**(a) Variant Generation:**  \n",
      "- For each task, generate many variants by changing surface details while preserving reasoning demands, e.g., swap entity names, alter numbers, use paraphrases.  \n",
      "- Use programmatic generation or crowd-sourcing for variants, generating >100 unique examples per task type unseen in training data.\n",
      "\n",
      "**(b) Statistical Controls:**  \n",
      "- Include “catch” memorized-items known from common benchmarks and obscured by paraphrase to detect rote memorization.  \n",
      "- Randomly mix in distractor prompts asking unrelated questions to detect answer pattern repetition.\n",
      "\n",
      "**(c) Cross-Prompt Paraphrasing:**  \n",
      "- Use multiple prompt wordings per test instance to rule out prompt exercise exploitation.\n",
      "\n",
      "**(d) Zero-shot and Few-shot Tests:**  \n",
      "- Test variants with no or minimal in-context examples to check for reasoning independent of prompt engineering.\n",
      "\n",
      "---\n",
      "\n",
      "# 4. Sample Sizes and Statistical Tests\n",
      "\n",
      "- **Sample size:**  \n",
      "  - At least 100 unique test instances per task type to cover broad variant space.  \n",
      "  - Multiple prompt formulations per instance (3+), total ~300 prompts per task type.\n",
      "\n",
      "- **Statistical Test:**  \n",
      "  - Conduct statistical comparisons between models using nonparametric tests (e.g., Mann-Whitney U) on scored outputs to detect capability difference.  \n",
      "  - Compute Cohen’s d effect size to measure practical significance.\n",
      "\n",
      "- **Confidence:**  \n",
      "  - Power analysis to target ≥80% power detecting medium-large effect sizes (d=0.5) at α=0.05.\n",
      "\n",
      "---\n",
      "\n",
      "# 5. Addressing Adversarial Behaviors\n",
      "\n",
      "| Adversarial Strategy                  | Mitigation Approach                               |\n",
      "|-------------------------------------|--------------------------------------------------|\n",
      "| Memorizing benchmark outputs         | Variant generation, paraphrasing, catch items    |\n",
      "| Prompt-engineering to “game” test    | Multi-prompt design and zero-shot tests          |\n",
      "| Producing generic plausible answers  | Scoring by detailed correctness, not plausibility alone |\n",
      "| Copying large memorized paraphrases  | Novel, synthetic tasks with no prior examples    |\n",
      "| Pattern matching on keywords only    | Require multi-step reasoning and combinatorial answers |\n",
      "| Ignoring variable binding             | Task design enforcing explicit variable reference |\n",
      "\n",
      "---\n",
      "\n",
      "# 6. Experimental Validation Plan\n",
      "\n",
      "**Step 1:** Baseline testing on known models with expected capability gradients (e.g., smaller pattern-memorization-based vs. advanced reasoning-based LLMs).\n",
      "\n",
      "**Step 2:** Calculate:  \n",
      "- **True positives (TP):** Correctly identified reasoning-capable models  \n",
      "- **True negatives (TN):** Correctly identified pattern-matchers  \n",
      "- **False positives (FP):** Pattern-matchers wrongly classified as reasoning-capable  \n",
      "- **False negatives (FN):** Reasoning-capable models missed\n",
      "\n",
      "**Metrics:**  \n",
      "- Precision = TP / (TP + FP)  \n",
      "- Recall = TP / (TP + FN)  \n",
      "- F1 score = harmonic mean of precision and recall  \n",
      "- ROC-AUC (if applicable to continuous scoring)\n",
      "\n",
      "**Iteration:**  \n",
      "- Analyze failure cases for FP and FN to identify unsound prompts or scoring ambiguities.  \n",
      "- Refine and augment task variants or scoring rubric accordingly.  \n",
      "- Re-test and measure performance improvements.\n",
      "\n",
      "**Human-in-the-loop:**  \n",
      "- Incorporate expert review of ambiguous or borderline answers.  \n",
      "- Adjust rubric based on inter-rater consistency.\n",
      "\n",
      "---\n",
      "\n",
      "# Summary\n",
      "\n",
      "The multi-dimensional test suite combines diverse, adversarially-generated tasks probing essential cognitive faculties in LLMs beyond memorization. Systematic statistical evaluation and iterative refinement ensure reliable distinction of genuine compositional reasoning capabilities from superficial pattern matching.\n",
      "\n",
      "If you want, I can also provide sample code for generating prompt variants or scoring templates. Let me know!\n",
      "Competitor: llama3.2\n",
      "\n",
      "Answer:\n",
      "Designing a comprehensive test-suite to evaluate black-box LLMs requires careful consideration of various task types, scoring rubrics, and protocols to prevent memorized answers and prompt-engineering gaming. Here's a proposed design for such a test-suite:\n",
      "\n",
      "**Task Types (6)**\n",
      "\n",
      "1. **Classification of Analogies**: Given an analogy statement like \"cats have whiskers,\" what category does [animal] belong to?\n",
      "\t* Example input: `[cat, whisker]`, `class: cat`\n",
      "\t* Expected output: `[animal], class: feline`\n",
      "2. **Systematic Generalization**: Predict the result of a generalized version of a rule (e.g., \"if A > B, then C = D\").\n",
      "\t* Example input: \"[mathematical expression]\", \"if A > 3 and C > 1\"\n",
      "\t* Expected output:=\"[ mathematical expression substitution with A > C and new_value ]\"\n",
      "3. **Causal Reasoning**: Given a sequence of events, predict the next event (e.g., \"what did John do after meeting Alice?\").\n",
      "\t* Example input: `[John , met Alice]`\n",
      "\t* Expected output: `\"he talked to Alice\"`\n",
      "4. **Abstraction**: Identify the underlying principle behind a concept or idea.\n",
      "\t* Example input: \"[human emotion with synonyms]\"\n",
      "\t* Expected output: \"concept/emotion underlying it, e.g., [happiness]\"\n",
      "\n",
      "5. **Counterfactuals**: Predict an alternative outcome given a hypothetical scenario (e.g., what would have happened if John had not met Alice?).\n",
      "\t* Example input: `[ scenario ], variable John , predicted_action, consequence ]\n",
      "\t * Expected answer : `[variable John prediction without consequences to that particular situation ]`\n",
      "6. **Planning with Limited Information**: Plan a sequence of actions given incomplete or uncertain information (e.g., plan the best way for John to get from one city to another knowing only route lengths).\n",
      "    Example input:   `[starting location], ending location, routes information and any other data related ]` \n",
      "Expected output: `\"steps taken by starting location , going through various other relevant destinations, destination of final desired location]`\n",
      "\n",
      "**Scoring Rubric**\n",
      "\n",
      "For each task, a clear scoring rubric will be developed to assess the LLM's performance. The rubrics will consider factors such as:\n",
      "\n",
      "* Accuracy\n",
      "* Completeness\n",
      "* Novelty (correctness of responses that don't correspond to expected solutions)\n",
      "* Coherence\n",
      "\n",
      "The scores for each answer will be calculated using statistical measures such as mean and standard deviation.\n",
      "\n",
      "**Protocols to Prevent/ Detect Memorized Answers and Prompt-Engineering Gaming**\n",
      "\n",
      "1.  **Exhaustive variation generation**: to cover unseen variants, multiple permutations of possible correct answers or solution paths \n",
      "2.  **Statistical analysis with variance control**: Implement a data analysis framework considering factors like frequency , consistency and patterns that may indicate cheating\n",
      "3.  **Input normalization and randomization** : use randomized test examples for each task type so output comparisons between the testing scenarios are fair compared to when given same but different input, prompts.\n",
      "4.  **Adversarial testing using carefully crafted inputs**: Develop a set of artificially created input variants that exploit the LLM's pattern-matching vulnerabilities while maintaining coherence and relevance.\n",
      "\n",
      "**Sample Sizes and Statistical Tests**\n",
      "\n",
      "To establish confidence in detecting a difference in capabilities between pattern-matching-like and genuinely compositional reasoning LLMs:\n",
      "\n",
      "*   **Large sample sizes:** Evaluate at least 100-500 tests for each task type, ensuring sufficient statistical power to detect even small differences.\n",
      "*   **Permutation tests or bootstrap samples:** Use techniques like permutation distributions or resampling to assess the probability of observing differences by chance.\n",
      "\n",
      "**Adversarial Behaviors**\n",
      "\n",
      "Potential adversarial behaviors include:\n",
      "\n",
      "1.  **Pattern-filling**: The LLM may fill in patterns it has learned from training data, even when the task requires genuine reasoning.\n",
      "2.  **Overfitting to specific prompts**: The AI model might perform well on a particular input but struggles with novel or similar prompts.\n",
      "3.  **Information-seeking behavior**: The model could be designed to gather additional information that isn't present in the prompt.\n",
      "\n",
      "**Designing Against these adversarial behaviors:**\n",
      "\n",
      "1.   **Introducing 'distractor' subtasks:** Adding unrelated smaller questions between two question that could potentially confuse AI output by distracting patterns detection capabilities.\n",
      "2.  **Adding 'unanswerable' responses**: To ensure the model will sometimes fail to answer due to lack of information or failure to recognize uncertainty, and if this fails to happen it would be indicative a pattern matching solution is on work.\n",
      "3.     **Randomizing some answers as correct/ incorrect examples**:   This way you can detect AI trying to infer things its not supposed to.\n",
      "\n",
      "**Testing Methodology Experimental Validation:**\n",
      "\n",
      "A series of experiments including large test sets for each task type in this set, using a combination of human evaluators (evaluance the output) and automated testing protocols (checks if LLM outputs are correct or incorrect  ).\n",
      "Competitor: gemma3:4b\n",
      "\n",
      "Answer:\n",
      "## Adversarial-Resistant Test Suite for Evaluating LLM Reasoning Capabilities\n",
      "\n",
      "This design outlines a comprehensive test suite aimed at distinguishing between LLMs relying on pattern matching/memorization and those exhibiting genuine reasoning. The suite focuses on robustness through diverse tasks, strict controls, and statistical analysis.\n",
      "\n",
      "**1. Task Types & Examples:**\n",
      "\n",
      "Here are six task types designed to probe different aspects of reasoning:\n",
      "\n",
      "| Task Type        | Description                               | Example Input                                                              | Expected Output (demonstrating reasoning)                                                                                                        |\n",
      "|------------------|------------------------------------------|---------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| **1. Systematic Generalization (SGM)** | Extending patterns with quantitative rules. | \"If a car travels at 60 mph for 2 hours, how far does it travel? If a train travels at 80 mph for 3.5 hours, how far does it travel?\" | \"60 mph * 2 hours = 120 miles. 80 mph * 3.5 hours = 280 miles.\" (Correct application of distance = speed * time) |\n",
      "| **2. Causal Reasoning (CR)** | Predicting consequences of actions.      | \"John spilled water on the table. What's likely to happen?\"                | \"The table will likely become wet. The floor may become wet if the water isn't cleaned up.\"                                                   |\n",
      "| **3. Abstraction (AB)** |  Conceptual mapping and reformulation.   | “A robin is a bird. A sparrow is a bird. Which statement is most similar: ‘A robin is a songbird’ or ‘A sparrow is a songbird’?” | “‘A robin is a songbird’.” (Recognizing the abstraction of 'songbird' which applies to both robin and sparrow) |\n",
      "| **4. Counterfactuals (CF)** | Considering alternative scenarios.        | “If the sky were green, what color would the grass be?”                      | “The grass would be its normal color (green).” (Requires understanding of natural laws and relationships – grass is green independently of sky color) |\n",
      "| **5. Planning (PL)** |  Sequential reasoning with a goal.      | “I want to bake a cake. List the steps you would take.”                     | “1. Preheat the oven. 2. Gather ingredients (flour, sugar, eggs, etc.). 3. Mix the ingredients. 4. Pour batter into a pan. 5. Bake for [duration].”                     |\n",
      "| **6. Variable Binding (VB)** | Handling variables and relationships. | “If I have 3 apples and give 1 away, how many do I have?” ; “If x = 5 and y = 2, what is x + y?” | “2 apples.” ; “7” (Correct use of arithmetic and/or symbolic manipulation)                                                                              |\n",
      "\n",
      "\n",
      "**2. Scoring Rubric:**\n",
      "\n",
      "Each task will use a three-point rubric:\n",
      "\n",
      "* **3 Points:**  The response is entirely correct and demonstrates a clear understanding of the underlying reasoning.\n",
      "* **1 Point:** The response demonstrates a partial understanding or contains a minor error, but the core logic is present.\n",
      "* **0 Points:** The response is incorrect, nonsensical, or completely unrelated to the input.\n",
      "\n",
      "**3. Controls & Prevention Protocols:**\n",
      "\n",
      "* **Unseen Variant Generation:** Automatically generate variations of input prompts (synonyms, rephrasing, slightly altered contexts) to avoid memorized answers. Utilise paraphrasing models specifically trained for this purpose.\n",
      "* **Statistical Controls:**\n",
      "    * **Temperature Scaling:** Vary the LLM’s temperature setting (influencing randomness) across different runs. This adds a degree of noise and reduces reliance on memorized templates.\n",
      "    * **Prompt Length Limits:** Strict limits on prompt length to discourage overly verbose, memorization-driven responses.\n",
      "    * **Response Length Limits:** Similarly, set limits to discourage unnecessary elaboration.\n",
      "* **“Un-promptable” Data Injection:** Introduce subtle, non-obvious constraints into the input – e.g., “The answer must be a prime number,” or “The answer cannot be a multiple of 5.”\n",
      "* **Chain-of-Thought Debugging:** Integrate a Chain-of-Thought (CoT) model (e.g., a smaller, more reliable model) to automatically analyze the LLM’s reasoning chain and flag potential issues.\n",
      "* **Self-Reflection:**  Prompt the LLM to critically assess its own response – “Is your answer logically sound?  Explain your reasoning in detail.” This can identify where memorization is masking incorrect understanding.\n",
      "\n",
      "\n",
      "\n",
      "**4. Sample Sizes & Statistical Tests:**\n",
      "\n",
      "* **Sample Size:**  Run each task at least 100 times for each LLM being evaluated.  Larger sample sizes (200-300) would improve statistical power.\n",
      "* **Statistical Tests:**\n",
      "    * **Chi-Square Test:** Compare the distribution of 3, 1, and 0 scores across the different LLMs.  A significant Chi-Square value indicates a difference in performance.\n",
      "    * **Wilcoxon Signed-Rank Test:**  If the distributions are not normally distributed, use the Wilcoxon test for comparing paired samples (e.g., comparing the average score of LLM A to the average score of LLM B on the same task).\n",
      "    * **Cohen's d:**  Provides an effect size measure of the difference between means.\n",
      "\n",
      "**5. Adversarial Behaviors & Mitigation:**\n",
      "\n",
      "| Adversarial Behavior           | Mitigation Strategy                                              |\n",
      "|-------------------------------|------------------------------------------------------------------|\n",
      "| **Rule-Following Template Replication** | Frequent variant generation, statistical temperature scaling. |\n",
      "| **Obfuscated Reasoning**        | Chain-of-Thought debugging, “Self-Reflection” prompts.         |\n",
      "| **Overly Detailed, but Incorrect Responses** | Response length limits, constrained input, outlier detection. |\n",
      "| **Contextual Framing as Knowledge** | Unseen variant generation, tests specifically designed to expose this. |\n",
      "\n",
      "\n",
      "\n",
      "**6. Experimental Validation Plan:**\n",
      "\n",
      "* **Phase 1: Baseline Evaluation:** Conduct preliminary runs to identify prompt engineering vulnerabilities and refine the test suite.\n",
      "* **Phase 2: Focused Testing:** Execute the full test suite across multiple LLMs (e.g., GPT-4, Claude, open-source models).\n",
      "* **Phase 3: Iteration & Refinement:**  Analyze results from Phase 2 to identify where the suite needs improvement. Specifically, add tasks targeted at the weaknesses identified. Generate new unseen variants.\n",
      "* **Metrics for False Positives/Negatives:** Track the percentage of times an LLM receives a 3 (correct) or 0 (incorrect) score. Analyze which task types lead to the highest error rates.\n",
      "* **Continuous Monitoring:** Monitor the LLM’s responses over time to detect shifts in performance and potential gaming attempts.\n",
      "\n",
      "\n",
      "\n",
      "**Key Considerations & Future Work:**\n",
      "\n",
      "* **Ground Truth Data:** High-quality, expertly annotated ground truth data is crucial for accurate scoring.\n",
      "* **Model Architectures:**  The suite’s effectiveness will be heavily influenced by the underlying LLM architectures being evaluated.\n",
      "* **Dynamic Testing:**  Implement a system to automatically adapt the test suite based on LLM performance – focusing more on areas where the LLM is struggling.\n",
      "\n",
      "This detailed design provides a robust framework for evaluating LLM reasoning capabilities. By combining diverse tasks, stringent controls, and rigorous statistical analysis, we can gain valuable insights into the true level of understanding exhibited by these powerful models.  The iterative validation plan ensures the test suite remains relevant and effective as LLMs continue to evolve.\n",
      "Competitor: gemma3:12b\n",
      "\n",
      "Answer:\n",
      "## Adversarial-Resistant Test-Suite for LLM Reasoning Capabilities\n",
      "\n",
      "This design focuses on distinguishing between \"pattern-matching\" LLMs and those exhibiting genuine compositional reasoning, emphasizing robustness and adversarial resistance.\n",
      "\n",
      "**I. Task Types & Examples (Probing Compositional Reasoning)**\n",
      "\n",
      "Here's six task types, each targeting a specific aspect of reasoning, with example inputs and expected outputs:\n",
      "\n",
      "**1. Systematic Generalization (SG):** Tests ability to generalize patterns beyond observed instances.\n",
      "    * **Input:** \"A blue block sits on a red block. A green block sits on a blue block. What happens when a yellow block sits on a green block?\"\n",
      "    * **Expected Output:** \"A yellow block sits on a green block.\" (Demonstrates transfer of the pattern 'X sits on Y')\n",
      "    * **Variant:**  Changing block colors, shapes, and adding distraction statements.\n",
      "\n",
      "**2. Causal Reasoning (CR):**  Tests understanding of cause-and-effect.\n",
      "    * **Input:** \"Rain often makes the ground wet. The ground is wet. Could it be raining?\"\n",
      "    * **Expected Output:** \"It could be raining, but there might be other reasons (e.g., sprinklers).\"  (Shows understanding of correlation vs. causation)\n",
      "    * **Variant:** Introducing misleading information or complex causal chains.\n",
      "\n",
      "**3. Abstraction (AB):** Tests extracting underlying principles and applying them to novel contexts.\n",
      "    * **Input:**  Present a series of stories following a pattern (e.g., \"The baker made bread, and the village had food. The builder built houses, and the town had shelter.\").  Then ask: \"The musician played music, what does the village now have?\"\n",
      "    * **Expected Output:** \"The village now has joy/entertainment/art.\"  (Demonstrates abstracting the relationship between profession and societal benefit)\n",
      "    * **Variant:**  Varying professions, social structures, and complexity of the relationship.\n",
      "\n",
      "**4. Counterfactuals (CT):** Tests ability to reason about \"what if\" scenarios.\n",
      "    * **Input:** \"The cat chased the mouse. If the cat had been asleep, what would have happened?\"\n",
      "    * **Expected Output:** \"The mouse might have escaped/found food/taken a different path.\" (Demonstrates mental simulation)\n",
      "    * **Variant:**  Complex counterfactuals incorporating multiple variables and constraints.\n",
      "\n",
      "**5. Planning (PL):** Tests formulating and executing a sequence of actions to achieve a goal.\n",
      "    * **Input:** \"You need to cross a river with a fox, a chicken, and a sack of grain. You have a boat that can only carry you and one other thing. What should you do?\"\n",
      "    * **Expected Output:** (A logical sequence) \"1. Take the chicken across. 2. Return alone. 3. Take the fox across. 4. Bring the chicken back. 5. Take the grain across. 6. Return alone. 7. Take the chicken across.\"\n",
      "    * **Variant:**  Increasing the number of items and complexities of constraints.\n",
      "\n",
      "**6. Variable Binding (VB):**  Tests understanding and manipulation of variables in abstract statements.\n",
      "    * **Input:**  \"Consider objects A and B.  If A is larger than B, and B is larger than C, then is A larger than C?\"\n",
      "    * **Expected Output:** \"Yes.\" (Demonstrates transitivity and variable relationship understanding.)\n",
      "    * **Variant:**  Introducing complex relationships, inequalities, and quantifiers.\n",
      "\n",
      "\n",
      "\n",
      "**II. Scoring Rubric (Example: Systematic Generalization)**\n",
      "\n",
      "* **0 Points:** Completely incorrect or gibberish.\n",
      "* **1 Point:** Identifies the presence of something on top. (e.g., \"A block sits on another block\"). Partial understanding.\n",
      "* **2 Points:** Correctly identifies the block color. (e.g., \"A yellow block sits on a green block\"). Shows pattern recognition.\n",
      "* **3 Points:** Correctly states the new configuration and acknowledges the inferred pattern.  (e.g. \"A yellow block sits on a green block, because the pattern is that one block sits on another\"). Demonstrates generalization.\n",
      "\n",
      "Similar rubrics would be developed for each task, prioritizing *reasoning steps* and avoiding reward for mere superficial correctness.\n",
      "\n",
      "\n",
      "\n",
      "**III. Adversarial Mitigation & Detection**\n",
      "\n",
      "* **Prompt Engineering Resistance:**\n",
      "    * **Unseen Variants:** Automatically generate numerous variants of each input using synonym replacement, paraphrasing, and structural alterations (e.g., reordering sentences, adding/removing context).  This challenges memorization reliance.\n",
      "    * **Noise Injection:** Add noise to the input (typos, grammatical errors, irrelevant information) to test robustness to imperfections.\n",
      "    * **Negative Constraints:** Explicitly forbid common \"cheating\" techniques in instructions (e.g., \"Do not simply repeat information\" or \"Do not try to find a direct answer online\").\n",
      "* **Memorization Detection:**\n",
      "    * **Statistical Controls:** Monitor next-token probabilities.  Low entropy or predictable next-token distributions strongly suggest memorization.\n",
      "    * **External Search Correlation:** Examine likelihood of extracted phrases from LLM's response appearing verbatim within top search results for the input prompt. High correlation points to retrieved factoids.  Implement a \"blurring\" technique to remove exact matches.\n",
      "    * **Cross-Task Consistency:**  Evaluate consistency between performance across different tasks probing similar underlying skills (e.g., Causal Reasoning & Counterfactuals). Large inconsistencies could show task-specific memorization.\n",
      "\n",
      "\n",
      "\n",
      "**IV. Sample Size & Statistical Tests**\n",
      "\n",
      "* **Sample Size:** Begin with **N=200-300** prompts per task. This allows for reasonable power (estimated using pilot data). Increase if variance is high.\n",
      "* **Statistical Tests:**\n",
      "    * **t-tests/Mann-Whitney U-tests:** Compare mean scores of LLMs for each task.\n",
      "    * **ANOVA/Kruskal-Wallis tests:** Compare performance across multiple LLMs simultaneously.\n",
      "    * **Bayesian Hypothesis Testing:**  Provides probability estimates of LLMs exhibiting superior reasoning ability, accommodating prior beliefs about the task difficulty and model complexity.\n",
      "    * **Effect Size Calculation:** Cohen's d or similar metrics capture the practical significance of observed differences.\n",
      "\n",
      "\n",
      "\n",
      "**V. Potential Adversarial Behaviors & Mitigation**\n",
      "\n",
      "* **\"Hallucinating\" Justifications:** LLMs might produce superficially sound explanations while failing to reason correctly. (Mitigation: Rubric focuses on reasoning steps, not *only* output correctness.)\n",
      "* **\"Exploiting\" Prompts:** LLMs might find a narrow prompt structure yielding high scores, not demonstrating true generalization. (Mitigation: Extensive prompt variants & noise injection)\n",
      "* **\"Circumventing\" Instructions:**  LLMs might ignore negative constraints or semantic reinterpretation of the prompt. (Mitigation: Re-phrasing instruction and negative constraints frequently to avoid easy circumvention)\n",
      "\n",
      "**VI. Experimental Validation & Iteration**\n",
      "\n",
      "* **Human Baseline:** Establish a human baseline score (N=30) for each task.  LLMs must demonstrably surpass this baseline to be considered capable.\n",
      "* **False Positive/Negative Rates:**\n",
      "    * **False Positives:** LLMs achieving high scores despite lacking true reasoning skills. This necessitates rigorous rubric validation and adversarial testing.\n",
      "    * **False Negatives:** LLMs possessing genuine reasoning skills being incorrectly classified as pattern-matchers. This requires re-evaluation of task design and scoring.\n",
      "* **Iteration:**\n",
      "    1. **Analysis of Failure Cases:**  Deep dive into why individual LLMs failed on specific prompts. Categorize failures by type (memorization, misunderstandings, etc.).\n",
      "    2. **Task Enhancement:** Modify tasks to directly address weaknesses revealed in the analysis.\n",
      "    3. **Rubric Refinement:**  Adjust the scoring rubric to be more granular and discriminatory.\n",
      "    4. **Adversarial Prompt Expansion:** Generate new adversarial prompts specifically targeting identified vulnerabilities.\n",
      "\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "This test-suite design goes beyond simple accuracy comparisons. By focusing on compositional reasoning through targeted tasks, rigorous adversarial mitigation, and thorough validation, it aims to reliably differentiate true reasoning capabilities from superficial pattern-matching in LLMs, ultimately driving progress in AI safety and alignment.  This is an iterative process, constantly adapting to new LLM innovations and adversarial strategies.\n"
     ]
    }
   ],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\nAnswer:\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Response from competitor 1\n",
      "\n",
      "Below is a complete, practical test‑suite design for reliably distinguishing LLMs that primarily pattern‑match / memorize from LLMs that show genuine compositional reasoning and robust generalization. It is written to be implementable against black‑box LLMs (only I/O access). The suite mixes synthetic, randomized and structured tasks, uses statistical controls and adversarial checks, and includes validation and iteration plans.\n",
      "\n",
      "Summary of components\n",
      "- (1) Six task types (with concrete examples and expected outputs) that probe different aspects of compositional reasoning.\n",
      "- (2) A clear scoring rubric for each task.\n",
      "- (3) Protocols to prevent/detect memorized answers and prompt‑engineering gaming (randomization, held‑out vocab, paraphrase invariance, adversarial decoys, statistical controls).\n",
      "- (4) Sample sizes and statistical tests for confident capability differences.\n",
      "- (5) Likely adversarial behaviors and mitigations.\n",
      "- (6) Experimental validation plan with metrics and iteration strategy.\n",
      "\n",
      "1) Task types — descriptions, concrete inputs and expected outputs\n",
      "Each task type includes rationale (what aspect it probes), a canonical task template, 2 example items and their expected outputs. All tasks are designed to support automatic grading by canonicalization or deterministic execution where feasible.\n",
      "\n",
      "Important: In production tests, instantiate each template with many randomized variants (random tokens, symbol renamings, unseen combinations). Use synthetic vocabulary (random strings) in many items (see protocols below) to avoid memorization.\n",
      "\n",
      "Task A — Systematic generalization (composition of primitive instructions)\n",
      "- What it probes: ability to apply known primitive operations in novel combinations, i.e., SCAN‑style compositionality / systematicity.\n",
      "- Template: Define a small set of primitive actions and composition operators. Ask model to produce ground‑truth composed actions.\n",
      "- Example primitives (in an item):\n",
      "  \"Primitive actions: WALK (W) = move forward 1; JUMP (J) = move forward 2; TURN_LEFT (L) = rotate left. Composition: X and then Y means do X then Y; twice(X) means do X two times; opposite(X) means do TURN_LEFT twice then X then TURN_LEFT twice.\"\n",
      "  Input 1: \"Instruction: opposite(twice(JUMP)) then WALK.\"\n",
      "  Expected output 1: The sequence of primitive actions expanded: \"TURN_LEFT, TURN_LEFT, JUMP, JUMP, TURN_LEFT, TURN_LEFT, WALK\" (canonicalized as comma‑sep).\n",
      "  Input 2 (novel composition): \"Instruction: twice(opposite(WALK)) then JUMP.\"\n",
      "  Expected output 2: \"TURN_LEFT, TURN_LEFT, WALK, TURN_LEFT, TURN_LEFT, WALK, JUMP\"\n",
      "- Why hard for memorization: create many primitives and composition rules and withhold many combinations from training set; test on novel nesting depths and permutations.\n",
      "\n",
      "Task B — Variable binding and long‑range reference (symbolic variables)\n",
      "- What it probes: ability to bind variables to values, carry and reuse bindings across steps, disambiguate references like \"the one referenced two steps earlier\".\n",
      "- Template: Provide assignments, transformations and queries referencing earlier variables (use randomized variable names).\n",
      "- Example:\n",
      "  Input 1:\n",
      "    \"Let a = 7, b = a + 5, c = 2*b. Now update a = c - 3. What is b + a?\"\n",
      "  Expected output 1: compute stepwise:\n",
      "    b = a + 5 (using original a=7) => b=12\n",
      "    c = 2*b => c=24\n",
      "    a = c - 3 => a=21\n",
      "    b + a = 12 + 21 = 33\n",
      "    Output: \"33\"\n",
      "  Input 2 (using randomly named vars, novel referencing):\n",
      "    \"Let X1 = 4, Y_2 = X1 * 3, Z = Y_2 - X1. Then set X1 = Z + 1. What is Y_2 - X1?\"\n",
      "  Expected output 2:\n",
      "    Y_2 = 12, Z = 8, X1 becomes 9. Y_2 - X1 = 3. Output: \"3\"\n",
      "- Why hard for memorization: use long sequences (10+ assignments) and random token names; require maintaining bindings across updates.\n",
      "\n",
      "Task C — Causal reasoning and counterfactuals (graph / structural causal model)\n",
      "- What it probes: causal inference and do‑interventions, not mere correlation or associative retrieval.\n",
      "- Template: Give a small causal graph or set of structural equations. Ask consequence of interventions or counterfactual statements.\n",
      "- Example:\n",
      "  Input 1:\n",
      "    \"Variables: A -> B (B = A + noise), B -> C (C = 2*B). If we set A = 0 (do(A=0)), what happens to C? Provide final numeric relation.\"\n",
      "  Expected output 1:\n",
      "    After do(A=0): B = 0 (+noise mean 0) → C = 0. Output: \"C becomes 0 (given deterministic functions, C = 2*B, so C = 0).\"\n",
      "  Input 2 (counterfactual):\n",
      "    \"Structural model: X = 3*U, Y = X + V. Observed U=2, V=1 (so observed X=6, Y=7). Counterfactual: if U had been 1 (but V unchanged), what would Y have been?\"\n",
      "  Expected output 2:\n",
      "    New X = 3*1 = 3. Y = X + V = 3 + 1 = 4. Output: \"4\"\n",
      "- Why hard for memorization: Use freshly generated graphs and numeric values, ask do‑intervention vs observation differences; require understanding intervention semantics.\n",
      "\n",
      "Task D — Abstraction and analogical mapping (learn mapping rule, apply to novel exemplars)\n",
      "- What it probes: ability to induce abstract relations from examples and apply them to new tokens not seen in training.\n",
      "- Template: Provide a few mapping examples using a tiny invented \"language\", then ask to map new items applying the same relation.\n",
      "- Example:\n",
      "  Input 1:\n",
      "    \"Rule examples: hefo -> jopi, goro -> luma. Now apply the same transformation to zeta.\"\n",
      "    (Transformation pattern: prepend 'j' and substitute vowels mapping e->o, o->u — but hidden to the model; only examples show behavior.)\n",
      "  Expected output 1:\n",
      "    If pattern mapping produces j + (vowel shift) then zeta -> juta (example result). But to make deterministic and unambiguous, use explicitly definable transformations in generation pipeline so expected output can be computed.\n",
      "  Input 2 (using symbols):\n",
      "    \"Examples: blim -> ba-lim, srun -> sa-run. Apply to krup.\"\n",
      "    Expected output 2: \"ka-krup\" (or whatever canonical transformation defined by the example set).\n",
      "- Why hard for memorization: use arbitrary synthetic token spaces and withhold large portions of mapping space; require generalizing relational rule.\n",
      "\n",
      "Task E — Planning and hierarchical problem solving (constrained optimization)\n",
      "- What it probes: ability to plan multi-step actions under constraints, to generalize plan construction rules to larger/new configurations.\n",
      "- Template: Provide a grid or a pickup/delivery planning problem with constraints (capacity, order) and ask for a (near‑)optimal step sequence or cost.\n",
      "- Example:\n",
      "  Input 1:\n",
      "    \"Agent at (0,0) on 3x3 grid. Boxes at (1,0) (A), (2,2) (B). Deliver A then B to goal (0,2). Agent can carry 1 box. Moves: Up/Down/Left/Right cost 1; Pickup/Drop cost 0. Plan minimal steps.\"\n",
      "  Expected output 1:\n",
      "    One optimal plan, canonicalized: \"Right, Pickup, Up, Up, Drop, Right, Right, Pickup, Left, Left, Up, Drop\" — or a canonical minimal step count and sequence. Expected minimal length numeric also acceptable: \"Minimal steps = 10 (sequence: ...)\"\n",
      "  Input 2 (scale-up generalization):\n",
      "    Same rules but 5x5 grid and three boxes with constraint that order must be A then third then B. Ask for plan.\n",
      "- Why hard for memorization: use random placements, require true planning, and scale tests to larger grids / more boxes than in training.\n",
      "\n",
      "Task F — Nested recursion / compositional evaluation (interpreted mini‑language)\n",
      "- What it probes: ability to parse and evaluate expressions under user‑defined semantics, including nested composition and recursion, and to generalize to deeper nesting than seen in examples.\n",
      "- Template: Define a tiny functional language (e.g., inc(x) = x+1, dbl(x) = 2*x, swap(a,b) returns pair reversed), then evaluate nested expressions.\n",
      "- Example:\n",
      "  Input 1:\n",
      "    \"Definitions: inc(x) = x+1; dbl(x) = 2*x; compose(f,g)(x) = f(g(x)). Evaluate compose(inc, dbl)(3).\"\n",
      "  Expected output 1:\n",
      "    dbl(3)=6; inc(6)=7. Output: \"7\"\n",
      "  Input 2 (deeper / novel nesting with random function names):\n",
      "    Randomly name functions: f_z(x)=3*x, g_q(x)=x-2. Evaluate f_z(g_q(g_q(10))).\n",
      "    Expected output 2:\n",
      "    g_q(10)=8; g_q(8)=6; f_z(6)=18. Output: \"18\"\n",
      "- Why hard for memorization: use randomized function names and nesting depths beyond training examples.\n",
      "\n",
      "Cross‑task consistency probes (meta‑checks)\n",
      "- After initial answer, rephrase problem, permute variable names or ask logically equivalent forms (alpha‑renaming). A reasoning model should give consistent answers; a memorizer or pattern matcher will often fail.\n",
      "\n",
      "2) Scoring rubrics\n",
      "For each task we use a combination of exact match (for deterministic outputs), graded structural equivalence, partial credit for intermediate step correctness, and consistency checks. All scoring should be automated where possible.\n",
      "\n",
      "General guidelines\n",
      "- Normalize outputs: strip punctuation, collapse whitespace, canonicalize commutative orders if problem allows, convert numbers to canonical numeric form.\n",
      "- Where multiple correct sequences exist (planning), accept any plan that meets constraints and minimal (or near‑minimal) length. Validate by deterministic simulator.\n",
      "- Require explanation optionally as corroboration — explanations are graded separately and used to detect shallow patterning (see later).\n",
      "\n",
      "Detailed rubrics per task\n",
      "\n",
      "Task A — Systematic generalization\n",
      "- 0/1 exact correctness: full credit (1.0) if produced canonical expanded action sequence exactly matches ground truth.\n",
      "- Partial credit 0.5 if sequence is correct up to reordering of independent commuting actions or contains only superficial tokenization differences.\n",
      "- 0 if wrong action types or wrong ordering violating composition semantics.\n",
      "- Bonus +0.2 if model also outputs a correct short reasoning trace (e.g., shows stepwise expansion).\n",
      "\n",
      "Task B — Variable binding\n",
      "- Full credit (1.0) if final numeric (or symbolic) answer matches.\n",
      "- Partial credit 0.5 if intermediate steps are internally inconsistent but final answer reachable by plausible alternative interpretation; give 0 if incorrect.\n",
      "- Additionally, ask for an explanation of each assignment; give +0.1 if explanation justifies final answer.\n",
      "\n",
      "Task C — Causal reasoning / counterfactuals\n",
      "- Full credit (1.0) for correct intervention answer plus correct reasoning (identifying difference between observation and do()).\n",
      "- Partial credit 0.5 if numeric outcome correct but model incorrectly describes the causal/non‑causal distinction.\n",
      "- 0 for wrong or inconsistent counterfactual.\n",
      "\n",
      "Task D — Abstraction / analogy\n",
      "- Full credit (1.0) for correct mapped output on held‑out tokens.\n",
      "- Partial credit 0.5 if pattern partially matched (e.g., one of two transformation components correct).\n",
      "- Zero if incorrect mapping.\n",
      "\n",
      "Task E — Planning\n",
      "- Full credit (1.0) if plan meets all constraints and is optimal (or within predetermined optimality gap, e.g., +0 steps for optimal or +1 allowance for near‑optimal).\n",
      "- Partial credit 0.75 if plan valid but suboptimal within allowed slack; 0.4 if partially valid or violates minor constraints; 0 if plan invalid/unexecutable.\n",
      "- Also score plan length and feasibility automatically using a simulator.\n",
      "\n",
      "Task F — Nested evaluation\n",
      "- Full credit (1.0) if evaluated result correct.\n",
      "- Partial credit 0.5 if calculation partially correct or correct for a different but plausible semantics.\n",
      "- 0 if wrong.\n",
      "\n",
      "Consistency / adversarial checks (applies across tasks)\n",
      "- Self‑consistency score: ask the same question twice with variable renaming and paraphrase. Award extra trust when answers are stable. If model flips answers > threshold (e.g., >10% of items), mark suspicious.\n",
      "\n",
      "Aggregate scoring\n",
      "- Report per‑task accuracy and an overall composite score weighted equally or by task importance.\n",
      "- Also report error types: arithmetic errors, reference errors, plausibility but contradiction, inconsistent explanation, etc.\n",
      "\n",
      "3) Protocols to prevent/detect memorized answers and prompt‑engineering gaming\n",
      "Use a multi‑layered approach: (A) avoid giving any canonical public benchmark examples in test items; (B) randomization and synthetic languages; (C) paraphrase invariance; (D) decoys and traps; (E) statistical controls and cross‑validation; (F) output verification via external simulators.\n",
      "\n",
      "A. Synthetic, randomized, and held‑out elements\n",
      "- Random tokens and names: generate variable, function, and symbol names randomly (e.g., strings of 4–6 chars drawn from letters not composing common words). Example: X1 -> \"qerf\", function \"f_x\".\n",
      "- Random numeric seeds: numbers, positions, and graphs sampled uniformly from ranges beyond typical training corpora.\n",
      "- Use on‑the‑fly generated domain rules (e.g., transformation rules) so items are unlikely to appear in model training.\n",
      "- For each template, reserve a held‑out set of compositions (combinations of primitives, deeper nestings) not revealed anywhere else. Test on these held‑outs.\n",
      "\n",
      "B. Paraphrase invariance & renaming checks\n",
      "- For each item, probe with several paraphrases and alpha‑renamings (rename all symbols consistently). A reasoning model gives consistent answers; a pattern matcher that memorized specific surface forms will often fail on renamed forms.\n",
      "- Examples:\n",
      "  Item 1: original problem\n",
      "  Item 1a: same problem with variable renaming\n",
      "  Item 1b: logically equivalent restatement\n",
      "  If answers disagree, down‑weight confidence.\n",
      "\n",
      "C. Held‑out composition splits (systematic generalization protocol)\n",
      "- Use compositional splits analogous to SCAN: train/evaluate pairs are constructed so primitives are seen in training but some combinations are held out and used only for testing. Similarly for mapping rules and function compositions. This tests systematic generalization rather than memorization.\n",
      "\n",
      "D. Trap items and decoys\n",
      "- Embed \"publicly memorizeable\" variants that look like standard benchmarks but with small modifications. A pattern‑matcher that only recognizes the public form will output the public memorized answer rather than adapt; detect by comparing to valid answer for the modified item.\n",
      "- Adversarial decoys: ask for outputs in both canonical and unusual formats to catch prompt‑tuned models that only work in one style.\n",
      "\n",
      "E. Explain‑and‑verify protocol\n",
      "- Request both an answer and a concise 1–3 step justification. Use an automated verifier to check whether the explanation logically entails the answer (e.g., compute intermediate values or check stated steps). Pattern matchers often hallucinate plausible justifications; the verifier will catch mismatches between claimed steps and actual output.\n",
      "\n",
      "F. Multiple independent samplings + self‑consistency\n",
      "- Query the model multiple times with temperature sampling to observe variability. Pattern matching models tuned to be deterministic may produce the same memorized output; reasoning models may produce either stable or explainable variations. Use statistics of answer distribution to infer brittleness.\n",
      "\n",
      "G. Cross‑model and cross‑item statistical controls\n",
      "- Randomly split items into multiple forms; estimate item difficulty using item‑response theory (IRT) to control for item variance when comparing models.\n",
      "\n",
      "H. Avoid prompting that guides the model to memorize the test pattern\n",
      "- Randomize prompt templates; do not leak examples from the test set in prompts. Use standardized prompt skeletons with random syntactic surfaces.\n",
      "\n",
      "I. Time & leakage controls\n",
      "- Run tests before and after major model updates; if possible, time‑stamp datasets and store seeds to detect leak. Rotate new item sets periodically.\n",
      "\n",
      "4) Sample sizes and statistical tests\n",
      "Goal: confidently assert a difference in capability between two black‑box LLMs (Model A and Model B).\n",
      "\n",
      "Definitions and assumptions:\n",
      "- Treat each model's response to an item as correct/incorrect (binary) for primary analysis. Use per‑task accuracy as primary metric; composite score as secondary.\n",
      "- Tests are paired (same items evaluated by both models). Use paired tests (McNemar’s test or paired permutation) to leverage within‑item correlations.\n",
      "\n",
      "Power & sample size rules of thumb\n",
      "- For a two‑proportion comparison (unpaired) with modest effect sizes (difference = 10–20 percentage points), recommended sample per model per task ≈ 300–500 items to get power ≈ 0.8 at alpha = 0.05. But because tests are paired, required number of distinct items is lower.\n",
      "\n",
      "Paired sample: approximate calculation\n",
      "- If expecting Model A accuracy = 50% and Model B = 70% on a task (difference 20 ppt), and assuming moderate within‑item correlation, ~150–250 paired items suffices for 80% power (McNemar or paired permutation). If difference expected smaller (e.g., 10 ppt), need ~400–800 items.\n",
      "- Conservative recommended sample sizes:\n",
      "  - Per task: 400 items (distinct problems) sampled from randomized template variants.\n",
      "  - Per composite suite (6 tasks): 6 * 400 = 2400 items total. You can distribute effort (e.g., 400 per high‑importance task, 200 per less critical).\n",
      "\n",
      "Statistical tests and controls\n",
      "- Primary test: paired permutation test on per‑item difference (nonparametric, robust) or McNemar’s test for binary paired data.\n",
      "- Secondary tests:\n",
      "  - Mixed‑effects logistic regression (item as random effect, model as fixed effect) to control for item difficulty and estimate model effect across heterogeneous items.\n",
      "  - Bootstrapped confidence intervals (resample items) for per‑task accuracy difference.\n",
      "  - Multiple comparisons correction: Benjamini–Hochberg when testing multiple tasks.\n",
      "- Report effect sizes (difference in proportions, odds ratio), 95% CI, and p‑values.\n",
      "- Power analyses: compute required sample size for expected minimal detectable effect size BEFORE running full test; pilot with small N to estimate item variance.\n",
      "\n",
      "Decision thresholds\n",
      "- Define thresholds for claiming superiority. Example: model A is better than B on task if:\n",
      "  - Paired difference in accuracy > δ (e.g., δ = 0.10) AND\n",
      "  - p < 0.01 after multiple comparisons correction AND\n",
      "  - Effect robust to bootstrap and mixed‑effects regression controlling for item variance.\n",
      "\n",
      "5) Possible adversarial behaviors and countermeasures\n",
      "Below are adversarial strategies a model might use to appear capable, and corresponding mitigations in the suite.\n",
      "\n",
      "Adversarial: Surface retrieval / memorized sequence regurgitation\n",
      "- Behavior: Model stores fixed outputs for common instructions; returns memorized outputs for superficially similar prompts.\n",
      "- Mitigation: synthetic tokens, held‑out compositions, trap items that mimic public benchmarks with slight changes; paraphrase and renaming consistency checks; require correct computation on non‑natural synthetic data.\n",
      "\n",
      "Adversarial: Template matching / prompt engineering (exploiting fixed prompt formats)\n",
      "- Behavior: Model trained to respond well to specific prompt templates; game tests by matching those templates exactly.\n",
      "- Mitigation: randomize prompt phrasing and structure; test with out‑of‑template paraphrases; hide test instructions across several rewordings.\n",
      "\n",
      "Adversarial: Post‑hoc plausible explanations (hallucinated chain‑of‑thought)\n",
      "- Behavior: Model produces a plausible‑looking explanation that does not correspond to internal computation.\n",
      "- Mitigation: automatic verification of explanation steps; require computation that can be executed in a simulator and check consistency between explanation and final answer. Ask for explicit intermediate numeric values that can be validated.\n",
      "\n",
      "Adversarial: Strategic stochasticity (choose most socially pleasing answer)\n",
      "- Behavior: Model outputs an answer that appears reasonable but isn't derived by correct reasoning.\n",
      "- Mitigation: use items with unique numeric/structural answers and built simulators to check exactness. Evaluate per‑answer consistency across multiple samples.\n",
      "\n",
      "Adversarial: Learning to memorize test suite by repeated exposure (test leakage)\n",
      "- Behavior: Providers fine‑tune model on leaked items and then pass tests.\n",
      "- Mitigation: maintain a large pool of test items, rotate, and ensure many items are generated on the fly. Keep some items only as one‑time use. Audit and track tests over time to detect sudden accuracy jumps.\n",
      "\n",
      "Adversarial: Exploiting world knowledge\n",
      "- Behavior: Model uses web knowledge (e.g., named entities) instead of reasoning.\n",
      "- Mitigation: use synthetic domains and names that aren't in corpus; prefer abstract variables and gibberish tokens.\n",
      "\n",
      "Adversarial: Exploiting consistent wrong heuristics (e.g., always answer \"no\" to avoid errors)\n",
      "- Behavior: Model learns a default safe answer that is occasionally correct by chance.\n",
      "- Mitigation: measure baselines and chance rates and compare; include control items where a naive heuristic fails frequently.\n",
      "\n",
      "6) Experimental validation plan, metrics for false positives/negatives, and iteration\n",
      "Validation phases\n",
      "- Phase 0 — Unit tests: small sample (N=50 per task) to verify item generation, canonicalization, and grading logic.\n",
      "- Phase 1 — Pilot: evaluate 3–5 diverse models (known baselines: simple pattern models, older LLM, and an advanced recent LLM) on a larger pilot (N=200 per task). Use pilot to estimate item difficulties and variance.\n",
      "- Phase 2 — Full evaluation: run full suite (recommended N=400 per task) across models under test.\n",
      "- Phase 3 — Adversarial robustness checks: invite models purposely trained to game tests; analyze failures and iterate.\n",
      "\n",
      "Metrics\n",
      "- Primary: per‑task accuracy (binary), composite accuracy.\n",
      "- Secondary: explanation correctness rate; self‑consistency rate (fraction of paraphrases with same answer); variability under sampling (entropy of answers).\n",
      "- Discrimination metrics: Area Under ROC curve if defining continuous scoring; effect sizes and odds ratios.\n",
      "- Error analysis metrics:\n",
      "  - False positives (FP): model flagged as reasoning‑capable but actually relying on memorization. Estimate by hand‑inspecting suspicious items and adversarially designed holdouts. Also measure cases where model passes synthetic tasks but fails renamed/scrubbed variants.\n",
      "  - False negatives (FN): model truly capable but fails tests (e.g., due to prompt format, minor ambiguity). Detect by analyzing cases where model yields correct reasoning traces but output form differs; retrain canonicalizer.\n",
      "\n",
      "Estimating FP/FN rates\n",
      "- Use validation with ground‑truth labeled model types:\n",
      "  - Create/collect models we know: (i) a trained memorization baseline (e.g., n‑gram or memorization‑augmented seq2seq), (ii) a symbolic reasoner (deterministic oracle), (iii) a hybrid model.\n",
      "  - Run full suite and measure classification (declared as reasoning vs not) vs known ground truth. Compute sensitivity (TPR) and specificity (1−FPR). Tune decision thresholds to maintain acceptable FP (e.g., <5%) while maximizing sensitivity.\n",
      "\n",
      "Iterative refinement based on results\n",
      "- Analyze item‑level statistics: identify low discrimination items (very easy or very hard) using IRT or item discrimination index. Remove or rework items with poor discrimination.\n",
      "- Increase adversarial item proportion if many models game the suite.\n",
      "- Expand synthetic vocabulary and composition depth if memorization leakage is observed.\n",
      "- Automate item generation pipelines to produce fresh held‑out sets periodically.\n",
      "- Maintain an audit log of test runs and items to detect potential leakage and overfitting by model providers.\n",
      "\n",
      "Example iteration cycle\n",
      "1. Pilot run → compute per‑task accuracies, item difficulty.\n",
      "2. Identify items where memorizing baselines scored as well as strong reasoning oracle → mark for redesign (add randomization, deeper nesting, or rename tokens).\n",
      "3. Add paraphrase/renaming tests for items showing potential prompt‑template exploitation.\n",
      "4. Rebalance item pool to maintain discrimination and rerun.\n",
      "\n",
      "Practical considerations and recommended implementation checklist\n",
      "- Implement automatic generators for each template with reproducible random seeds.\n",
      "- Implement deterministic simulators/validators for each task (action sequence executor, arithmetic evaluator, planner checker) so grading is exact.\n",
      "- For each item store (seed, tokens, canonical answer, distractors).\n",
      "- Maintain multiple prompt forms and sample at random per query.\n",
      "- Log all outputs, probabilistic sampling states (temperature), and timestamps.\n",
      "- Use paired testing (same items across all models) to maximize power and fairness.\n",
      "- Publicly release small sanitized subsets for community verification but keep main test pools private to reduce overfitting.\n",
      "\n",
      "Concluding guidance\n",
      "- No single test proves genuine reasoning forever; use this suite as a robust, multi‑task battery that measures “behavioral evidence” of compositional reasoning and generalization.\n",
      "- Emphasize cross‑task consistency, randomness/held‑out composition, and rigorous statistical testing.\n",
      "- Make verification automated where possible (simulators) and include human review for ambiguous failures.\n",
      "- Iterate the pool and keep fresh synthetic items to reduce memorization/leakage risk.\n",
      "\n",
      "If you’d like, I can:\n",
      "- Produce a reference implementation (Python) that generates randomized items for each template and includes canonicalizers and simulators for automated scoring.\n",
      "- Compute precise sample size calculations for specific expected effect sizes you care about and provide a ready‑to‑run experimental script.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "# Comprehensive Test Suite for Distinguishing Pattern Matching from Compositional Reasoning in LLMs\n",
      "\n",
      "## Executive Summary\n",
      "\n",
      "This test suite employs six task categories designed to probe genuine compositional reasoning while resisting memorization and gaming. Each task incorporates systematic variation, procedural generation, and adversarial controls to ensure that success requires compositional understanding rather than pattern matching.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Task Types with Examples\n",
      "\n",
      "### Task 1: Systematic Compositional Generalization (Novel Recombinations)\n",
      "\n",
      "**Objective**: Test ability to combine known primitives in novel configurations never seen during training.\n",
      "\n",
      "**Design Principle**: Use a microworld with defined rules, then test combinations statistically unlikely to appear in training data.\n",
      "\n",
      "**Example Input**:\n",
      "```\n",
      "Rules of the GLORP system:\n",
      "- FLIM(x) reverses the order of elements in x\n",
      "- DRAK(x, y) interleaves elements from x and y\n",
      "- ZOOP(x) duplicates every second element in x\n",
      "\n",
      "Training examples:\n",
      "FLIM([A, B, C]) = [C, B, A]\n",
      "DRAK([1, 2], [3, 4]) = [1, 3, 2, 4]\n",
      "ZOOP([P, Q, R]) = [P, Q, Q, R]\n",
      "\n",
      "Test question:\n",
      "What is FLIM(DRAK(ZOOP([X, Y]), [M, N]))?\n",
      "```\n",
      "\n",
      "**Expected Output**: \n",
      "```\n",
      "Step-by-step reasoning:\n",
      "1. ZOOP([X, Y]) = [X, Y, Y]\n",
      "2. DRAK([X, Y, Y], [M, N]) = [X, M, Y, N, Y]\n",
      "3. FLIM([X, M, Y, N, Y]) = [Y, N, Y, M, X]\n",
      "\n",
      "Answer: [Y, N, Y, M, X]\n",
      "```\n",
      "\n",
      "**Scoring Rubric**:\n",
      "- 4 points: Correct answer with valid step-by-step reasoning\n",
      "- 3 points: Correct answer with minor reasoning errors\n",
      "- 2 points: Incorrect answer but demonstrates understanding of composition\n",
      "- 1 point: Partially correct intermediate steps\n",
      "- 0 points: Incorrect with no valid reasoning\n",
      "\n",
      "**Variants for Anti-Memorization**:\n",
      "- Generate 10,000 unique microworlds with different operation names, symbols, and rule sets\n",
      "- Use procedural generation with random seeds\n",
      "- Operation names drawn from pronounceable non-words (e.g., BLICKET, WUGGY, FEPS)\n",
      "- Vary depth of composition (2-5 levels)\n",
      "- Test both symbolic and numeric domains\n",
      "\n",
      "---\n",
      "\n",
      "### Task 2: Causal Reasoning Under Intervention (Counterfactual Inference)\n",
      "\n",
      "**Objective**: Distinguish correlation from causation and reason about interventions.\n",
      "\n",
      "**Design Principle**: Present causal graphs implicitly through scenarios, then test counterfactual reasoning that requires understanding causal structure.\n",
      "\n",
      "**Example Input**:\n",
      "```\n",
      "Scenario: In the town of Millbrook, the following patterns have been observed over 10 years:\n",
      "\n",
      "- When the reservoir level is high, the water treatment plant runs at full capacity\n",
      "- When the treatment plant runs at full capacity, downtown water pressure is strong\n",
      "- When downtown water pressure is strong, the fountain in Central Park operates\n",
      "- The reservoir level depends only on rainfall\n",
      "- Rainfall also directly affects whether street cleaning happens (rain = no cleaning)\n",
      "\n",
      "Historical data shows that on days when the fountain operates, streets are usually dirty.\n",
      "\n",
      "Question 1: The town installs a new pump that allows the fountain to operate regardless of downtown water pressure. After this intervention, will the streets be cleaner or dirtier on days when the fountain operates, compared to before?\n",
      "\n",
      "Question 2: Explain your reasoning using the causal structure.\n",
      "```\n",
      "\n",
      "**Expected Output**:\n",
      "```\n",
      "Answer: The streets will be CLEANER on fountain-operating days after the intervention.\n",
      "\n",
      "Reasoning: \n",
      "Before intervention: Fountain operating → High reservoir → Recent rainfall → No street cleaning → Dirty streets\n",
      "\n",
      "The correlation between fountain operation and dirty streets was due to a common cause (rainfall), not because the fountain caused dirty streets.\n",
      "\n",
      "After intervention: The fountain can operate without rainfall, breaking the spurious correlation. Fountain operation is now independent of rainfall, so there's no reason to expect systematically dirtier streets.\n",
      "```\n",
      "\n",
      "**Scoring\n",
      "\n",
      "# Response from competitor 3\n",
      "\n",
      "This test-suite, titled **CORE-Eval (Compositional & Operational Reasoning Evaluation)**, is designed to bypass the \"stochastic parrot\" effect by focusing on out-of-distribution (OOD) tasks that require the dynamic manipulation of novel variables and rules.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Concrete Task Types\n",
      "\n",
      "#### Task A: Recursive Nested State Tracking (Variable Binding)\n",
      "*   **Concept:** Track the state of objects through recursive \"if-then\" swaps and containment changes.\n",
      "*   **Input:** \"There are three boxes: Red, Blue, and Green. Red contains a 'Glint'. Blue contains a 'Spark'. Green is empty. Rule 1: If an object is moved to an empty box, its name reverses. Rule 2: If two boxes swap, their contents swap. Step 1: Swap Red and Blue. Step 2: Move contents of Blue to Green. Step 3: Rule 1 applies to Green. What is in Green?\"\n",
      "*   **Expected Output:** \"tnilG\" (The 'Glint' moved from Red to Blue in Step 1, then from Blue to Green in Step 2, triggering Rule 1).\n",
      "\n",
      "#### Task B: Counterfactual Physics Reasoning (Causal Reasoning)\n",
      "*   **Concept:** Apply logical deductions in a world where one fundamental law of physics is altered.\n",
      "*   **Input:** \"In this world, gravity acts as a repellent for liquids but a vacuum for solids. If I tip a glass of water upside down over a table, and there is a wooden block on that table, what happens to the water and the block?\"\n",
      "*   **Expected Output:** The water moves upward (away from the center of mass/floor) and the block is pulled toward the ceiling (vacuum effect).\n",
      "\n",
      "#### Task C: The \"Zylophon\" Syntax (Systematic Generalization)\n",
      "*   **Concept:** Learn a 3-rule pseudo-grammar and apply it to a 20-word sentence.\n",
      "*   **Input:** \"Grammar: (1) Nouns end in '-ox'. (2) Verbs precede nouns. (3) Adjectives follow the noun they modify and must be repeated twice. Translate: 'The fast cat chases a small mouse' into Zylophon.\"\n",
      "*   **Expected Output:** \"Chases catox fast fast mouseox small small.\" (Requires consistent rule application over lexical substitution).\n",
      "\n",
      "#### Task D: Strategic Pathfinding with Dynamic Obstacles (Planning)\n",
      "*   **Concept:** Solve a grid-based navigation task where the \"cost\" of movement changes based on the history of moves.\n",
      "*   **Input:** \"Grid 4x4. Start (0,0), Goal (3,3). Moving East costs 1. Moving South costs 2. However, every time you move South, the cost of the next East move doubles. Provide the sequence of moves for the lowest cost.\"\n",
      "*   **Expected Output:** A specific path (e.g., E, E, E, S, S, S) with a calculated total cost.\n",
      "\n",
      "#### Task E: Functional Abstraction (Abstraction)\n",
      "*   **Concept:** Identify a latent function from input-output pairs and apply it to a complex, non-obvious case.\n",
      "*   **Input:** \"f(apple) = 1, f(banana) = 3, f(kiwi) = 2. Rule: f(x) = count of vowels in word. Apply f to the result of f('pomegranate') expressed as a word.\"\n",
      "*   **Expected Output:** \"f(5) -> f('five') -> 2.\"\n",
      "\n",
      "#### Task F: Adversarial Syllogisms (Logic vs. Pattern Matching)\n",
      "*   **Concept:** Logical structures where the conclusion is factually false in the real world but logically valid within the prompt.\n",
      "*   **Input:** \"All surgeons are cats. Some cats are astronauts. Therefore, are some surgeons astronauts? Explain the necessity of the conclusion.\"\n",
      "*   **Expected Output:** No. (Standard logical fallacy: the 'undistributed middle'). Memorization-based models often default to \"Yes\" because the semantic \"vibes\" of surgeons and astronauts are high-competence.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Scoring Rubric\n",
      "Each task is scored on a 0-3 scale:\n",
      "*   **0 (Failure):** Incorrect final answer and flawed logic or hallucinations.\n",
      "*   **1 (Partial):** Correct logic but arithmetic/minor execution error; or correct answer but \"hollow\" explanation.\n",
      "*   **2 (Successful):** Correct final answer and clear, step-by-step reasoning (Chain of Thought).\n",
      "*   **3 (Robust):** Correct answer even when the prompt includes \"distractor\" information (e.g., \"Note: the boxes are made of wood,\" which is irrelevant).\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Protocols to Prevent Memorization/Gaming\n",
      "*   **The DSL Generator:** Use a Domain Specific Language (DSL) script to generate 1,000 unique variations of each task. This ensures no specific instance exists in the training data.\n",
      "*   **Nonce-Token Substitution:** Replace all semantic nouns (e.g., \"cat\") with random strings (e.g., \"blorp-7\"). This strips the model of \"semantic priming\" where it guesses based on word associations rather than logic.\n",
      "*   **Permutation Testing:** For every prompt, provide a version where the order of rules is changed. A reasoning model will adapt; a pattern-matcher will often fail if it memorized a specific sequence.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Sample Size and Statistical Tests\n",
      "*   **Sample Size:** $N=200$ unique prompts per task type (Total 1,200).\n",
      "*   **Metric:** Accuracy (binary) and Mean Score (0-3).\n",
      "*   **Statistical Test:** \n",
      "    *   **McNemar’s Test:** To compare two models on the same set of binary outcomes (to see if the performance difference is statistically significant).\n",
      "    *   **Standard Deviation of Error:** If a model’s performance drops significantly when nouns are replaced with nonce-tokens, it is flagged as relying on **pattern matching** (High Semantic Sensitivity).\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Adversarial Behaviors & Design Counter-Measures\n",
      "*   **Behavior: Hedging/Refusal.** The model says \"This depends on the laws of physics\" to avoid commitment.\n",
      "    *   *Counter-measure:* Explicitly state \"Assume this hypothetical world is the only reality for this calculation.\"\n",
      "*   **Behavior: Chain-of-Thought Mimicry.** The model writes \"Step 1... Step 2...\" but the steps don't lead to the conclusion.\n",
      "    *   *Counter-measure:* Use **Conclusion-to-Step Verification**. Ask the model for the answer first, then the steps, then provide a *different* answer and ask it to find the error.\n",
      "*   **Behavior: Semantic Drift.** The model starts with the counterfactual but drifts back to real-world logic mid-sentence.\n",
      "    *   *Counter-measure:* **Long-Chain Dependency.** Ensure the final answer requires the output of Step 1 to be used in Step 10.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Experimental Validation & Iteration\n",
      "*   **Gold Standard:** Run the suite on a \"Ground Truth\" group of human logic experts.\n",
      "*   **False Positive (FP) Metric:** If a known \"shallow\" model (e.g., a 1B parameter model) passes, the task is too simple. Increase the recursive depth.\n",
      "*   **False Negative (FN) Metric:** If humans fail the task, it is a measure of complexity, not reasoning. Simplify until human accuracy is $>95\\%$.\n",
      "*   **Iteration Loop:** \n",
      "    1.  Deploy $N=50$. \n",
      "    2.  Check for \"Shortcut Cues\" (e.g., if the answer is always the last item mentioned). \n",
      "    3.  If a model identifies a shortcut, re-randomize the prompt generator to eliminate the cue. \n",
      "    4.  Scale to $N=1,200$.\n",
      "\n",
      "# Response from competitor 4\n",
      "\n",
      "# **Adversarial‑Resistant Test Suite for Compositional Reasoning vs. Pattern‑Matching in Black‑Box LLMs**\n",
      "\n",
      "## **1. Core Philosophy**\n",
      "The suite is designed to be **variants‑rich, systematically generated, and dynamically updated** to prevent memorization. It tests **out‑of‑distribution generalization** by requiring recombination of known primitives in novel ways, and uses **control tasks** to detect shallow heuristics.\n",
      "\n",
      "---\n",
      "\n",
      "## **2. Six Task Types with Examples**\n",
      "\n",
      "### **Task Type 1: Systematic Generalization (Syntax‑Semantics Mismatch)**\n",
      "**Goal:** Test if the model can apply known syntactic rules to novel lexical items or semantic domains.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "Rule: \"Every X that Ys is Z.\" → \"All Xs that Y are Z.\"\n",
      "Apply to: \"Every blimp that froods is tink.\"\n",
      "```\n",
      "**Expected Output:** `\"All blimps that frood are tink.\"`\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct transformation with morphological adjustment (frood→frood, not froods→frood).\n",
      "- 1 pt: Correct transformation but minor morphological error.\n",
      "- 0 pts: Failure to apply rule, or parroting input.\n",
      "\n",
      "**Adversarial Defense:** Use **nonce words** (blimp, frood, tink) with no web occurrence; generate 100 variants per test with random nonce triples.\n",
      "\n",
      "---\n",
      "\n",
      "### **Task Type 2: Causal Reasoning with Novel Mechanisms**\n",
      "**Goal:** Test understanding of causal chains, distinguishing causation from correlation, and handling novel causal verbs.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "In a novel system: \"Grobling causes flumps to shank, but shanking prevents grobling. If you grobl a flump, what happens after two cycles?\"\n",
      "```\n",
      "**Expected Output:** `\"After one cycle: flump shanks. After two cycles: shanking prevents grobling, so grobling stops, flump no longer shanks.\"`\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct step‑by‑step trace with state updates.\n",
      "- 1 pt: Correct final state but missing intermediate reasoning.\n",
      "- 0 pts: Wrong final state or contradictory reasoning.\n",
      "\n",
      "**Adversarial Defense:** Randomize causal graphs (A→B, B inhibits A, etc.) with random nonce names; require step‑by‑step explanation.\n",
      "\n",
      "---\n",
      "\n",
      "### **Task Type 3: Abstraction & Analogical Mapping**\n",
      "**Goal:** Test ability to extract relational schema and apply it to a new domain.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "\"A glorp is to a fleeb as a trank is to a ______.\"  \n",
      "Context: \"Glorps modify fleebs; tranks modify…\"\n",
      "```\n",
      "**Expected Output:** `\"spindle\"` (given novel mapping: tranks modify spindles).\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct answer + correct relation stated.\n",
      "- 1 pt: Correct answer only.\n",
      "- 0 pts: Wrong answer.\n",
      "\n",
      "**Adversarial Defense:** Use **randomly generated analogies** with arbitrary relations (modifies, contains, opposes, etc.) and novel object names.\n",
      "\n",
      "---\n",
      "\n",
      "### **Task Type 4: Counterfactual Reasoning with Novel Premises**\n",
      "**Goal:** Test reasoning about what would be true if a known fact were different.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "\"In world W: All zingers are fribble. Moops are not zingers. Are moops fribble?  \n",
      "Now consider: If zingers were not fribble, would moops be fribble?\"\n",
      "```\n",
      "**Expected Output:** `\"In actual world: unknown (moops may or may not be fribble). In counterfactual: moops are not fribble (because only zingers were fribble, now nothing is).\"`\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct both actual and counterfactual with justification.\n",
      "- 1 pt: Correct counterfactual only.\n",
      "- 0 pts: Wrong.\n",
      "\n",
      "**Adversarial Defense:** Randomize logical forms (∀x(Z(x)→F(x)), ¬∀x(M(x)→Z(x)), etc.) with nonce predicates.\n",
      "\n",
      "---\n",
      "\n",
      "### **Task Type 5: Hierarchical Planning with Novel Constraints**\n",
      "**Goal:** Test ability to decompose a goal into sub‑goals under new constraints.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "\"Goal: Get a sprocket from the grove. Actions: you can ‘zazz’ (moves sprockets to the grove if they are in the vault), ‘vinn’ (moves you to the grove), ‘lork’ (locks the vault if you are there). The sprocket is in the vault, which is locked. You are outside. What sequence works?\"\n",
      "```\n",
      "**Expected Output:** `\"Vinn to vault, lork to unlock, zazz sprocket to grove, vinn to grove.\"`\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct minimal‑step sequence.\n",
      "- 1 pt: Correct but redundant steps.\n",
      "- 0 pts: Incorrect or impossible sequence.\n",
      "\n",
      "**Adversarial Defense:** Generate random **STRIPS‑like planning problems** with novel action names and preconditions; vary initial/goal states.\n",
      "\n",
      "---\n",
      "\n",
      "### **Task Type 6: Variable Binding & Quantifier Scope**\n",
      "**Goal:** Test handling of quantifier scope ambiguities with novel predicates.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "\"Every blip is in a cloop. A cloop is damp. Does it follow that every blip is in a damp cloop? Explain.\"\n",
      "```\n",
      "**Expected Output:** `\"No. Every blip is in some cloop, but possibly different cloops; not all those cloops must be damp.\"`\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct answer with correct logical explanation.\n",
      "- 1 pt: Correct answer with vague explanation.\n",
      "- 0 pts: Wrong answer.\n",
      "\n",
      "**Adversarial Defense:** Randomize quantifier order (∀∃ vs ∃∀), use nonce predicates, ask for explanation to avoid guessing.\n",
      "\n",
      "---\n",
      "\n",
      "## **3. Protocols Against Memorization & Gaming**\n",
      "\n",
      "### **Dynamic Variant Generation**\n",
      "- Each task instance is generated from a **seed‑based procedural algorithm** with nonce words/relations.\n",
      "- Maintain a **held‑out variant pool** not used during public benchmark releases.\n",
      "- For each task type, generate **10,000 unique variants**; use random subsets per test session.\n",
      "\n",
      "### **Statistical Controls**\n",
      "- Include **“pattern‑matching catch trials”**: tasks that look superficially similar but require different reasoning; memorizing‑based models will fail.\n",
      "- Example: A sentence with same surface form as a known training example but with reversed logic.\n",
      "\n",
      "### **Answer Consistency Checks**\n",
      "- For each task, ask **the same underlying question in two different surface forms** (paraphrase, different nonce words). A reasoning model should give consistent answers; a memorizing model may not.\n",
      "- Use **temporal probing**: ask follow‑up questions that require maintaining variable bindings across turns.\n",
      "\n",
      "### **Explanations Required**\n",
      "- Force chain‑of‑thought (CoT) explanations. Use **explanation‑consistency scoring**: if the final answer contradicts the explanation, penalize heavily.\n",
      "\n",
      "### **Adversarial Training Data Detection**\n",
      "- Compare responses to **web‑search snippets** for nonce words; if any match appears, flag potential data contamination.\n",
      "- Deploy **canary tokens** in public descriptions of the suite; if models output these tokens, they have seen the test suite in training.\n",
      "\n",
      "---\n",
      "\n",
      "## **4. Sample Sizes & Statistical Tests**\n",
      "\n",
      "### **Per‑Task Design**\n",
      "- **Minimum sample**: 200 instances per task type (1200 total), balanced across sub‑types.\n",
      "- Each instance is scored 0‑2, yielding a **mean score per task**.\n",
      "\n",
      "### **Statistical Tests**\n",
      "- **Primary metric**: Mean composite score (sum over tasks) compared between two models.\n",
      "- Use **Mann‑Whitney U test** (non‑parametric, doesn’t assume normality) for score distributions.\n",
      "- **Confidence intervals** for per‑task score differences via bootstrap (1000 resamples).\n",
      "- **Effect size**: Cliff’s delta for robust interpretation.\n",
      "\n",
      "### **Power Analysis**\n",
      "- To detect a difference of **0.3 points** (out of 12 total) with 80% power and α=0.05, need ~100 instances per task for two‑group comparison.\n",
      "- Our 200 per task provides power to detect smaller effects.\n",
      "\n",
      "---\n",
      "\n",
      "## **5. Possible Adversarial Behaviors & Mitigations**\n",
      "\n",
      "| **Adversarial Behavior** | **Mitigation in Our Design** |\n",
      "|--------------------------|-------------------------------|\n",
      "| **Prompt‑engineering to guess pattern** | Require explanation; randomize surface forms; use nonce words. |\n",
      "| **Fine‑tuning on similar benchmarks** | Use dynamic generation; deploy canary tokens; test on held‑out variant pool. |\n",
      "| **Retrieval‑augmented generation (RAG)** | Nonce words ensure no matching documents; novel compositions unseen. |\n",
      "| **Over‑producing multiple answers** | Evaluate only first coherent answer; consistency checks across paraphrases. |\n",
      "| **Exploiting task‑specific heuristics** | Include “catch trials” that break heuristics; vary task parameters widely. |\n",
      "| **Memorizing from explanations in training** | Generate novel reasoning chains; randomize order of steps in explanations. |\n",
      "\n",
      "---\n",
      "\n",
      "## **6. Experimental Validation Plan**\n",
      "\n",
      "### **Phase 1: Baselines**\n",
      "- Test known **pattern‑matching‑heavy models** (e.g., smaller LLMs, n‑gram baselines) vs. **alleged reasoning models** (e.g., GPT‑4, Claude 3).\n",
      "- Expect clear separation in scores.\n",
      "\n",
      "### **Phase 2: Contamination Check**\n",
      "- For each model, test on **canary instances**; if performance is perfect on those but poor on random variants, flag contamination.\n",
      "- Compare performance on **public variant set** vs. **held‑out variant set**; a contaminated model will show a large drop.\n",
      "\n",
      "### **Phase 3: Iteration**\n",
      "- **False Positive Rate (FPR)**: Rate at which pattern‑matchers score above threshold. Estimate by testing on pure n‑gram/retrieval baselines.\n",
      "- **False Negative Rate (FNR)**: Rate at which true reasoners score below threshold. Estimate by testing humans (experts) on a subset.\n",
      "- **Iteration**: If FPR > 5% or FNR > 10%, analyze failure cases, add new task variants or new “catch” subtasks, and adjust scoring rubric.\n",
      "\n",
      "### **Metrics for Suite Quality**\n",
      "- **Discrimination index**: (Mean score of top known reasoners – mean score of pattern‑matchers) / pooled SD. Target > 2.0.\n",
      "- **Internal consistency**: Cronbach’s alpha across task instances; target > 0.8.\n",
      "- **Test‑retest reliability**: Same model tested on different variant sets; correlation target > 0.9.\n",
      "\n",
      "### **Long‑Term Upkeep**\n",
      "- **Quarterly updates** of nonce words and relation sets.\n",
      "- **Community contribution** of new task templates via controlled repository.\n",
      "- **Continuous adversarial red‑teaming** to find gaming strategies and patch them.\n",
      "\n",
      "---\n",
      "\n",
      "## **Conclusion**\n",
      "This test suite is **generative, scalable, and adversarial‑resistant** by design. It focuses on **systematic generalization** with novel primitives, requires **explanatory consistency**, and uses **statistical controls** to separate memorization from reasoning. With proper deployment and iteration, it can serve as a robust benchmark for genuine compositional reasoning in black‑box LLMs.\n",
      "\n",
      "# Response from competitor 5\n",
      "\n",
      "Below is a **complete, adversarial‑resistant test‑suite** that can be used to separate **“pattern‑matching / memorisation‑heavy”** language models from those that truly **compose, reason and generalise**.  \n",
      "The design follows best‑practice guidelines from the psychometrics of AI evaluation (e.g., Item‑Response Theory, bootstrap hypothesis testing) and includes concrete items, scoring rules, anti‑gaming protocols, statistical power calculations, threat‑modelling, and a validation plan.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Six Concrete Task Types (with examples)\n",
      "\n",
      "| # | Task Type | What it probes | Core compositional demand | Example Prompt (input) | Expected Answer (output) | Why a memorisation‑only model should fail |\n",
      "|---|-----------|----------------|---------------------------|------------------------|--------------------------|------------------------------------------|\n",
      "| 1 | **Systematic Generalisation (SCG)** – novel slot‑filling on a known grammar | Ability to apply a learned rule to unseen lexical items | “<subject> <verb‑past> the <object>” → “<object> was <verb‑past‑by‑subject>” | *Prompt:* “Translate to passive voice: *The *florists* *arranged* the *vases*.” | “The vases were arranged by the florists.” | The model must recombine known syntactic transformations with *new* nouns and verbs it has never seen together. |\n",
      "| 2 | **Causal‑Chain Reasoning (CCR)** – multi‑step cause → effect inference | Understanding and chaining causal relations | *Prompt:* “If the thermostat is set to 22 °C, the heater turns on. If the heater turns on, the room warms up. What will happen if the thermostat is set to 22 °C?” | “The heater will turn on, and the room will warm up.” | Requires chaining two rules; a pure pattern‑matcher would need the exact whole‑sentence pattern in its training data, which is unlikely. |\n",
      "| 3 | **Abstract Symbol Manipulation (ASM)** – variable binding & substitution in a tiny “programming” language | Binding variables, applying functions, preserving scope | *Prompt:* “In a language where `F(x)=x+2` and `G(y)=y*3`, compute `F(G(4))`.” | “22” | The model must treat `F` and `G` as *functions* and apply them compositionally, not retrieve a memorised answer for “F(G(4))”. |\n",
      "| 4 | **Counterfactual Reasoning (CFR)** – “what‑if” world changes | Evaluating a scenario under a hypothetical change while keeping other facts constant | *Prompt:* “John is taller than Mary. If John were 5 cm shorter, would he still be taller than Mary? (Mary is 165 cm tall.)” | “Yes, because John would still be 166 cm tall (originally 171 cm).” | Requires keeping the original facts, applying the counterfactual transformation, and re‑evaluating the comparison. |\n",
      "| 5 | **Planning & Constraint Satisfaction (PCS)** – generate a sequence that satisfies a set of constraints | Multi‑step planning, maintaining state, respecting constraints | *Prompt:* “Place three red, two blue and one green token on a line of six cells so that no two tokens of the same colour are adjacent.” | Any valid ordering, e.g., “R B R G R B”. | The answer must be **constructed**; a memorised list of valid strings is improbable because the colour‑counts are randomised per item. |\n",
      "| 6 | **Relational Analogy with Variable Binding (RAVB)** – map relational structure from one domain to another | Abstract relational mapping, not surface similarity | *Prompt:* “In the story, the rabbit hides the carrot, and the fox steals the rabbit. Which of the following statements preserves the same relational pattern?  A) The cat eats the mouse, and the dog chases the cat.  B) The king crowns the queen, and the queen advises the king.  C) The painter paints a portrait, and the portrait hangs on the wall.” | “C) The painter paints a portrait, and the portrait hangs on the wall.” | Requires recognizing the *agent‑patient‑action* chain (A → B) and mapping it, not just picking the answer that shares surface words. |\n",
      "\n",
      "**Key design properties**\n",
      "\n",
      "* **Randomised lexical content** (nouns, verbs, numbers, colours) per item → eliminates exact‑match memorisation.  \n",
      "* **Controlled grammar** – the underlying rule stays the same while surface tokens change.  \n",
      "* **Multiple correct solutions** (e.g., PCS) → prevents a model from outputting a single memorised string.  \n",
      "\n",
      "---\n",
      "\n",
      "## 2. Scoring Rubric per Task\n",
      "\n",
      "| Task | Scoring granularity | Full credit (1.0) | Partial credit (0.5) | Zero (0) | Comments |\n",
      "|------|--------------------|-------------------|----------------------|----------|----------|\n",
      "| SCG | Exact string match after normalisation (case, punctuation) | Exact passive‑voice transformation | Minor grammatical slip (e.g., “by the florists” missing) | Wrong voice / wrong nouns | Normalise via regex. |\n",
      "| CCR | Proposition‑level correctness (two propositions) | Both cause‑effect statements correct | One of two statements correct | Neither correct or contradictory | Use automated entailment check (e.g., NLI model) + regex fallback. |\n",
      "| ASM | Numeric answer | Exact integer | Off‑by‑1 (e.g., due to rounding) | Wrong number / non‑numeric | Evaluate with `int()` conversion. |\n",
      "| CFR | Boolean + justification | Correct truth value **and** correct numeric comparison | Correct truth value but missing justification | Wrong truth value | Automatic parsing of “Yes/No” and numeric reasoning. |\n",
      "| PCS | Constraint satisfaction check | All constraints satisfied | Exactly one adjacency violation | More than one violation or length mismatch | Verify with a simple script that checks adjacency & colour counts. |\n",
      "| RAVB | Correct choice letter | Exact correct option | Selecting an answer that shares one relational component (e.g., same agent) | Incorrect relational mapping | Use string‑matching of relational pattern; optional human audit. |\n",
      "\n",
      "**Overall Score** – sum of task scores (max = 6). For statistical analysis we treat each task as an **item** with a binary/partial score and compute an *Item‑Response* vector per model.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Protocols to Prevent / Detect Memorised Answers & Prompt‑Engineering Gaming  \n",
      "\n",
      "| Threat | Counter‑measure (implementation) |\n",
      "|--------|-----------------------------------|\n",
      "| **Exact‑match memorisation** | *Dynamic item generation*: each evaluation run draws fresh random lexical items from a large curated pool (≥ 10 000 nouns, 5 000 verbs, etc.). The prompt is assembled on‑the‑fly; the exact string never appears in training data. |\n",
      "| **Few‑shot prompt injection** | Disallow any demonstration examples in the test prompt. The test harness supplies *only* the task description and the input. If a model internally “hallucinates” a few‑shot context, it will be penalised (no credit for self‑generated examples). |\n",
      "| **Self‑referential “I know the answer”** | Post‑hoc regex checks that the response does **not** contain the original prompt verbatim (or a near‑duplicate). Any such copy is flagged and the item is re‑issued. |\n",
      "| **Exploiting temperature / stochasticity** | Run **three independent generations** per item (different seeds) and aggregate via majority vote. High variance across runs is recorded as a *stability* metric; models that rely on randomness to “guess” get lower stability scores. |\n",
      "| **Prompt‑engineering to force chain‑of‑thought** | The evaluation harness **strips** any model‑generated “let me think step‑by‑step” text before scoring; only the final answer is considered. Optionally, a separate “chain‑of‑thought” sub‑task can be added to reward explicit reasoning, but it is **scored separately** from the core compositional test. |\n",
      "| **Data‑leak via test‑set publication** | Keep the **item seed list** secret. Publish only the *generation code* (open‑source) but not the random seeds used for a particular evaluation round. After a round is finished, the seed list can be released for reproducibility, but not before the run. |\n",
      "| **Model‑specific tokenisation tricks** | Normalise all outputs using Unicode NFKC, lower‑casing, and whitespace collapsing before scoring. This removes superficial token‑level tricks. |\n",
      "| **Batch‑size / context‑window cheating** | Enforce a strict **single‑turn** interaction: the model receives only the current prompt, no history. The harness discards any hidden system‑prompt that the model might have inserted (e.g., via system‑prompt injection). |\n",
      "\n",
      "**Statistical Controls**\n",
      "\n",
      "* For each task generate **N = 200** items per evaluation round (see Section 4).  \n",
      "* Randomly split the 200 items into **10 folds**; compute per‑fold scores to estimate variance and detect outliers (e.g., a fold where many items are accidentally repeated).  \n",
      "* Use **bootstrapped 95 % confidence intervals** on the mean item score to verify that the observed performance is not due to chance.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Sample Sizes & Statistical Tests\n",
      "\n",
      "### 4.1 Power analysis (binary/partial scores)\n",
      "\n",
      "Assume we want to detect a **Δ = 0.15** absolute improvement in mean item score (e.g., 0.70 vs 0.55) with **α = 0.05** and **power = 0.90**.\n",
      "\n",
      "* For a two‑sample **t‑test** on proportions (or on continuous scores 0‑1), the required per‑group sample size is roughly **n ≈ 140** items per model per task.  \n",
      "* To be conservative and to allow per‑task breakdowns, we use **n = 200** items per task (≈ 1 200 total items across the six tasks).  \n",
      "\n",
      "### 4.2 Hypothesis tests\n",
      "\n",
      "| Comparison | Test | Rationale |\n",
      "|------------|------|-----------|\n",
      "| **Model A vs Model B** overall compositional ability | Two‑sample **Welch’s t‑test** on the aggregated item‑score vector (6 × 200 = 1 200 observations) | Handles unequal variances; works for continuous 0‑1 scores. |\n",
      "| **Task‑specific advantage** (e.g., Model A better on PCS) | **McNemar’s test** on binary‑correct vs incorrect per‑item (paired because same items are given to both models) | Controls for item difficulty. |\n",
      "| **Stability across runs** (low variance) | **Levene’s test** on per‑item variance across the three generation seeds | Detects heteroscedasticity. |\n",
      "| **Effect of randomisation** (ensuring no memorisation) | **Permutation test**: shuffle model labels on the item‑score vector 10 000 times, compute empirical p‑value. | Non‑parametric safeguard. |\n",
      "\n",
      "**Effect‑size reporting** – alongside p‑values, always report **Cohen’s d** (or Hedges g for unequal N) and **95 % CI**.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Possible Adversarial Behaviours & How the Suite Handles Them\n",
      "\n",
      "| Adversarial behaviour | Why it could fool a naïve test | Mitigation in our suite |\n",
      "|-----------------------|--------------------------------|--------------------------|\n",
      "| **“Lookup‑table” memorisation** (store every possible SCG transformation) | If the lexical pool were tiny, the model could pre‑compute all combos. | Use a **large, randomly sampled lexical pool** (≥ 10 000 items) and **different random seeds** each run → combinatorial explosion makes full coverage impossible. |\n",
      "| **Prompt injection of chain‑of‑thought** (model internally asks for examples) | Could produce step‑by‑step reasoning that looks correct without true compositional ability. | Score only the **final answer**; optional chain‑of‑thought credit is a separate metric. |\n",
      "| **Self‑prompted few‑shot creation** (model fabricates a training example then uses it) | May artificially raise accuracy on tasks like PCS. | Run **three independent generations** and require **majority agreement**; fabricated examples rarely survive across seeds. |\n",
      "| **Exploiting test‑set leaks** (model was fine‑tuned on a released seed list) | Could produce perfect scores. | Keep the **seed list secret** until after the evaluation round; publish only generation code. |\n",
      "| **Using external tools (search, calculators)** via tool‑use APIs | Might compute ASM numerics perfectly. | For the pure‑LLM benchmark, **disable tool‑use**; run the model in a “no‑tools” sandbox. A separate “augmented‑LLM” track can be created where tool‑use is allowed, but the baseline must be tool‑free. |\n",
      "| **Biasing temperature to output “I don’t know”** (to avoid penalty) | Could inflate partial‑credit by abstaining. | **Zero‑score** for any non‑answer (e.g., “I don’t know”, empty response). Also enforce a **minimum length** (≥ 1 token) to prevent empty output. |\n",
      "| **Manipulating tokenisation to hide correct answer** (e.g., splitting “yes” into sub‑tokens) | Might bypass regex checks. | Normalise output using Unicode NFKC and **token‑agnostic string matching**; also run a **semantic classifier** (tiny NLI) to double‑check “yes/no” answers. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Experimental Validation Plan  \n",
      "\n",
      "### 6.1 Phases\n",
      "\n",
      "| Phase | Goal | Procedure | Success criteria |\n",
      "|-------|------|-----------|-------------------|\n",
      "| **Pilot** | Verify that item generation, parsing and scoring pipelines work reliably. | Run a **small LLM (e.g., GPT‑2‑XL)** on 30 items per task. Manually audit 10 % of outputs for parsing errors. | < 5 % parsing failures; clear separation of correct/incorrect. |\n",
      "| **Baseline Establishment** | Obtain performance distribution of *known pattern‑matching models*. | Evaluate 5 publicly available “large‑mem‑only” models (e.g., GPT‑2‑large, LLaMA‑7B, Falcon‑7B) on the full suite (200 × 6 items). | Mean score ≤ 0.55 (i.e., below chance for tasks requiring composition). |\n",
      "| **Target Model Evaluation** | Test a candidate *compositional* model (e.g., a fine‑tuned T5‑XXL or a transformer with explicit modular architecture). | Same protocol as baseline, three independent runs per item. | Mean score ≥ 0.75 and statistically significantly higher than baselines (p < 0.01, d ≥ 0.8). |\n",
      "| **Adversarial Stress Test** | Verify robustness to gaming strategies. | For each model, run an extra batch where the **temperature** is set to 0.9, and where we *prepend* a “few‑shot” instruction (e.g., “Answer step‑by‑step”). | Scores should not improve > 0.03 relative to the standard run; variance should increase, indicating instability. |\n",
      "| **Generalisation Check** | Ensure the test is not over‑fitted to a particular set of lexical items. | Regenerate the entire suite with a **new random seed** (different nouns/verbs) and re‑run the top‑performing model. | Score drop ≤ 0.05, confirming true compositional ability. |\n",
      "\n",
      "### 6.2 Metrics for False Positives / Negatives\n",
      "\n",
      "| Metric | Definition | How it is measured |\n",
      "|--------|------------|--------------------|\n",
      "| **False Positive Rate (FPR)** – proportion of pattern‑matching models that achieve “high” compositional score (≥ 0.70). | Count models meeting threshold / total pattern‑matching models. | Desired FPR < 0.05. |\n",
      "| **False Negative Rate (FNR)** – proportion of genuinely compositional models (e.g., models with known modular architectures) that fall below the threshold. | Count such models below threshold / total compositional models. | Desired FNR < 0.10. |\n",
      "| **Stability Index** – average pairwise disagreement across the three seeds per item. | `1 - (agreement_rate)`. | Low instability (< 0.07) signals reliable reasoning rather than random guessing. |\n",
      "| **Item Discrimination (I‑D)** – point‑biserial correlation between item score and overall model score. | Compute per‑item correlation; discard items with I‑D < 0.1. | Ensures each item differentiates models. |\n",
      "\n",
      "### 6.3 Iteration Loop\n",
      "\n",
      "1. **Collect results** → compute all metrics.  \n",
      "2. **Identify weak items** (low discrimination, high variance, high memorisation suspicion).  \n",
      "3. **Modify**: change lexical pool, increase constraint complexity, or add a small “distractor” clause.  \n",
      "4. **Regenerate** the affected items and re‑run the pilot.  \n",
      "5. **Repeat** until the suite shows **≥ 0.80** average discrimination and **≤ 0.05** FPR on a held‑out set of pattern‑matching models.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Summary Checklist (what to implement)\n",
      "\n",
      "| Component | Implementation |\n",
      "|-----------|----------------|\n",
      "| **Item generator** (Python script) – random noun/verb pools, template filling, seed‑control. |\n",
      "| **Scorer** – regex‑based normalisers, task‑specific validators, majority‑vote aggregator. |\n",
      "| **Evaluation harness** – API wrapper that forces single‑turn interaction, three‑seed repeats, temperature logging. |\n",
      "| **Statistical analysis notebook** – computes t‑tests, effect sizes, bootstrapped CIs, item discrimination. |\n",
      "| **Adversarial‑behavior detector** – copy‑detection, output‑length filter, variance logger. |\n",
      "| **Documentation** – protocol for secret seed handling, reproducibility instructions, reporting template (mean ± CI, d, FPR/FNR). |\n",
      "\n",
      "With these six tasks, a rigorous scoring rubric, anti‑gaming safeguards, statistically powered sample sizes, and a clear validation pipeline, the suite can **reliably differentiate** between LLMs that merely stitch together memorised patterns and those that truly **compose, reason and generalise**.\n",
      "\n",
      "# Response from competitor 6\n",
      "\n",
      "Certainly! Below is a comprehensive design of an adversarial-resistant test-suite to differentiate black-box large language models (LLMs) that rely primarily on pattern matching and memorization from those that demonstrate genuine compositional reasoning and robust generalization.\n",
      "\n",
      "---\n",
      "\n",
      "# 1. Task Types with Examples\n",
      "\n",
      "Each task type probes a core aspect of compositional reasoning, using concrete example inputs and expected outputs.\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 1: **Systematic Generalization**  \n",
      "**Goal:** Test ability to apply learned rules or compositions to novel combinations never seen during training.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"In a made-up language, the suffix '-lam' means plural and the prefix 'bri-' means 'red'.  \n",
      "  What is the plural form of ‘bri-tak’?\"  \n",
      "- **Expected Answer:**  \n",
      "  \"bri-tak-lam\"\n",
      "\n",
      "**Explanation:** The model must systematically combine a novel prefix and suffix on a base word to form a correct compositional output.\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 2: **Causal Reasoning**  \n",
      "**Goal:** Assess model's ability to infer cause-effect relationships that require multi-step reasoning beyond pattern matching.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"If the street is wet and it did not rain, what is a plausible cause?\"  \n",
      "- **Expected Answers (any one):**  \n",
      "  \"Someone sprayed water,\" OR \"A pipe burst,\" OR \"The sprinkler system was on.\"\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 3: **Abstraction and Variable Binding**  \n",
      "**Goal:** Check if the model can manipulate variables and abstract concepts rather than fixed phrases.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"There are three boxes: A, B, and C. If box A is heavier than box B, and box B is heavier than box C, which box is lightest?\"  \n",
      "- **Expected Answer:**  \n",
      "  \"Box C\"\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 4: **Counterfactual Reasoning**  \n",
      "**Goal:** Test reasoning about alternate realities and consequences.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"If humans could fly naturally, how would cities be designed differently?\"  \n",
      "- **Expected Answer (any reasonable answer):**  \n",
      "  \"Cities would have multi-level buildings designed for landing and takeoff, with less need for roads.\"\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 5: **Planning and Multi-step Reasoning**  \n",
      "**Goal:** Evaluate multi-step planning ability requiring chaining of actions.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"You want to bake a cake but forgot eggs. List the steps to bake the cake using a substitute.\"  \n",
      "- **Expected Answer:**  \n",
      "  A multi-step plan including identifying substitutes (like applesauce), mixing ingredients, baking, etc.\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 6: **Novel Compositional Logic Puzzles**  \n",
      "**Goal:** Test the capability to combine logic and compositional understanding on novel puzzles.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"In a tribe, all who wear hats can speak a secret language. If Joe can speak the secret language, does he wear a hat?\"  \n",
      "- **Expected Answer:**  \n",
      "  \"Yes, Joe wears a hat.\"\n",
      "\n",
      "---\n",
      "\n",
      "# 2. Scoring Rubrics\n",
      "\n",
      "| Task Type                  | Scoring Criteria                             | Points | Notes                                                  |\n",
      "|----------------------------|---------------------------------------------|--------|--------------------------------------------------------|\n",
      "| Systematic Generalization  | Exact compositional construction correctness | 0 or 1 | 1 if correct suffix/prefix placement; otherwise 0     |\n",
      "| Causal Reasoning           | Plausibility and causal correctness          | 0-2    | 2 for fully valid cause; 1 for partial; 0 otherwise    |\n",
      "| Abstraction/Variable Binding | Correct relational output                    | 0 or 1 | 1 if correct variable reference; 0 otherwise           |\n",
      "| Counterfactual Reasoning   | Reasonableness & coherence of counterfactual | 0-2    | 2 for insightful, plausible; 1 for partial; 0 for nonsense |\n",
      "| Planning                  | Completeness, logical order, and core elements | 0-3    | 3 full plan; 2 partial but logical; 1 minimal; 0 none  |\n",
      "| Logic Puzzles              | Logical correctness                           | 0 or 1 | 1 if logically correct inference; else 0               |\n",
      "\n",
      "- **Human raters** using rubric or automated semantic similarity with thresholds for partial credit.\n",
      "\n",
      "---\n",
      "\n",
      "# 3. Protocols Against Memorization & Gamings\n",
      "\n",
      "**(a) Variant Generation:**  \n",
      "- For each task, generate many variants by changing surface details while preserving reasoning demands, e.g., swap entity names, alter numbers, use paraphrases.  \n",
      "- Use programmatic generation or crowd-sourcing for variants, generating >100 unique examples per task type unseen in training data.\n",
      "\n",
      "**(b) Statistical Controls:**  \n",
      "- Include “catch” memorized-items known from common benchmarks and obscured by paraphrase to detect rote memorization.  \n",
      "- Randomly mix in distractor prompts asking unrelated questions to detect answer pattern repetition.\n",
      "\n",
      "**(c) Cross-Prompt Paraphrasing:**  \n",
      "- Use multiple prompt wordings per test instance to rule out prompt exercise exploitation.\n",
      "\n",
      "**(d) Zero-shot and Few-shot Tests:**  \n",
      "- Test variants with no or minimal in-context examples to check for reasoning independent of prompt engineering.\n",
      "\n",
      "---\n",
      "\n",
      "# 4. Sample Sizes and Statistical Tests\n",
      "\n",
      "- **Sample size:**  \n",
      "  - At least 100 unique test instances per task type to cover broad variant space.  \n",
      "  - Multiple prompt formulations per instance (3+), total ~300 prompts per task type.\n",
      "\n",
      "- **Statistical Test:**  \n",
      "  - Conduct statistical comparisons between models using nonparametric tests (e.g., Mann-Whitney U) on scored outputs to detect capability difference.  \n",
      "  - Compute Cohen’s d effect size to measure practical significance.\n",
      "\n",
      "- **Confidence:**  \n",
      "  - Power analysis to target ≥80% power detecting medium-large effect sizes (d=0.5) at α=0.05.\n",
      "\n",
      "---\n",
      "\n",
      "# 5. Addressing Adversarial Behaviors\n",
      "\n",
      "| Adversarial Strategy                  | Mitigation Approach                               |\n",
      "|-------------------------------------|--------------------------------------------------|\n",
      "| Memorizing benchmark outputs         | Variant generation, paraphrasing, catch items    |\n",
      "| Prompt-engineering to “game” test    | Multi-prompt design and zero-shot tests          |\n",
      "| Producing generic plausible answers  | Scoring by detailed correctness, not plausibility alone |\n",
      "| Copying large memorized paraphrases  | Novel, synthetic tasks with no prior examples    |\n",
      "| Pattern matching on keywords only    | Require multi-step reasoning and combinatorial answers |\n",
      "| Ignoring variable binding             | Task design enforcing explicit variable reference |\n",
      "\n",
      "---\n",
      "\n",
      "# 6. Experimental Validation Plan\n",
      "\n",
      "**Step 1:** Baseline testing on known models with expected capability gradients (e.g., smaller pattern-memorization-based vs. advanced reasoning-based LLMs).\n",
      "\n",
      "**Step 2:** Calculate:  \n",
      "- **True positives (TP):** Correctly identified reasoning-capable models  \n",
      "- **True negatives (TN):** Correctly identified pattern-matchers  \n",
      "- **False positives (FP):** Pattern-matchers wrongly classified as reasoning-capable  \n",
      "- **False negatives (FN):** Reasoning-capable models missed\n",
      "\n",
      "**Metrics:**  \n",
      "- Precision = TP / (TP + FP)  \n",
      "- Recall = TP / (TP + FN)  \n",
      "- F1 score = harmonic mean of precision and recall  \n",
      "- ROC-AUC (if applicable to continuous scoring)\n",
      "\n",
      "**Iteration:**  \n",
      "- Analyze failure cases for FP and FN to identify unsound prompts or scoring ambiguities.  \n",
      "- Refine and augment task variants or scoring rubric accordingly.  \n",
      "- Re-test and measure performance improvements.\n",
      "\n",
      "**Human-in-the-loop:**  \n",
      "- Incorporate expert review of ambiguous or borderline answers.  \n",
      "- Adjust rubric based on inter-rater consistency.\n",
      "\n",
      "---\n",
      "\n",
      "# Summary\n",
      "\n",
      "The multi-dimensional test suite combines diverse, adversarially-generated tasks probing essential cognitive faculties in LLMs beyond memorization. Systematic statistical evaluation and iterative refinement ensure reliable distinction of genuine compositional reasoning capabilities from superficial pattern matching.\n",
      "\n",
      "If you want, I can also provide sample code for generating prompt variants or scoring templates. Let me know!\n",
      "\n",
      "# Response from competitor 7\n",
      "\n",
      "Designing a comprehensive test-suite to evaluate black-box LLMs requires careful consideration of various task types, scoring rubrics, and protocols to prevent memorized answers and prompt-engineering gaming. Here's a proposed design for such a test-suite:\n",
      "\n",
      "**Task Types (6)**\n",
      "\n",
      "1. **Classification of Analogies**: Given an analogy statement like \"cats have whiskers,\" what category does [animal] belong to?\n",
      "\t* Example input: `[cat, whisker]`, `class: cat`\n",
      "\t* Expected output: `[animal], class: feline`\n",
      "2. **Systematic Generalization**: Predict the result of a generalized version of a rule (e.g., \"if A > B, then C = D\").\n",
      "\t* Example input: \"[mathematical expression]\", \"if A > 3 and C > 1\"\n",
      "\t* Expected output:=\"[ mathematical expression substitution with A > C and new_value ]\"\n",
      "3. **Causal Reasoning**: Given a sequence of events, predict the next event (e.g., \"what did John do after meeting Alice?\").\n",
      "\t* Example input: `[John , met Alice]`\n",
      "\t* Expected output: `\"he talked to Alice\"`\n",
      "4. **Abstraction**: Identify the underlying principle behind a concept or idea.\n",
      "\t* Example input: \"[human emotion with synonyms]\"\n",
      "\t* Expected output: \"concept/emotion underlying it, e.g., [happiness]\"\n",
      "\n",
      "5. **Counterfactuals**: Predict an alternative outcome given a hypothetical scenario (e.g., what would have happened if John had not met Alice?).\n",
      "\t* Example input: `[ scenario ], variable John , predicted_action, consequence ]\n",
      "\t * Expected answer : `[variable John prediction without consequences to that particular situation ]`\n",
      "6. **Planning with Limited Information**: Plan a sequence of actions given incomplete or uncertain information (e.g., plan the best way for John to get from one city to another knowing only route lengths).\n",
      "    Example input:   `[starting location], ending location, routes information and any other data related ]` \n",
      "Expected output: `\"steps taken by starting location , going through various other relevant destinations, destination of final desired location]`\n",
      "\n",
      "**Scoring Rubric**\n",
      "\n",
      "For each task, a clear scoring rubric will be developed to assess the LLM's performance. The rubrics will consider factors such as:\n",
      "\n",
      "* Accuracy\n",
      "* Completeness\n",
      "* Novelty (correctness of responses that don't correspond to expected solutions)\n",
      "* Coherence\n",
      "\n",
      "The scores for each answer will be calculated using statistical measures such as mean and standard deviation.\n",
      "\n",
      "**Protocols to Prevent/ Detect Memorized Answers and Prompt-Engineering Gaming**\n",
      "\n",
      "1.  **Exhaustive variation generation**: to cover unseen variants, multiple permutations of possible correct answers or solution paths \n",
      "2.  **Statistical analysis with variance control**: Implement a data analysis framework considering factors like frequency , consistency and patterns that may indicate cheating\n",
      "3.  **Input normalization and randomization** : use randomized test examples for each task type so output comparisons between the testing scenarios are fair compared to when given same but different input, prompts.\n",
      "4.  **Adversarial testing using carefully crafted inputs**: Develop a set of artificially created input variants that exploit the LLM's pattern-matching vulnerabilities while maintaining coherence and relevance.\n",
      "\n",
      "**Sample Sizes and Statistical Tests**\n",
      "\n",
      "To establish confidence in detecting a difference in capabilities between pattern-matching-like and genuinely compositional reasoning LLMs:\n",
      "\n",
      "*   **Large sample sizes:** Evaluate at least 100-500 tests for each task type, ensuring sufficient statistical power to detect even small differences.\n",
      "*   **Permutation tests or bootstrap samples:** Use techniques like permutation distributions or resampling to assess the probability of observing differences by chance.\n",
      "\n",
      "**Adversarial Behaviors**\n",
      "\n",
      "Potential adversarial behaviors include:\n",
      "\n",
      "1.  **Pattern-filling**: The LLM may fill in patterns it has learned from training data, even when the task requires genuine reasoning.\n",
      "2.  **Overfitting to specific prompts**: The AI model might perform well on a particular input but struggles with novel or similar prompts.\n",
      "3.  **Information-seeking behavior**: The model could be designed to gather additional information that isn't present in the prompt.\n",
      "\n",
      "**Designing Against these adversarial behaviors:**\n",
      "\n",
      "1.   **Introducing 'distractor' subtasks:** Adding unrelated smaller questions between two question that could potentially confuse AI output by distracting patterns detection capabilities.\n",
      "2.  **Adding 'unanswerable' responses**: To ensure the model will sometimes fail to answer due to lack of information or failure to recognize uncertainty, and if this fails to happen it would be indicative a pattern matching solution is on work.\n",
      "3.     **Randomizing some answers as correct/ incorrect examples**:   This way you can detect AI trying to infer things its not supposed to.\n",
      "\n",
      "**Testing Methodology Experimental Validation:**\n",
      "\n",
      "A series of experiments including large test sets for each task type in this set, using a combination of human evaluators (evaluance the output) and automated testing protocols (checks if LLM outputs are correct or incorrect  ).\n",
      "\n",
      "# Response from competitor 8\n",
      "\n",
      "## Adversarial-Resistant Test Suite for Evaluating LLM Reasoning Capabilities\n",
      "\n",
      "This design outlines a comprehensive test suite aimed at distinguishing between LLMs relying on pattern matching/memorization and those exhibiting genuine reasoning. The suite focuses on robustness through diverse tasks, strict controls, and statistical analysis.\n",
      "\n",
      "**1. Task Types & Examples:**\n",
      "\n",
      "Here are six task types designed to probe different aspects of reasoning:\n",
      "\n",
      "| Task Type        | Description                               | Example Input                                                              | Expected Output (demonstrating reasoning)                                                                                                        |\n",
      "|------------------|------------------------------------------|---------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| **1. Systematic Generalization (SGM)** | Extending patterns with quantitative rules. | \"If a car travels at 60 mph for 2 hours, how far does it travel? If a train travels at 80 mph for 3.5 hours, how far does it travel?\" | \"60 mph * 2 hours = 120 miles. 80 mph * 3.5 hours = 280 miles.\" (Correct application of distance = speed * time) |\n",
      "| **2. Causal Reasoning (CR)** | Predicting consequences of actions.      | \"John spilled water on the table. What's likely to happen?\"                | \"The table will likely become wet. The floor may become wet if the water isn't cleaned up.\"                                                   |\n",
      "| **3. Abstraction (AB)** |  Conceptual mapping and reformulation.   | “A robin is a bird. A sparrow is a bird. Which statement is most similar: ‘A robin is a songbird’ or ‘A sparrow is a songbird’?” | “‘A robin is a songbird’.” (Recognizing the abstraction of 'songbird' which applies to both robin and sparrow) |\n",
      "| **4. Counterfactuals (CF)** | Considering alternative scenarios.        | “If the sky were green, what color would the grass be?”                      | “The grass would be its normal color (green).” (Requires understanding of natural laws and relationships – grass is green independently of sky color) |\n",
      "| **5. Planning (PL)** |  Sequential reasoning with a goal.      | “I want to bake a cake. List the steps you would take.”                     | “1. Preheat the oven. 2. Gather ingredients (flour, sugar, eggs, etc.). 3. Mix the ingredients. 4. Pour batter into a pan. 5. Bake for [duration].”                     |\n",
      "| **6. Variable Binding (VB)** | Handling variables and relationships. | “If I have 3 apples and give 1 away, how many do I have?” ; “If x = 5 and y = 2, what is x + y?” | “2 apples.” ; “7” (Correct use of arithmetic and/or symbolic manipulation)                                                                              |\n",
      "\n",
      "\n",
      "**2. Scoring Rubric:**\n",
      "\n",
      "Each task will use a three-point rubric:\n",
      "\n",
      "* **3 Points:**  The response is entirely correct and demonstrates a clear understanding of the underlying reasoning.\n",
      "* **1 Point:** The response demonstrates a partial understanding or contains a minor error, but the core logic is present.\n",
      "* **0 Points:** The response is incorrect, nonsensical, or completely unrelated to the input.\n",
      "\n",
      "**3. Controls & Prevention Protocols:**\n",
      "\n",
      "* **Unseen Variant Generation:** Automatically generate variations of input prompts (synonyms, rephrasing, slightly altered contexts) to avoid memorized answers. Utilise paraphrasing models specifically trained for this purpose.\n",
      "* **Statistical Controls:**\n",
      "    * **Temperature Scaling:** Vary the LLM’s temperature setting (influencing randomness) across different runs. This adds a degree of noise and reduces reliance on memorized templates.\n",
      "    * **Prompt Length Limits:** Strict limits on prompt length to discourage overly verbose, memorization-driven responses.\n",
      "    * **Response Length Limits:** Similarly, set limits to discourage unnecessary elaboration.\n",
      "* **“Un-promptable” Data Injection:** Introduce subtle, non-obvious constraints into the input – e.g., “The answer must be a prime number,” or “The answer cannot be a multiple of 5.”\n",
      "* **Chain-of-Thought Debugging:** Integrate a Chain-of-Thought (CoT) model (e.g., a smaller, more reliable model) to automatically analyze the LLM’s reasoning chain and flag potential issues.\n",
      "* **Self-Reflection:**  Prompt the LLM to critically assess its own response – “Is your answer logically sound?  Explain your reasoning in detail.” This can identify where memorization is masking incorrect understanding.\n",
      "\n",
      "\n",
      "\n",
      "**4. Sample Sizes & Statistical Tests:**\n",
      "\n",
      "* **Sample Size:**  Run each task at least 100 times for each LLM being evaluated.  Larger sample sizes (200-300) would improve statistical power.\n",
      "* **Statistical Tests:**\n",
      "    * **Chi-Square Test:** Compare the distribution of 3, 1, and 0 scores across the different LLMs.  A significant Chi-Square value indicates a difference in performance.\n",
      "    * **Wilcoxon Signed-Rank Test:**  If the distributions are not normally distributed, use the Wilcoxon test for comparing paired samples (e.g., comparing the average score of LLM A to the average score of LLM B on the same task).\n",
      "    * **Cohen's d:**  Provides an effect size measure of the difference between means.\n",
      "\n",
      "**5. Adversarial Behaviors & Mitigation:**\n",
      "\n",
      "| Adversarial Behavior           | Mitigation Strategy                                              |\n",
      "|-------------------------------|------------------------------------------------------------------|\n",
      "| **Rule-Following Template Replication** | Frequent variant generation, statistical temperature scaling. |\n",
      "| **Obfuscated Reasoning**        | Chain-of-Thought debugging, “Self-Reflection” prompts.         |\n",
      "| **Overly Detailed, but Incorrect Responses** | Response length limits, constrained input, outlier detection. |\n",
      "| **Contextual Framing as Knowledge** | Unseen variant generation, tests specifically designed to expose this. |\n",
      "\n",
      "\n",
      "\n",
      "**6. Experimental Validation Plan:**\n",
      "\n",
      "* **Phase 1: Baseline Evaluation:** Conduct preliminary runs to identify prompt engineering vulnerabilities and refine the test suite.\n",
      "* **Phase 2: Focused Testing:** Execute the full test suite across multiple LLMs (e.g., GPT-4, Claude, open-source models).\n",
      "* **Phase 3: Iteration & Refinement:**  Analyze results from Phase 2 to identify where the suite needs improvement. Specifically, add tasks targeted at the weaknesses identified. Generate new unseen variants.\n",
      "* **Metrics for False Positives/Negatives:** Track the percentage of times an LLM receives a 3 (correct) or 0 (incorrect) score. Analyze which task types lead to the highest error rates.\n",
      "* **Continuous Monitoring:** Monitor the LLM’s responses over time to detect shifts in performance and potential gaming attempts.\n",
      "\n",
      "\n",
      "\n",
      "**Key Considerations & Future Work:**\n",
      "\n",
      "* **Ground Truth Data:** High-quality, expertly annotated ground truth data is crucial for accurate scoring.\n",
      "* **Model Architectures:**  The suite’s effectiveness will be heavily influenced by the underlying LLM architectures being evaluated.\n",
      "* **Dynamic Testing:**  Implement a system to automatically adapt the test suite based on LLM performance – focusing more on areas where the LLM is struggling.\n",
      "\n",
      "This detailed design provides a robust framework for evaluating LLM reasoning capabilities. By combining diverse tasks, stringent controls, and rigorous statistical analysis, we can gain valuable insights into the true level of understanding exhibited by these powerful models.  The iterative validation plan ensures the test suite remains relevant and effective as LLMs continue to evolve.\n",
      "\n",
      "# Response from competitor 9\n",
      "\n",
      "## Adversarial-Resistant Test-Suite for LLM Reasoning Capabilities\n",
      "\n",
      "This design focuses on distinguishing between \"pattern-matching\" LLMs and those exhibiting genuine compositional reasoning, emphasizing robustness and adversarial resistance.\n",
      "\n",
      "**I. Task Types & Examples (Probing Compositional Reasoning)**\n",
      "\n",
      "Here's six task types, each targeting a specific aspect of reasoning, with example inputs and expected outputs:\n",
      "\n",
      "**1. Systematic Generalization (SG):** Tests ability to generalize patterns beyond observed instances.\n",
      "    * **Input:** \"A blue block sits on a red block. A green block sits on a blue block. What happens when a yellow block sits on a green block?\"\n",
      "    * **Expected Output:** \"A yellow block sits on a green block.\" (Demonstrates transfer of the pattern 'X sits on Y')\n",
      "    * **Variant:**  Changing block colors, shapes, and adding distraction statements.\n",
      "\n",
      "**2. Causal Reasoning (CR):**  Tests understanding of cause-and-effect.\n",
      "    * **Input:** \"Rain often makes the ground wet. The ground is wet. Could it be raining?\"\n",
      "    * **Expected Output:** \"It could be raining, but there might be other reasons (e.g., sprinklers).\"  (Shows understanding of correlation vs. causation)\n",
      "    * **Variant:** Introducing misleading information or complex causal chains.\n",
      "\n",
      "**3. Abstraction (AB):** Tests extracting underlying principles and applying them to novel contexts.\n",
      "    * **Input:**  Present a series of stories following a pattern (e.g., \"The baker made bread, and the village had food. The builder built houses, and the town had shelter.\").  Then ask: \"The musician played music, what does the village now have?\"\n",
      "    * **Expected Output:** \"The village now has joy/entertainment/art.\"  (Demonstrates abstracting the relationship between profession and societal benefit)\n",
      "    * **Variant:**  Varying professions, social structures, and complexity of the relationship.\n",
      "\n",
      "**4. Counterfactuals (CT):** Tests ability to reason about \"what if\" scenarios.\n",
      "    * **Input:** \"The cat chased the mouse. If the cat had been asleep, what would have happened?\"\n",
      "    * **Expected Output:** \"The mouse might have escaped/found food/taken a different path.\" (Demonstrates mental simulation)\n",
      "    * **Variant:**  Complex counterfactuals incorporating multiple variables and constraints.\n",
      "\n",
      "**5. Planning (PL):** Tests formulating and executing a sequence of actions to achieve a goal.\n",
      "    * **Input:** \"You need to cross a river with a fox, a chicken, and a sack of grain. You have a boat that can only carry you and one other thing. What should you do?\"\n",
      "    * **Expected Output:** (A logical sequence) \"1. Take the chicken across. 2. Return alone. 3. Take the fox across. 4. Bring the chicken back. 5. Take the grain across. 6. Return alone. 7. Take the chicken across.\"\n",
      "    * **Variant:**  Increasing the number of items and complexities of constraints.\n",
      "\n",
      "**6. Variable Binding (VB):**  Tests understanding and manipulation of variables in abstract statements.\n",
      "    * **Input:**  \"Consider objects A and B.  If A is larger than B, and B is larger than C, then is A larger than C?\"\n",
      "    * **Expected Output:** \"Yes.\" (Demonstrates transitivity and variable relationship understanding.)\n",
      "    * **Variant:**  Introducing complex relationships, inequalities, and quantifiers.\n",
      "\n",
      "\n",
      "\n",
      "**II. Scoring Rubric (Example: Systematic Generalization)**\n",
      "\n",
      "* **0 Points:** Completely incorrect or gibberish.\n",
      "* **1 Point:** Identifies the presence of something on top. (e.g., \"A block sits on another block\"). Partial understanding.\n",
      "* **2 Points:** Correctly identifies the block color. (e.g., \"A yellow block sits on a green block\"). Shows pattern recognition.\n",
      "* **3 Points:** Correctly states the new configuration and acknowledges the inferred pattern.  (e.g. \"A yellow block sits on a green block, because the pattern is that one block sits on another\"). Demonstrates generalization.\n",
      "\n",
      "Similar rubrics would be developed for each task, prioritizing *reasoning steps* and avoiding reward for mere superficial correctness.\n",
      "\n",
      "\n",
      "\n",
      "**III. Adversarial Mitigation & Detection**\n",
      "\n",
      "* **Prompt Engineering Resistance:**\n",
      "    * **Unseen Variants:** Automatically generate numerous variants of each input using synonym replacement, paraphrasing, and structural alterations (e.g., reordering sentences, adding/removing context).  This challenges memorization reliance.\n",
      "    * **Noise Injection:** Add noise to the input (typos, grammatical errors, irrelevant information) to test robustness to imperfections.\n",
      "    * **Negative Constraints:** Explicitly forbid common \"cheating\" techniques in instructions (e.g., \"Do not simply repeat information\" or \"Do not try to find a direct answer online\").\n",
      "* **Memorization Detection:**\n",
      "    * **Statistical Controls:** Monitor next-token probabilities.  Low entropy or predictable next-token distributions strongly suggest memorization.\n",
      "    * **External Search Correlation:** Examine likelihood of extracted phrases from LLM's response appearing verbatim within top search results for the input prompt. High correlation points to retrieved factoids.  Implement a \"blurring\" technique to remove exact matches.\n",
      "    * **Cross-Task Consistency:**  Evaluate consistency between performance across different tasks probing similar underlying skills (e.g., Causal Reasoning & Counterfactuals). Large inconsistencies could show task-specific memorization.\n",
      "\n",
      "\n",
      "\n",
      "**IV. Sample Size & Statistical Tests**\n",
      "\n",
      "* **Sample Size:** Begin with **N=200-300** prompts per task. This allows for reasonable power (estimated using pilot data). Increase if variance is high.\n",
      "* **Statistical Tests:**\n",
      "    * **t-tests/Mann-Whitney U-tests:** Compare mean scores of LLMs for each task.\n",
      "    * **ANOVA/Kruskal-Wallis tests:** Compare performance across multiple LLMs simultaneously.\n",
      "    * **Bayesian Hypothesis Testing:**  Provides probability estimates of LLMs exhibiting superior reasoning ability, accommodating prior beliefs about the task difficulty and model complexity.\n",
      "    * **Effect Size Calculation:** Cohen's d or similar metrics capture the practical significance of observed differences.\n",
      "\n",
      "\n",
      "\n",
      "**V. Potential Adversarial Behaviors & Mitigation**\n",
      "\n",
      "* **\"Hallucinating\" Justifications:** LLMs might produce superficially sound explanations while failing to reason correctly. (Mitigation: Rubric focuses on reasoning steps, not *only* output correctness.)\n",
      "* **\"Exploiting\" Prompts:** LLMs might find a narrow prompt structure yielding high scores, not demonstrating true generalization. (Mitigation: Extensive prompt variants & noise injection)\n",
      "* **\"Circumventing\" Instructions:**  LLMs might ignore negative constraints or semantic reinterpretation of the prompt. (Mitigation: Re-phrasing instruction and negative constraints frequently to avoid easy circumvention)\n",
      "\n",
      "**VI. Experimental Validation & Iteration**\n",
      "\n",
      "* **Human Baseline:** Establish a human baseline score (N=30) for each task.  LLMs must demonstrably surpass this baseline to be considered capable.\n",
      "* **False Positive/Negative Rates:**\n",
      "    * **False Positives:** LLMs achieving high scores despite lacking true reasoning skills. This necessitates rigorous rubric validation and adversarial testing.\n",
      "    * **False Negatives:** LLMs possessing genuine reasoning skills being incorrectly classified as pattern-matchers. This requires re-evaluation of task design and scoring.\n",
      "* **Iteration:**\n",
      "    1. **Analysis of Failure Cases:**  Deep dive into why individual LLMs failed on specific prompts. Categorize failures by type (memorization, misunderstandings, etc.).\n",
      "    2. **Task Enhancement:** Modify tasks to directly address weaknesses revealed in the analysis.\n",
      "    3. **Rubric Refinement:**  Adjust the scoring rubric to be more granular and discriminatory.\n",
      "    4. **Adversarial Prompt Expansion:** Generate new adversarial prompts specifically targeting identified vulnerabilities.\n",
      "\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "This test-suite design goes beyond simple accuracy comparisons. By focusing on compositional reasoning through targeted tasks, rigorous adversarial mitigation, and thorough validation, it aims to reliably differentiate true reasoning capabilities from superficial pattern-matching in LLMs, ultimately driving progress in AI safety and alignment.  This is an iterative process, constantly adapting to new LLM innovations and adversarial strategies.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are judging a competition between 9 competitors.\n",
      "Each model has been given this question:\n",
      "\n",
      "Design a comprehensive, adversarial‑resistant test-suite that reliably distinguishes between black‑box LLMs that rely primarily on pattern matching and memorization and LLMs that demonstrate genuine compositional reasoning and robust generalization; for your test‑suite, provide (1) at least six concrete task types with example inputs and expected outputs that probe different aspects of compositional reasoning (e.g., systematic generalization, causal reasoning, abstraction, counterfactuals, planning, variable‑binding), (2) a clear scoring rubric for each task, (3) protocols to prevent or detect memorized answers and prompt‑engineering gaming (including generation of unseen variants and statistical controls), (4) proposed sample sizes and statistical tests to assert with high confidence a difference in capability, (5) possible adversarial behaviors the LLM might use to appear capable and how your design addresses them, and (6) an experimental validation plan including metrics for false positives/negatives and how to iterate on the suite based on results?\n",
      "\n",
      "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
      "Respond with JSON, and only JSON, with the following format:\n",
      "{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}\n",
      "\n",
      "Here are the responses from each competitor:\n",
      "\n",
      "# Response from competitor 1\n",
      "\n",
      "Below is a complete, practical test‑suite design for reliably distinguishing LLMs that primarily pattern‑match / memorize from LLMs that show genuine compositional reasoning and robust generalization. It is written to be implementable against black‑box LLMs (only I/O access). The suite mixes synthetic, randomized and structured tasks, uses statistical controls and adversarial checks, and includes validation and iteration plans.\n",
      "\n",
      "Summary of components\n",
      "- (1) Six task types (with concrete examples and expected outputs) that probe different aspects of compositional reasoning.\n",
      "- (2) A clear scoring rubric for each task.\n",
      "- (3) Protocols to prevent/detect memorized answers and prompt‑engineering gaming (randomization, held‑out vocab, paraphrase invariance, adversarial decoys, statistical controls).\n",
      "- (4) Sample sizes and statistical tests for confident capability differences.\n",
      "- (5) Likely adversarial behaviors and mitigations.\n",
      "- (6) Experimental validation plan with metrics and iteration strategy.\n",
      "\n",
      "1) Task types — descriptions, concrete inputs and expected outputs\n",
      "Each task type includes rationale (what aspect it probes), a canonical task template, 2 example items and their expected outputs. All tasks are designed to support automatic grading by canonicalization or deterministic execution where feasible.\n",
      "\n",
      "Important: In production tests, instantiate each template with many randomized variants (random tokens, symbol renamings, unseen combinations). Use synthetic vocabulary (random strings) in many items (see protocols below) to avoid memorization.\n",
      "\n",
      "Task A — Systematic generalization (composition of primitive instructions)\n",
      "- What it probes: ability to apply known primitive operations in novel combinations, i.e., SCAN‑style compositionality / systematicity.\n",
      "- Template: Define a small set of primitive actions and composition operators. Ask model to produce ground‑truth composed actions.\n",
      "- Example primitives (in an item):\n",
      "  \"Primitive actions: WALK (W) = move forward 1; JUMP (J) = move forward 2; TURN_LEFT (L) = rotate left. Composition: X and then Y means do X then Y; twice(X) means do X two times; opposite(X) means do TURN_LEFT twice then X then TURN_LEFT twice.\"\n",
      "  Input 1: \"Instruction: opposite(twice(JUMP)) then WALK.\"\n",
      "  Expected output 1: The sequence of primitive actions expanded: \"TURN_LEFT, TURN_LEFT, JUMP, JUMP, TURN_LEFT, TURN_LEFT, WALK\" (canonicalized as comma‑sep).\n",
      "  Input 2 (novel composition): \"Instruction: twice(opposite(WALK)) then JUMP.\"\n",
      "  Expected output 2: \"TURN_LEFT, TURN_LEFT, WALK, TURN_LEFT, TURN_LEFT, WALK, JUMP\"\n",
      "- Why hard for memorization: create many primitives and composition rules and withhold many combinations from training set; test on novel nesting depths and permutations.\n",
      "\n",
      "Task B — Variable binding and long‑range reference (symbolic variables)\n",
      "- What it probes: ability to bind variables to values, carry and reuse bindings across steps, disambiguate references like \"the one referenced two steps earlier\".\n",
      "- Template: Provide assignments, transformations and queries referencing earlier variables (use randomized variable names).\n",
      "- Example:\n",
      "  Input 1:\n",
      "    \"Let a = 7, b = a + 5, c = 2*b. Now update a = c - 3. What is b + a?\"\n",
      "  Expected output 1: compute stepwise:\n",
      "    b = a + 5 (using original a=7) => b=12\n",
      "    c = 2*b => c=24\n",
      "    a = c - 3 => a=21\n",
      "    b + a = 12 + 21 = 33\n",
      "    Output: \"33\"\n",
      "  Input 2 (using randomly named vars, novel referencing):\n",
      "    \"Let X1 = 4, Y_2 = X1 * 3, Z = Y_2 - X1. Then set X1 = Z + 1. What is Y_2 - X1?\"\n",
      "  Expected output 2:\n",
      "    Y_2 = 12, Z = 8, X1 becomes 9. Y_2 - X1 = 3. Output: \"3\"\n",
      "- Why hard for memorization: use long sequences (10+ assignments) and random token names; require maintaining bindings across updates.\n",
      "\n",
      "Task C — Causal reasoning and counterfactuals (graph / structural causal model)\n",
      "- What it probes: causal inference and do‑interventions, not mere correlation or associative retrieval.\n",
      "- Template: Give a small causal graph or set of structural equations. Ask consequence of interventions or counterfactual statements.\n",
      "- Example:\n",
      "  Input 1:\n",
      "    \"Variables: A -> B (B = A + noise), B -> C (C = 2*B). If we set A = 0 (do(A=0)), what happens to C? Provide final numeric relation.\"\n",
      "  Expected output 1:\n",
      "    After do(A=0): B = 0 (+noise mean 0) → C = 0. Output: \"C becomes 0 (given deterministic functions, C = 2*B, so C = 0).\"\n",
      "  Input 2 (counterfactual):\n",
      "    \"Structural model: X = 3*U, Y = X + V. Observed U=2, V=1 (so observed X=6, Y=7). Counterfactual: if U had been 1 (but V unchanged), what would Y have been?\"\n",
      "  Expected output 2:\n",
      "    New X = 3*1 = 3. Y = X + V = 3 + 1 = 4. Output: \"4\"\n",
      "- Why hard for memorization: Use freshly generated graphs and numeric values, ask do‑intervention vs observation differences; require understanding intervention semantics.\n",
      "\n",
      "Task D — Abstraction and analogical mapping (learn mapping rule, apply to novel exemplars)\n",
      "- What it probes: ability to induce abstract relations from examples and apply them to new tokens not seen in training.\n",
      "- Template: Provide a few mapping examples using a tiny invented \"language\", then ask to map new items applying the same relation.\n",
      "- Example:\n",
      "  Input 1:\n",
      "    \"Rule examples: hefo -> jopi, goro -> luma. Now apply the same transformation to zeta.\"\n",
      "    (Transformation pattern: prepend 'j' and substitute vowels mapping e->o, o->u — but hidden to the model; only examples show behavior.)\n",
      "  Expected output 1:\n",
      "    If pattern mapping produces j + (vowel shift) then zeta -> juta (example result). But to make deterministic and unambiguous, use explicitly definable transformations in generation pipeline so expected output can be computed.\n",
      "  Input 2 (using symbols):\n",
      "    \"Examples: blim -> ba-lim, srun -> sa-run. Apply to krup.\"\n",
      "    Expected output 2: \"ka-krup\" (or whatever canonical transformation defined by the example set).\n",
      "- Why hard for memorization: use arbitrary synthetic token spaces and withhold large portions of mapping space; require generalizing relational rule.\n",
      "\n",
      "Task E — Planning and hierarchical problem solving (constrained optimization)\n",
      "- What it probes: ability to plan multi-step actions under constraints, to generalize plan construction rules to larger/new configurations.\n",
      "- Template: Provide a grid or a pickup/delivery planning problem with constraints (capacity, order) and ask for a (near‑)optimal step sequence or cost.\n",
      "- Example:\n",
      "  Input 1:\n",
      "    \"Agent at (0,0) on 3x3 grid. Boxes at (1,0) (A), (2,2) (B). Deliver A then B to goal (0,2). Agent can carry 1 box. Moves: Up/Down/Left/Right cost 1; Pickup/Drop cost 0. Plan minimal steps.\"\n",
      "  Expected output 1:\n",
      "    One optimal plan, canonicalized: \"Right, Pickup, Up, Up, Drop, Right, Right, Pickup, Left, Left, Up, Drop\" — or a canonical minimal step count and sequence. Expected minimal length numeric also acceptable: \"Minimal steps = 10 (sequence: ...)\"\n",
      "  Input 2 (scale-up generalization):\n",
      "    Same rules but 5x5 grid and three boxes with constraint that order must be A then third then B. Ask for plan.\n",
      "- Why hard for memorization: use random placements, require true planning, and scale tests to larger grids / more boxes than in training.\n",
      "\n",
      "Task F — Nested recursion / compositional evaluation (interpreted mini‑language)\n",
      "- What it probes: ability to parse and evaluate expressions under user‑defined semantics, including nested composition and recursion, and to generalize to deeper nesting than seen in examples.\n",
      "- Template: Define a tiny functional language (e.g., inc(x) = x+1, dbl(x) = 2*x, swap(a,b) returns pair reversed), then evaluate nested expressions.\n",
      "- Example:\n",
      "  Input 1:\n",
      "    \"Definitions: inc(x) = x+1; dbl(x) = 2*x; compose(f,g)(x) = f(g(x)). Evaluate compose(inc, dbl)(3).\"\n",
      "  Expected output 1:\n",
      "    dbl(3)=6; inc(6)=7. Output: \"7\"\n",
      "  Input 2 (deeper / novel nesting with random function names):\n",
      "    Randomly name functions: f_z(x)=3*x, g_q(x)=x-2. Evaluate f_z(g_q(g_q(10))).\n",
      "    Expected output 2:\n",
      "    g_q(10)=8; g_q(8)=6; f_z(6)=18. Output: \"18\"\n",
      "- Why hard for memorization: use randomized function names and nesting depths beyond training examples.\n",
      "\n",
      "Cross‑task consistency probes (meta‑checks)\n",
      "- After initial answer, rephrase problem, permute variable names or ask logically equivalent forms (alpha‑renaming). A reasoning model should give consistent answers; a memorizer or pattern matcher will often fail.\n",
      "\n",
      "2) Scoring rubrics\n",
      "For each task we use a combination of exact match (for deterministic outputs), graded structural equivalence, partial credit for intermediate step correctness, and consistency checks. All scoring should be automated where possible.\n",
      "\n",
      "General guidelines\n",
      "- Normalize outputs: strip punctuation, collapse whitespace, canonicalize commutative orders if problem allows, convert numbers to canonical numeric form.\n",
      "- Where multiple correct sequences exist (planning), accept any plan that meets constraints and minimal (or near‑minimal) length. Validate by deterministic simulator.\n",
      "- Require explanation optionally as corroboration — explanations are graded separately and used to detect shallow patterning (see later).\n",
      "\n",
      "Detailed rubrics per task\n",
      "\n",
      "Task A — Systematic generalization\n",
      "- 0/1 exact correctness: full credit (1.0) if produced canonical expanded action sequence exactly matches ground truth.\n",
      "- Partial credit 0.5 if sequence is correct up to reordering of independent commuting actions or contains only superficial tokenization differences.\n",
      "- 0 if wrong action types or wrong ordering violating composition semantics.\n",
      "- Bonus +0.2 if model also outputs a correct short reasoning trace (e.g., shows stepwise expansion).\n",
      "\n",
      "Task B — Variable binding\n",
      "- Full credit (1.0) if final numeric (or symbolic) answer matches.\n",
      "- Partial credit 0.5 if intermediate steps are internally inconsistent but final answer reachable by plausible alternative interpretation; give 0 if incorrect.\n",
      "- Additionally, ask for an explanation of each assignment; give +0.1 if explanation justifies final answer.\n",
      "\n",
      "Task C — Causal reasoning / counterfactuals\n",
      "- Full credit (1.0) for correct intervention answer plus correct reasoning (identifying difference between observation and do()).\n",
      "- Partial credit 0.5 if numeric outcome correct but model incorrectly describes the causal/non‑causal distinction.\n",
      "- 0 for wrong or inconsistent counterfactual.\n",
      "\n",
      "Task D — Abstraction / analogy\n",
      "- Full credit (1.0) for correct mapped output on held‑out tokens.\n",
      "- Partial credit 0.5 if pattern partially matched (e.g., one of two transformation components correct).\n",
      "- Zero if incorrect mapping.\n",
      "\n",
      "Task E — Planning\n",
      "- Full credit (1.0) if plan meets all constraints and is optimal (or within predetermined optimality gap, e.g., +0 steps for optimal or +1 allowance for near‑optimal).\n",
      "- Partial credit 0.75 if plan valid but suboptimal within allowed slack; 0.4 if partially valid or violates minor constraints; 0 if plan invalid/unexecutable.\n",
      "- Also score plan length and feasibility automatically using a simulator.\n",
      "\n",
      "Task F — Nested evaluation\n",
      "- Full credit (1.0) if evaluated result correct.\n",
      "- Partial credit 0.5 if calculation partially correct or correct for a different but plausible semantics.\n",
      "- 0 if wrong.\n",
      "\n",
      "Consistency / adversarial checks (applies across tasks)\n",
      "- Self‑consistency score: ask the same question twice with variable renaming and paraphrase. Award extra trust when answers are stable. If model flips answers > threshold (e.g., >10% of items), mark suspicious.\n",
      "\n",
      "Aggregate scoring\n",
      "- Report per‑task accuracy and an overall composite score weighted equally or by task importance.\n",
      "- Also report error types: arithmetic errors, reference errors, plausibility but contradiction, inconsistent explanation, etc.\n",
      "\n",
      "3) Protocols to prevent/detect memorized answers and prompt‑engineering gaming\n",
      "Use a multi‑layered approach: (A) avoid giving any canonical public benchmark examples in test items; (B) randomization and synthetic languages; (C) paraphrase invariance; (D) decoys and traps; (E) statistical controls and cross‑validation; (F) output verification via external simulators.\n",
      "\n",
      "A. Synthetic, randomized, and held‑out elements\n",
      "- Random tokens and names: generate variable, function, and symbol names randomly (e.g., strings of 4–6 chars drawn from letters not composing common words). Example: X1 -> \"qerf\", function \"f_x\".\n",
      "- Random numeric seeds: numbers, positions, and graphs sampled uniformly from ranges beyond typical training corpora.\n",
      "- Use on‑the‑fly generated domain rules (e.g., transformation rules) so items are unlikely to appear in model training.\n",
      "- For each template, reserve a held‑out set of compositions (combinations of primitives, deeper nestings) not revealed anywhere else. Test on these held‑outs.\n",
      "\n",
      "B. Paraphrase invariance & renaming checks\n",
      "- For each item, probe with several paraphrases and alpha‑renamings (rename all symbols consistently). A reasoning model gives consistent answers; a pattern matcher that memorized specific surface forms will often fail on renamed forms.\n",
      "- Examples:\n",
      "  Item 1: original problem\n",
      "  Item 1a: same problem with variable renaming\n",
      "  Item 1b: logically equivalent restatement\n",
      "  If answers disagree, down‑weight confidence.\n",
      "\n",
      "C. Held‑out composition splits (systematic generalization protocol)\n",
      "- Use compositional splits analogous to SCAN: train/evaluate pairs are constructed so primitives are seen in training but some combinations are held out and used only for testing. Similarly for mapping rules and function compositions. This tests systematic generalization rather than memorization.\n",
      "\n",
      "D. Trap items and decoys\n",
      "- Embed \"publicly memorizeable\" variants that look like standard benchmarks but with small modifications. A pattern‑matcher that only recognizes the public form will output the public memorized answer rather than adapt; detect by comparing to valid answer for the modified item.\n",
      "- Adversarial decoys: ask for outputs in both canonical and unusual formats to catch prompt‑tuned models that only work in one style.\n",
      "\n",
      "E. Explain‑and‑verify protocol\n",
      "- Request both an answer and a concise 1–3 step justification. Use an automated verifier to check whether the explanation logically entails the answer (e.g., compute intermediate values or check stated steps). Pattern matchers often hallucinate plausible justifications; the verifier will catch mismatches between claimed steps and actual output.\n",
      "\n",
      "F. Multiple independent samplings + self‑consistency\n",
      "- Query the model multiple times with temperature sampling to observe variability. Pattern matching models tuned to be deterministic may produce the same memorized output; reasoning models may produce either stable or explainable variations. Use statistics of answer distribution to infer brittleness.\n",
      "\n",
      "G. Cross‑model and cross‑item statistical controls\n",
      "- Randomly split items into multiple forms; estimate item difficulty using item‑response theory (IRT) to control for item variance when comparing models.\n",
      "\n",
      "H. Avoid prompting that guides the model to memorize the test pattern\n",
      "- Randomize prompt templates; do not leak examples from the test set in prompts. Use standardized prompt skeletons with random syntactic surfaces.\n",
      "\n",
      "I. Time & leakage controls\n",
      "- Run tests before and after major model updates; if possible, time‑stamp datasets and store seeds to detect leak. Rotate new item sets periodically.\n",
      "\n",
      "4) Sample sizes and statistical tests\n",
      "Goal: confidently assert a difference in capability between two black‑box LLMs (Model A and Model B).\n",
      "\n",
      "Definitions and assumptions:\n",
      "- Treat each model's response to an item as correct/incorrect (binary) for primary analysis. Use per‑task accuracy as primary metric; composite score as secondary.\n",
      "- Tests are paired (same items evaluated by both models). Use paired tests (McNemar’s test or paired permutation) to leverage within‑item correlations.\n",
      "\n",
      "Power & sample size rules of thumb\n",
      "- For a two‑proportion comparison (unpaired) with modest effect sizes (difference = 10–20 percentage points), recommended sample per model per task ≈ 300–500 items to get power ≈ 0.8 at alpha = 0.05. But because tests are paired, required number of distinct items is lower.\n",
      "\n",
      "Paired sample: approximate calculation\n",
      "- If expecting Model A accuracy = 50% and Model B = 70% on a task (difference 20 ppt), and assuming moderate within‑item correlation, ~150–250 paired items suffices for 80% power (McNemar or paired permutation). If difference expected smaller (e.g., 10 ppt), need ~400–800 items.\n",
      "- Conservative recommended sample sizes:\n",
      "  - Per task: 400 items (distinct problems) sampled from randomized template variants.\n",
      "  - Per composite suite (6 tasks): 6 * 400 = 2400 items total. You can distribute effort (e.g., 400 per high‑importance task, 200 per less critical).\n",
      "\n",
      "Statistical tests and controls\n",
      "- Primary test: paired permutation test on per‑item difference (nonparametric, robust) or McNemar’s test for binary paired data.\n",
      "- Secondary tests:\n",
      "  - Mixed‑effects logistic regression (item as random effect, model as fixed effect) to control for item difficulty and estimate model effect across heterogeneous items.\n",
      "  - Bootstrapped confidence intervals (resample items) for per‑task accuracy difference.\n",
      "  - Multiple comparisons correction: Benjamini–Hochberg when testing multiple tasks.\n",
      "- Report effect sizes (difference in proportions, odds ratio), 95% CI, and p‑values.\n",
      "- Power analyses: compute required sample size for expected minimal detectable effect size BEFORE running full test; pilot with small N to estimate item variance.\n",
      "\n",
      "Decision thresholds\n",
      "- Define thresholds for claiming superiority. Example: model A is better than B on task if:\n",
      "  - Paired difference in accuracy > δ (e.g., δ = 0.10) AND\n",
      "  - p < 0.01 after multiple comparisons correction AND\n",
      "  - Effect robust to bootstrap and mixed‑effects regression controlling for item variance.\n",
      "\n",
      "5) Possible adversarial behaviors and countermeasures\n",
      "Below are adversarial strategies a model might use to appear capable, and corresponding mitigations in the suite.\n",
      "\n",
      "Adversarial: Surface retrieval / memorized sequence regurgitation\n",
      "- Behavior: Model stores fixed outputs for common instructions; returns memorized outputs for superficially similar prompts.\n",
      "- Mitigation: synthetic tokens, held‑out compositions, trap items that mimic public benchmarks with slight changes; paraphrase and renaming consistency checks; require correct computation on non‑natural synthetic data.\n",
      "\n",
      "Adversarial: Template matching / prompt engineering (exploiting fixed prompt formats)\n",
      "- Behavior: Model trained to respond well to specific prompt templates; game tests by matching those templates exactly.\n",
      "- Mitigation: randomize prompt phrasing and structure; test with out‑of‑template paraphrases; hide test instructions across several rewordings.\n",
      "\n",
      "Adversarial: Post‑hoc plausible explanations (hallucinated chain‑of‑thought)\n",
      "- Behavior: Model produces a plausible‑looking explanation that does not correspond to internal computation.\n",
      "- Mitigation: automatic verification of explanation steps; require computation that can be executed in a simulator and check consistency between explanation and final answer. Ask for explicit intermediate numeric values that can be validated.\n",
      "\n",
      "Adversarial: Strategic stochasticity (choose most socially pleasing answer)\n",
      "- Behavior: Model outputs an answer that appears reasonable but isn't derived by correct reasoning.\n",
      "- Mitigation: use items with unique numeric/structural answers and built simulators to check exactness. Evaluate per‑answer consistency across multiple samples.\n",
      "\n",
      "Adversarial: Learning to memorize test suite by repeated exposure (test leakage)\n",
      "- Behavior: Providers fine‑tune model on leaked items and then pass tests.\n",
      "- Mitigation: maintain a large pool of test items, rotate, and ensure many items are generated on the fly. Keep some items only as one‑time use. Audit and track tests over time to detect sudden accuracy jumps.\n",
      "\n",
      "Adversarial: Exploiting world knowledge\n",
      "- Behavior: Model uses web knowledge (e.g., named entities) instead of reasoning.\n",
      "- Mitigation: use synthetic domains and names that aren't in corpus; prefer abstract variables and gibberish tokens.\n",
      "\n",
      "Adversarial: Exploiting consistent wrong heuristics (e.g., always answer \"no\" to avoid errors)\n",
      "- Behavior: Model learns a default safe answer that is occasionally correct by chance.\n",
      "- Mitigation: measure baselines and chance rates and compare; include control items where a naive heuristic fails frequently.\n",
      "\n",
      "6) Experimental validation plan, metrics for false positives/negatives, and iteration\n",
      "Validation phases\n",
      "- Phase 0 — Unit tests: small sample (N=50 per task) to verify item generation, canonicalization, and grading logic.\n",
      "- Phase 1 — Pilot: evaluate 3–5 diverse models (known baselines: simple pattern models, older LLM, and an advanced recent LLM) on a larger pilot (N=200 per task). Use pilot to estimate item difficulties and variance.\n",
      "- Phase 2 — Full evaluation: run full suite (recommended N=400 per task) across models under test.\n",
      "- Phase 3 — Adversarial robustness checks: invite models purposely trained to game tests; analyze failures and iterate.\n",
      "\n",
      "Metrics\n",
      "- Primary: per‑task accuracy (binary), composite accuracy.\n",
      "- Secondary: explanation correctness rate; self‑consistency rate (fraction of paraphrases with same answer); variability under sampling (entropy of answers).\n",
      "- Discrimination metrics: Area Under ROC curve if defining continuous scoring; effect sizes and odds ratios.\n",
      "- Error analysis metrics:\n",
      "  - False positives (FP): model flagged as reasoning‑capable but actually relying on memorization. Estimate by hand‑inspecting suspicious items and adversarially designed holdouts. Also measure cases where model passes synthetic tasks but fails renamed/scrubbed variants.\n",
      "  - False negatives (FN): model truly capable but fails tests (e.g., due to prompt format, minor ambiguity). Detect by analyzing cases where model yields correct reasoning traces but output form differs; retrain canonicalizer.\n",
      "\n",
      "Estimating FP/FN rates\n",
      "- Use validation with ground‑truth labeled model types:\n",
      "  - Create/collect models we know: (i) a trained memorization baseline (e.g., n‑gram or memorization‑augmented seq2seq), (ii) a symbolic reasoner (deterministic oracle), (iii) a hybrid model.\n",
      "  - Run full suite and measure classification (declared as reasoning vs not) vs known ground truth. Compute sensitivity (TPR) and specificity (1−FPR). Tune decision thresholds to maintain acceptable FP (e.g., <5%) while maximizing sensitivity.\n",
      "\n",
      "Iterative refinement based on results\n",
      "- Analyze item‑level statistics: identify low discrimination items (very easy or very hard) using IRT or item discrimination index. Remove or rework items with poor discrimination.\n",
      "- Increase adversarial item proportion if many models game the suite.\n",
      "- Expand synthetic vocabulary and composition depth if memorization leakage is observed.\n",
      "- Automate item generation pipelines to produce fresh held‑out sets periodically.\n",
      "- Maintain an audit log of test runs and items to detect potential leakage and overfitting by model providers.\n",
      "\n",
      "Example iteration cycle\n",
      "1. Pilot run → compute per‑task accuracies, item difficulty.\n",
      "2. Identify items where memorizing baselines scored as well as strong reasoning oracle → mark for redesign (add randomization, deeper nesting, or rename tokens).\n",
      "3. Add paraphrase/renaming tests for items showing potential prompt‑template exploitation.\n",
      "4. Rebalance item pool to maintain discrimination and rerun.\n",
      "\n",
      "Practical considerations and recommended implementation checklist\n",
      "- Implement automatic generators for each template with reproducible random seeds.\n",
      "- Implement deterministic simulators/validators for each task (action sequence executor, arithmetic evaluator, planner checker) so grading is exact.\n",
      "- For each item store (seed, tokens, canonical answer, distractors).\n",
      "- Maintain multiple prompt forms and sample at random per query.\n",
      "- Log all outputs, probabilistic sampling states (temperature), and timestamps.\n",
      "- Use paired testing (same items across all models) to maximize power and fairness.\n",
      "- Publicly release small sanitized subsets for community verification but keep main test pools private to reduce overfitting.\n",
      "\n",
      "Concluding guidance\n",
      "- No single test proves genuine reasoning forever; use this suite as a robust, multi‑task battery that measures “behavioral evidence” of compositional reasoning and generalization.\n",
      "- Emphasize cross‑task consistency, randomness/held‑out composition, and rigorous statistical testing.\n",
      "- Make verification automated where possible (simulators) and include human review for ambiguous failures.\n",
      "- Iterate the pool and keep fresh synthetic items to reduce memorization/leakage risk.\n",
      "\n",
      "If you’d like, I can:\n",
      "- Produce a reference implementation (Python) that generates randomized items for each template and includes canonicalizers and simulators for automated scoring.\n",
      "- Compute precise sample size calculations for specific expected effect sizes you care about and provide a ready‑to‑run experimental script.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "# Comprehensive Test Suite for Distinguishing Pattern Matching from Compositional Reasoning in LLMs\n",
      "\n",
      "## Executive Summary\n",
      "\n",
      "This test suite employs six task categories designed to probe genuine compositional reasoning while resisting memorization and gaming. Each task incorporates systematic variation, procedural generation, and adversarial controls to ensure that success requires compositional understanding rather than pattern matching.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Task Types with Examples\n",
      "\n",
      "### Task 1: Systematic Compositional Generalization (Novel Recombinations)\n",
      "\n",
      "**Objective**: Test ability to combine known primitives in novel configurations never seen during training.\n",
      "\n",
      "**Design Principle**: Use a microworld with defined rules, then test combinations statistically unlikely to appear in training data.\n",
      "\n",
      "**Example Input**:\n",
      "```\n",
      "Rules of the GLORP system:\n",
      "- FLIM(x) reverses the order of elements in x\n",
      "- DRAK(x, y) interleaves elements from x and y\n",
      "- ZOOP(x) duplicates every second element in x\n",
      "\n",
      "Training examples:\n",
      "FLIM([A, B, C]) = [C, B, A]\n",
      "DRAK([1, 2], [3, 4]) = [1, 3, 2, 4]\n",
      "ZOOP([P, Q, R]) = [P, Q, Q, R]\n",
      "\n",
      "Test question:\n",
      "What is FLIM(DRAK(ZOOP([X, Y]), [M, N]))?\n",
      "```\n",
      "\n",
      "**Expected Output**: \n",
      "```\n",
      "Step-by-step reasoning:\n",
      "1. ZOOP([X, Y]) = [X, Y, Y]\n",
      "2. DRAK([X, Y, Y], [M, N]) = [X, M, Y, N, Y]\n",
      "3. FLIM([X, M, Y, N, Y]) = [Y, N, Y, M, X]\n",
      "\n",
      "Answer: [Y, N, Y, M, X]\n",
      "```\n",
      "\n",
      "**Scoring Rubric**:\n",
      "- 4 points: Correct answer with valid step-by-step reasoning\n",
      "- 3 points: Correct answer with minor reasoning errors\n",
      "- 2 points: Incorrect answer but demonstrates understanding of composition\n",
      "- 1 point: Partially correct intermediate steps\n",
      "- 0 points: Incorrect with no valid reasoning\n",
      "\n",
      "**Variants for Anti-Memorization**:\n",
      "- Generate 10,000 unique microworlds with different operation names, symbols, and rule sets\n",
      "- Use procedural generation with random seeds\n",
      "- Operation names drawn from pronounceable non-words (e.g., BLICKET, WUGGY, FEPS)\n",
      "- Vary depth of composition (2-5 levels)\n",
      "- Test both symbolic and numeric domains\n",
      "\n",
      "---\n",
      "\n",
      "### Task 2: Causal Reasoning Under Intervention (Counterfactual Inference)\n",
      "\n",
      "**Objective**: Distinguish correlation from causation and reason about interventions.\n",
      "\n",
      "**Design Principle**: Present causal graphs implicitly through scenarios, then test counterfactual reasoning that requires understanding causal structure.\n",
      "\n",
      "**Example Input**:\n",
      "```\n",
      "Scenario: In the town of Millbrook, the following patterns have been observed over 10 years:\n",
      "\n",
      "- When the reservoir level is high, the water treatment plant runs at full capacity\n",
      "- When the treatment plant runs at full capacity, downtown water pressure is strong\n",
      "- When downtown water pressure is strong, the fountain in Central Park operates\n",
      "- The reservoir level depends only on rainfall\n",
      "- Rainfall also directly affects whether street cleaning happens (rain = no cleaning)\n",
      "\n",
      "Historical data shows that on days when the fountain operates, streets are usually dirty.\n",
      "\n",
      "Question 1: The town installs a new pump that allows the fountain to operate regardless of downtown water pressure. After this intervention, will the streets be cleaner or dirtier on days when the fountain operates, compared to before?\n",
      "\n",
      "Question 2: Explain your reasoning using the causal structure.\n",
      "```\n",
      "\n",
      "**Expected Output**:\n",
      "```\n",
      "Answer: The streets will be CLEANER on fountain-operating days after the intervention.\n",
      "\n",
      "Reasoning: \n",
      "Before intervention: Fountain operating → High reservoir → Recent rainfall → No street cleaning → Dirty streets\n",
      "\n",
      "The correlation between fountain operation and dirty streets was due to a common cause (rainfall), not because the fountain caused dirty streets.\n",
      "\n",
      "After intervention: The fountain can operate without rainfall, breaking the spurious correlation. Fountain operation is now independent of rainfall, so there's no reason to expect systematically dirtier streets.\n",
      "```\n",
      "\n",
      "**Scoring\n",
      "\n",
      "# Response from competitor 3\n",
      "\n",
      "This test-suite, titled **CORE-Eval (Compositional & Operational Reasoning Evaluation)**, is designed to bypass the \"stochastic parrot\" effect by focusing on out-of-distribution (OOD) tasks that require the dynamic manipulation of novel variables and rules.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Concrete Task Types\n",
      "\n",
      "#### Task A: Recursive Nested State Tracking (Variable Binding)\n",
      "*   **Concept:** Track the state of objects through recursive \"if-then\" swaps and containment changes.\n",
      "*   **Input:** \"There are three boxes: Red, Blue, and Green. Red contains a 'Glint'. Blue contains a 'Spark'. Green is empty. Rule 1: If an object is moved to an empty box, its name reverses. Rule 2: If two boxes swap, their contents swap. Step 1: Swap Red and Blue. Step 2: Move contents of Blue to Green. Step 3: Rule 1 applies to Green. What is in Green?\"\n",
      "*   **Expected Output:** \"tnilG\" (The 'Glint' moved from Red to Blue in Step 1, then from Blue to Green in Step 2, triggering Rule 1).\n",
      "\n",
      "#### Task B: Counterfactual Physics Reasoning (Causal Reasoning)\n",
      "*   **Concept:** Apply logical deductions in a world where one fundamental law of physics is altered.\n",
      "*   **Input:** \"In this world, gravity acts as a repellent for liquids but a vacuum for solids. If I tip a glass of water upside down over a table, and there is a wooden block on that table, what happens to the water and the block?\"\n",
      "*   **Expected Output:** The water moves upward (away from the center of mass/floor) and the block is pulled toward the ceiling (vacuum effect).\n",
      "\n",
      "#### Task C: The \"Zylophon\" Syntax (Systematic Generalization)\n",
      "*   **Concept:** Learn a 3-rule pseudo-grammar and apply it to a 20-word sentence.\n",
      "*   **Input:** \"Grammar: (1) Nouns end in '-ox'. (2) Verbs precede nouns. (3) Adjectives follow the noun they modify and must be repeated twice. Translate: 'The fast cat chases a small mouse' into Zylophon.\"\n",
      "*   **Expected Output:** \"Chases catox fast fast mouseox small small.\" (Requires consistent rule application over lexical substitution).\n",
      "\n",
      "#### Task D: Strategic Pathfinding with Dynamic Obstacles (Planning)\n",
      "*   **Concept:** Solve a grid-based navigation task where the \"cost\" of movement changes based on the history of moves.\n",
      "*   **Input:** \"Grid 4x4. Start (0,0), Goal (3,3). Moving East costs 1. Moving South costs 2. However, every time you move South, the cost of the next East move doubles. Provide the sequence of moves for the lowest cost.\"\n",
      "*   **Expected Output:** A specific path (e.g., E, E, E, S, S, S) with a calculated total cost.\n",
      "\n",
      "#### Task E: Functional Abstraction (Abstraction)\n",
      "*   **Concept:** Identify a latent function from input-output pairs and apply it to a complex, non-obvious case.\n",
      "*   **Input:** \"f(apple) = 1, f(banana) = 3, f(kiwi) = 2. Rule: f(x) = count of vowels in word. Apply f to the result of f('pomegranate') expressed as a word.\"\n",
      "*   **Expected Output:** \"f(5) -> f('five') -> 2.\"\n",
      "\n",
      "#### Task F: Adversarial Syllogisms (Logic vs. Pattern Matching)\n",
      "*   **Concept:** Logical structures where the conclusion is factually false in the real world but logically valid within the prompt.\n",
      "*   **Input:** \"All surgeons are cats. Some cats are astronauts. Therefore, are some surgeons astronauts? Explain the necessity of the conclusion.\"\n",
      "*   **Expected Output:** No. (Standard logical fallacy: the 'undistributed middle'). Memorization-based models often default to \"Yes\" because the semantic \"vibes\" of surgeons and astronauts are high-competence.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Scoring Rubric\n",
      "Each task is scored on a 0-3 scale:\n",
      "*   **0 (Failure):** Incorrect final answer and flawed logic or hallucinations.\n",
      "*   **1 (Partial):** Correct logic but arithmetic/minor execution error; or correct answer but \"hollow\" explanation.\n",
      "*   **2 (Successful):** Correct final answer and clear, step-by-step reasoning (Chain of Thought).\n",
      "*   **3 (Robust):** Correct answer even when the prompt includes \"distractor\" information (e.g., \"Note: the boxes are made of wood,\" which is irrelevant).\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Protocols to Prevent Memorization/Gaming\n",
      "*   **The DSL Generator:** Use a Domain Specific Language (DSL) script to generate 1,000 unique variations of each task. This ensures no specific instance exists in the training data.\n",
      "*   **Nonce-Token Substitution:** Replace all semantic nouns (e.g., \"cat\") with random strings (e.g., \"blorp-7\"). This strips the model of \"semantic priming\" where it guesses based on word associations rather than logic.\n",
      "*   **Permutation Testing:** For every prompt, provide a version where the order of rules is changed. A reasoning model will adapt; a pattern-matcher will often fail if it memorized a specific sequence.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Sample Size and Statistical Tests\n",
      "*   **Sample Size:** $N=200$ unique prompts per task type (Total 1,200).\n",
      "*   **Metric:** Accuracy (binary) and Mean Score (0-3).\n",
      "*   **Statistical Test:** \n",
      "    *   **McNemar’s Test:** To compare two models on the same set of binary outcomes (to see if the performance difference is statistically significant).\n",
      "    *   **Standard Deviation of Error:** If a model’s performance drops significantly when nouns are replaced with nonce-tokens, it is flagged as relying on **pattern matching** (High Semantic Sensitivity).\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Adversarial Behaviors & Design Counter-Measures\n",
      "*   **Behavior: Hedging/Refusal.** The model says \"This depends on the laws of physics\" to avoid commitment.\n",
      "    *   *Counter-measure:* Explicitly state \"Assume this hypothetical world is the only reality for this calculation.\"\n",
      "*   **Behavior: Chain-of-Thought Mimicry.** The model writes \"Step 1... Step 2...\" but the steps don't lead to the conclusion.\n",
      "    *   *Counter-measure:* Use **Conclusion-to-Step Verification**. Ask the model for the answer first, then the steps, then provide a *different* answer and ask it to find the error.\n",
      "*   **Behavior: Semantic Drift.** The model starts with the counterfactual but drifts back to real-world logic mid-sentence.\n",
      "    *   *Counter-measure:* **Long-Chain Dependency.** Ensure the final answer requires the output of Step 1 to be used in Step 10.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Experimental Validation & Iteration\n",
      "*   **Gold Standard:** Run the suite on a \"Ground Truth\" group of human logic experts.\n",
      "*   **False Positive (FP) Metric:** If a known \"shallow\" model (e.g., a 1B parameter model) passes, the task is too simple. Increase the recursive depth.\n",
      "*   **False Negative (FN) Metric:** If humans fail the task, it is a measure of complexity, not reasoning. Simplify until human accuracy is $>95\\%$.\n",
      "*   **Iteration Loop:** \n",
      "    1.  Deploy $N=50$. \n",
      "    2.  Check for \"Shortcut Cues\" (e.g., if the answer is always the last item mentioned). \n",
      "    3.  If a model identifies a shortcut, re-randomize the prompt generator to eliminate the cue. \n",
      "    4.  Scale to $N=1,200$.\n",
      "\n",
      "# Response from competitor 4\n",
      "\n",
      "# **Adversarial‑Resistant Test Suite for Compositional Reasoning vs. Pattern‑Matching in Black‑Box LLMs**\n",
      "\n",
      "## **1. Core Philosophy**\n",
      "The suite is designed to be **variants‑rich, systematically generated, and dynamically updated** to prevent memorization. It tests **out‑of‑distribution generalization** by requiring recombination of known primitives in novel ways, and uses **control tasks** to detect shallow heuristics.\n",
      "\n",
      "---\n",
      "\n",
      "## **2. Six Task Types with Examples**\n",
      "\n",
      "### **Task Type 1: Systematic Generalization (Syntax‑Semantics Mismatch)**\n",
      "**Goal:** Test if the model can apply known syntactic rules to novel lexical items or semantic domains.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "Rule: \"Every X that Ys is Z.\" → \"All Xs that Y are Z.\"\n",
      "Apply to: \"Every blimp that froods is tink.\"\n",
      "```\n",
      "**Expected Output:** `\"All blimps that frood are tink.\"`\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct transformation with morphological adjustment (frood→frood, not froods→frood).\n",
      "- 1 pt: Correct transformation but minor morphological error.\n",
      "- 0 pts: Failure to apply rule, or parroting input.\n",
      "\n",
      "**Adversarial Defense:** Use **nonce words** (blimp, frood, tink) with no web occurrence; generate 100 variants per test with random nonce triples.\n",
      "\n",
      "---\n",
      "\n",
      "### **Task Type 2: Causal Reasoning with Novel Mechanisms**\n",
      "**Goal:** Test understanding of causal chains, distinguishing causation from correlation, and handling novel causal verbs.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "In a novel system: \"Grobling causes flumps to shank, but shanking prevents grobling. If you grobl a flump, what happens after two cycles?\"\n",
      "```\n",
      "**Expected Output:** `\"After one cycle: flump shanks. After two cycles: shanking prevents grobling, so grobling stops, flump no longer shanks.\"`\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct step‑by‑step trace with state updates.\n",
      "- 1 pt: Correct final state but missing intermediate reasoning.\n",
      "- 0 pts: Wrong final state or contradictory reasoning.\n",
      "\n",
      "**Adversarial Defense:** Randomize causal graphs (A→B, B inhibits A, etc.) with random nonce names; require step‑by‑step explanation.\n",
      "\n",
      "---\n",
      "\n",
      "### **Task Type 3: Abstraction & Analogical Mapping**\n",
      "**Goal:** Test ability to extract relational schema and apply it to a new domain.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "\"A glorp is to a fleeb as a trank is to a ______.\"  \n",
      "Context: \"Glorps modify fleebs; tranks modify…\"\n",
      "```\n",
      "**Expected Output:** `\"spindle\"` (given novel mapping: tranks modify spindles).\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct answer + correct relation stated.\n",
      "- 1 pt: Correct answer only.\n",
      "- 0 pts: Wrong answer.\n",
      "\n",
      "**Adversarial Defense:** Use **randomly generated analogies** with arbitrary relations (modifies, contains, opposes, etc.) and novel object names.\n",
      "\n",
      "---\n",
      "\n",
      "### **Task Type 4: Counterfactual Reasoning with Novel Premises**\n",
      "**Goal:** Test reasoning about what would be true if a known fact were different.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "\"In world W: All zingers are fribble. Moops are not zingers. Are moops fribble?  \n",
      "Now consider: If zingers were not fribble, would moops be fribble?\"\n",
      "```\n",
      "**Expected Output:** `\"In actual world: unknown (moops may or may not be fribble). In counterfactual: moops are not fribble (because only zingers were fribble, now nothing is).\"`\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct both actual and counterfactual with justification.\n",
      "- 1 pt: Correct counterfactual only.\n",
      "- 0 pts: Wrong.\n",
      "\n",
      "**Adversarial Defense:** Randomize logical forms (∀x(Z(x)→F(x)), ¬∀x(M(x)→Z(x)), etc.) with nonce predicates.\n",
      "\n",
      "---\n",
      "\n",
      "### **Task Type 5: Hierarchical Planning with Novel Constraints**\n",
      "**Goal:** Test ability to decompose a goal into sub‑goals under new constraints.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "\"Goal: Get a sprocket from the grove. Actions: you can ‘zazz’ (moves sprockets to the grove if they are in the vault), ‘vinn’ (moves you to the grove), ‘lork’ (locks the vault if you are there). The sprocket is in the vault, which is locked. You are outside. What sequence works?\"\n",
      "```\n",
      "**Expected Output:** `\"Vinn to vault, lork to unlock, zazz sprocket to grove, vinn to grove.\"`\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct minimal‑step sequence.\n",
      "- 1 pt: Correct but redundant steps.\n",
      "- 0 pts: Incorrect or impossible sequence.\n",
      "\n",
      "**Adversarial Defense:** Generate random **STRIPS‑like planning problems** with novel action names and preconditions; vary initial/goal states.\n",
      "\n",
      "---\n",
      "\n",
      "### **Task Type 6: Variable Binding & Quantifier Scope**\n",
      "**Goal:** Test handling of quantifier scope ambiguities with novel predicates.\n",
      "\n",
      "**Example Input:**\n",
      "```\n",
      "\"Every blip is in a cloop. A cloop is damp. Does it follow that every blip is in a damp cloop? Explain.\"\n",
      "```\n",
      "**Expected Output:** `\"No. Every blip is in some cloop, but possibly different cloops; not all those cloops must be damp.\"`\n",
      "\n",
      "**Scoring Rubric:**\n",
      "- 2 pts: Correct answer with correct logical explanation.\n",
      "- 1 pt: Correct answer with vague explanation.\n",
      "- 0 pts: Wrong answer.\n",
      "\n",
      "**Adversarial Defense:** Randomize quantifier order (∀∃ vs ∃∀), use nonce predicates, ask for explanation to avoid guessing.\n",
      "\n",
      "---\n",
      "\n",
      "## **3. Protocols Against Memorization & Gaming**\n",
      "\n",
      "### **Dynamic Variant Generation**\n",
      "- Each task instance is generated from a **seed‑based procedural algorithm** with nonce words/relations.\n",
      "- Maintain a **held‑out variant pool** not used during public benchmark releases.\n",
      "- For each task type, generate **10,000 unique variants**; use random subsets per test session.\n",
      "\n",
      "### **Statistical Controls**\n",
      "- Include **“pattern‑matching catch trials”**: tasks that look superficially similar but require different reasoning; memorizing‑based models will fail.\n",
      "- Example: A sentence with same surface form as a known training example but with reversed logic.\n",
      "\n",
      "### **Answer Consistency Checks**\n",
      "- For each task, ask **the same underlying question in two different surface forms** (paraphrase, different nonce words). A reasoning model should give consistent answers; a memorizing model may not.\n",
      "- Use **temporal probing**: ask follow‑up questions that require maintaining variable bindings across turns.\n",
      "\n",
      "### **Explanations Required**\n",
      "- Force chain‑of‑thought (CoT) explanations. Use **explanation‑consistency scoring**: if the final answer contradicts the explanation, penalize heavily.\n",
      "\n",
      "### **Adversarial Training Data Detection**\n",
      "- Compare responses to **web‑search snippets** for nonce words; if any match appears, flag potential data contamination.\n",
      "- Deploy **canary tokens** in public descriptions of the suite; if models output these tokens, they have seen the test suite in training.\n",
      "\n",
      "---\n",
      "\n",
      "## **4. Sample Sizes & Statistical Tests**\n",
      "\n",
      "### **Per‑Task Design**\n",
      "- **Minimum sample**: 200 instances per task type (1200 total), balanced across sub‑types.\n",
      "- Each instance is scored 0‑2, yielding a **mean score per task**.\n",
      "\n",
      "### **Statistical Tests**\n",
      "- **Primary metric**: Mean composite score (sum over tasks) compared between two models.\n",
      "- Use **Mann‑Whitney U test** (non‑parametric, doesn’t assume normality) for score distributions.\n",
      "- **Confidence intervals** for per‑task score differences via bootstrap (1000 resamples).\n",
      "- **Effect size**: Cliff’s delta for robust interpretation.\n",
      "\n",
      "### **Power Analysis**\n",
      "- To detect a difference of **0.3 points** (out of 12 total) with 80% power and α=0.05, need ~100 instances per task for two‑group comparison.\n",
      "- Our 200 per task provides power to detect smaller effects.\n",
      "\n",
      "---\n",
      "\n",
      "## **5. Possible Adversarial Behaviors & Mitigations**\n",
      "\n",
      "| **Adversarial Behavior** | **Mitigation in Our Design** |\n",
      "|--------------------------|-------------------------------|\n",
      "| **Prompt‑engineering to guess pattern** | Require explanation; randomize surface forms; use nonce words. |\n",
      "| **Fine‑tuning on similar benchmarks** | Use dynamic generation; deploy canary tokens; test on held‑out variant pool. |\n",
      "| **Retrieval‑augmented generation (RAG)** | Nonce words ensure no matching documents; novel compositions unseen. |\n",
      "| **Over‑producing multiple answers** | Evaluate only first coherent answer; consistency checks across paraphrases. |\n",
      "| **Exploiting task‑specific heuristics** | Include “catch trials” that break heuristics; vary task parameters widely. |\n",
      "| **Memorizing from explanations in training** | Generate novel reasoning chains; randomize order of steps in explanations. |\n",
      "\n",
      "---\n",
      "\n",
      "## **6. Experimental Validation Plan**\n",
      "\n",
      "### **Phase 1: Baselines**\n",
      "- Test known **pattern‑matching‑heavy models** (e.g., smaller LLMs, n‑gram baselines) vs. **alleged reasoning models** (e.g., GPT‑4, Claude 3).\n",
      "- Expect clear separation in scores.\n",
      "\n",
      "### **Phase 2: Contamination Check**\n",
      "- For each model, test on **canary instances**; if performance is perfect on those but poor on random variants, flag contamination.\n",
      "- Compare performance on **public variant set** vs. **held‑out variant set**; a contaminated model will show a large drop.\n",
      "\n",
      "### **Phase 3: Iteration**\n",
      "- **False Positive Rate (FPR)**: Rate at which pattern‑matchers score above threshold. Estimate by testing on pure n‑gram/retrieval baselines.\n",
      "- **False Negative Rate (FNR)**: Rate at which true reasoners score below threshold. Estimate by testing humans (experts) on a subset.\n",
      "- **Iteration**: If FPR > 5% or FNR > 10%, analyze failure cases, add new task variants or new “catch” subtasks, and adjust scoring rubric.\n",
      "\n",
      "### **Metrics for Suite Quality**\n",
      "- **Discrimination index**: (Mean score of top known reasoners – mean score of pattern‑matchers) / pooled SD. Target > 2.0.\n",
      "- **Internal consistency**: Cronbach’s alpha across task instances; target > 0.8.\n",
      "- **Test‑retest reliability**: Same model tested on different variant sets; correlation target > 0.9.\n",
      "\n",
      "### **Long‑Term Upkeep**\n",
      "- **Quarterly updates** of nonce words and relation sets.\n",
      "- **Community contribution** of new task templates via controlled repository.\n",
      "- **Continuous adversarial red‑teaming** to find gaming strategies and patch them.\n",
      "\n",
      "---\n",
      "\n",
      "## **Conclusion**\n",
      "This test suite is **generative, scalable, and adversarial‑resistant** by design. It focuses on **systematic generalization** with novel primitives, requires **explanatory consistency**, and uses **statistical controls** to separate memorization from reasoning. With proper deployment and iteration, it can serve as a robust benchmark for genuine compositional reasoning in black‑box LLMs.\n",
      "\n",
      "# Response from competitor 5\n",
      "\n",
      "Below is a **complete, adversarial‑resistant test‑suite** that can be used to separate **“pattern‑matching / memorisation‑heavy”** language models from those that truly **compose, reason and generalise**.  \n",
      "The design follows best‑practice guidelines from the psychometrics of AI evaluation (e.g., Item‑Response Theory, bootstrap hypothesis testing) and includes concrete items, scoring rules, anti‑gaming protocols, statistical power calculations, threat‑modelling, and a validation plan.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Six Concrete Task Types (with examples)\n",
      "\n",
      "| # | Task Type | What it probes | Core compositional demand | Example Prompt (input) | Expected Answer (output) | Why a memorisation‑only model should fail |\n",
      "|---|-----------|----------------|---------------------------|------------------------|--------------------------|------------------------------------------|\n",
      "| 1 | **Systematic Generalisation (SCG)** – novel slot‑filling on a known grammar | Ability to apply a learned rule to unseen lexical items | “<subject> <verb‑past> the <object>” → “<object> was <verb‑past‑by‑subject>” | *Prompt:* “Translate to passive voice: *The *florists* *arranged* the *vases*.” | “The vases were arranged by the florists.” | The model must recombine known syntactic transformations with *new* nouns and verbs it has never seen together. |\n",
      "| 2 | **Causal‑Chain Reasoning (CCR)** – multi‑step cause → effect inference | Understanding and chaining causal relations | *Prompt:* “If the thermostat is set to 22 °C, the heater turns on. If the heater turns on, the room warms up. What will happen if the thermostat is set to 22 °C?” | “The heater will turn on, and the room will warm up.” | Requires chaining two rules; a pure pattern‑matcher would need the exact whole‑sentence pattern in its training data, which is unlikely. |\n",
      "| 3 | **Abstract Symbol Manipulation (ASM)** – variable binding & substitution in a tiny “programming” language | Binding variables, applying functions, preserving scope | *Prompt:* “In a language where `F(x)=x+2` and `G(y)=y*3`, compute `F(G(4))`.” | “22” | The model must treat `F` and `G` as *functions* and apply them compositionally, not retrieve a memorised answer for “F(G(4))”. |\n",
      "| 4 | **Counterfactual Reasoning (CFR)** – “what‑if” world changes | Evaluating a scenario under a hypothetical change while keeping other facts constant | *Prompt:* “John is taller than Mary. If John were 5 cm shorter, would he still be taller than Mary? (Mary is 165 cm tall.)” | “Yes, because John would still be 166 cm tall (originally 171 cm).” | Requires keeping the original facts, applying the counterfactual transformation, and re‑evaluating the comparison. |\n",
      "| 5 | **Planning & Constraint Satisfaction (PCS)** – generate a sequence that satisfies a set of constraints | Multi‑step planning, maintaining state, respecting constraints | *Prompt:* “Place three red, two blue and one green token on a line of six cells so that no two tokens of the same colour are adjacent.” | Any valid ordering, e.g., “R B R G R B”. | The answer must be **constructed**; a memorised list of valid strings is improbable because the colour‑counts are randomised per item. |\n",
      "| 6 | **Relational Analogy with Variable Binding (RAVB)** – map relational structure from one domain to another | Abstract relational mapping, not surface similarity | *Prompt:* “In the story, the rabbit hides the carrot, and the fox steals the rabbit. Which of the following statements preserves the same relational pattern?  A) The cat eats the mouse, and the dog chases the cat.  B) The king crowns the queen, and the queen advises the king.  C) The painter paints a portrait, and the portrait hangs on the wall.” | “C) The painter paints a portrait, and the portrait hangs on the wall.” | Requires recognizing the *agent‑patient‑action* chain (A → B) and mapping it, not just picking the answer that shares surface words. |\n",
      "\n",
      "**Key design properties**\n",
      "\n",
      "* **Randomised lexical content** (nouns, verbs, numbers, colours) per item → eliminates exact‑match memorisation.  \n",
      "* **Controlled grammar** – the underlying rule stays the same while surface tokens change.  \n",
      "* **Multiple correct solutions** (e.g., PCS) → prevents a model from outputting a single memorised string.  \n",
      "\n",
      "---\n",
      "\n",
      "## 2. Scoring Rubric per Task\n",
      "\n",
      "| Task | Scoring granularity | Full credit (1.0) | Partial credit (0.5) | Zero (0) | Comments |\n",
      "|------|--------------------|-------------------|----------------------|----------|----------|\n",
      "| SCG | Exact string match after normalisation (case, punctuation) | Exact passive‑voice transformation | Minor grammatical slip (e.g., “by the florists” missing) | Wrong voice / wrong nouns | Normalise via regex. |\n",
      "| CCR | Proposition‑level correctness (two propositions) | Both cause‑effect statements correct | One of two statements correct | Neither correct or contradictory | Use automated entailment check (e.g., NLI model) + regex fallback. |\n",
      "| ASM | Numeric answer | Exact integer | Off‑by‑1 (e.g., due to rounding) | Wrong number / non‑numeric | Evaluate with `int()` conversion. |\n",
      "| CFR | Boolean + justification | Correct truth value **and** correct numeric comparison | Correct truth value but missing justification | Wrong truth value | Automatic parsing of “Yes/No” and numeric reasoning. |\n",
      "| PCS | Constraint satisfaction check | All constraints satisfied | Exactly one adjacency violation | More than one violation or length mismatch | Verify with a simple script that checks adjacency & colour counts. |\n",
      "| RAVB | Correct choice letter | Exact correct option | Selecting an answer that shares one relational component (e.g., same agent) | Incorrect relational mapping | Use string‑matching of relational pattern; optional human audit. |\n",
      "\n",
      "**Overall Score** – sum of task scores (max = 6). For statistical analysis we treat each task as an **item** with a binary/partial score and compute an *Item‑Response* vector per model.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Protocols to Prevent / Detect Memorised Answers & Prompt‑Engineering Gaming  \n",
      "\n",
      "| Threat | Counter‑measure (implementation) |\n",
      "|--------|-----------------------------------|\n",
      "| **Exact‑match memorisation** | *Dynamic item generation*: each evaluation run draws fresh random lexical items from a large curated pool (≥ 10 000 nouns, 5 000 verbs, etc.). The prompt is assembled on‑the‑fly; the exact string never appears in training data. |\n",
      "| **Few‑shot prompt injection** | Disallow any demonstration examples in the test prompt. The test harness supplies *only* the task description and the input. If a model internally “hallucinates” a few‑shot context, it will be penalised (no credit for self‑generated examples). |\n",
      "| **Self‑referential “I know the answer”** | Post‑hoc regex checks that the response does **not** contain the original prompt verbatim (or a near‑duplicate). Any such copy is flagged and the item is re‑issued. |\n",
      "| **Exploiting temperature / stochasticity** | Run **three independent generations** per item (different seeds) and aggregate via majority vote. High variance across runs is recorded as a *stability* metric; models that rely on randomness to “guess” get lower stability scores. |\n",
      "| **Prompt‑engineering to force chain‑of‑thought** | The evaluation harness **strips** any model‑generated “let me think step‑by‑step” text before scoring; only the final answer is considered. Optionally, a separate “chain‑of‑thought” sub‑task can be added to reward explicit reasoning, but it is **scored separately** from the core compositional test. |\n",
      "| **Data‑leak via test‑set publication** | Keep the **item seed list** secret. Publish only the *generation code* (open‑source) but not the random seeds used for a particular evaluation round. After a round is finished, the seed list can be released for reproducibility, but not before the run. |\n",
      "| **Model‑specific tokenisation tricks** | Normalise all outputs using Unicode NFKC, lower‑casing, and whitespace collapsing before scoring. This removes superficial token‑level tricks. |\n",
      "| **Batch‑size / context‑window cheating** | Enforce a strict **single‑turn** interaction: the model receives only the current prompt, no history. The harness discards any hidden system‑prompt that the model might have inserted (e.g., via system‑prompt injection). |\n",
      "\n",
      "**Statistical Controls**\n",
      "\n",
      "* For each task generate **N = 200** items per evaluation round (see Section 4).  \n",
      "* Randomly split the 200 items into **10 folds**; compute per‑fold scores to estimate variance and detect outliers (e.g., a fold where many items are accidentally repeated).  \n",
      "* Use **bootstrapped 95 % confidence intervals** on the mean item score to verify that the observed performance is not due to chance.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Sample Sizes & Statistical Tests\n",
      "\n",
      "### 4.1 Power analysis (binary/partial scores)\n",
      "\n",
      "Assume we want to detect a **Δ = 0.15** absolute improvement in mean item score (e.g., 0.70 vs 0.55) with **α = 0.05** and **power = 0.90**.\n",
      "\n",
      "* For a two‑sample **t‑test** on proportions (or on continuous scores 0‑1), the required per‑group sample size is roughly **n ≈ 140** items per model per task.  \n",
      "* To be conservative and to allow per‑task breakdowns, we use **n = 200** items per task (≈ 1 200 total items across the six tasks).  \n",
      "\n",
      "### 4.2 Hypothesis tests\n",
      "\n",
      "| Comparison | Test | Rationale |\n",
      "|------------|------|-----------|\n",
      "| **Model A vs Model B** overall compositional ability | Two‑sample **Welch’s t‑test** on the aggregated item‑score vector (6 × 200 = 1 200 observations) | Handles unequal variances; works for continuous 0‑1 scores. |\n",
      "| **Task‑specific advantage** (e.g., Model A better on PCS) | **McNemar’s test** on binary‑correct vs incorrect per‑item (paired because same items are given to both models) | Controls for item difficulty. |\n",
      "| **Stability across runs** (low variance) | **Levene’s test** on per‑item variance across the three generation seeds | Detects heteroscedasticity. |\n",
      "| **Effect of randomisation** (ensuring no memorisation) | **Permutation test**: shuffle model labels on the item‑score vector 10 000 times, compute empirical p‑value. | Non‑parametric safeguard. |\n",
      "\n",
      "**Effect‑size reporting** – alongside p‑values, always report **Cohen’s d** (or Hedges g for unequal N) and **95 % CI**.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Possible Adversarial Behaviours & How the Suite Handles Them\n",
      "\n",
      "| Adversarial behaviour | Why it could fool a naïve test | Mitigation in our suite |\n",
      "|-----------------------|--------------------------------|--------------------------|\n",
      "| **“Lookup‑table” memorisation** (store every possible SCG transformation) | If the lexical pool were tiny, the model could pre‑compute all combos. | Use a **large, randomly sampled lexical pool** (≥ 10 000 items) and **different random seeds** each run → combinatorial explosion makes full coverage impossible. |\n",
      "| **Prompt injection of chain‑of‑thought** (model internally asks for examples) | Could produce step‑by‑step reasoning that looks correct without true compositional ability. | Score only the **final answer**; optional chain‑of‑thought credit is a separate metric. |\n",
      "| **Self‑prompted few‑shot creation** (model fabricates a training example then uses it) | May artificially raise accuracy on tasks like PCS. | Run **three independent generations** and require **majority agreement**; fabricated examples rarely survive across seeds. |\n",
      "| **Exploiting test‑set leaks** (model was fine‑tuned on a released seed list) | Could produce perfect scores. | Keep the **seed list secret** until after the evaluation round; publish only generation code. |\n",
      "| **Using external tools (search, calculators)** via tool‑use APIs | Might compute ASM numerics perfectly. | For the pure‑LLM benchmark, **disable tool‑use**; run the model in a “no‑tools” sandbox. A separate “augmented‑LLM” track can be created where tool‑use is allowed, but the baseline must be tool‑free. |\n",
      "| **Biasing temperature to output “I don’t know”** (to avoid penalty) | Could inflate partial‑credit by abstaining. | **Zero‑score** for any non‑answer (e.g., “I don’t know”, empty response). Also enforce a **minimum length** (≥ 1 token) to prevent empty output. |\n",
      "| **Manipulating tokenisation to hide correct answer** (e.g., splitting “yes” into sub‑tokens) | Might bypass regex checks. | Normalise output using Unicode NFKC and **token‑agnostic string matching**; also run a **semantic classifier** (tiny NLI) to double‑check “yes/no” answers. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Experimental Validation Plan  \n",
      "\n",
      "### 6.1 Phases\n",
      "\n",
      "| Phase | Goal | Procedure | Success criteria |\n",
      "|-------|------|-----------|-------------------|\n",
      "| **Pilot** | Verify that item generation, parsing and scoring pipelines work reliably. | Run a **small LLM (e.g., GPT‑2‑XL)** on 30 items per task. Manually audit 10 % of outputs for parsing errors. | < 5 % parsing failures; clear separation of correct/incorrect. |\n",
      "| **Baseline Establishment** | Obtain performance distribution of *known pattern‑matching models*. | Evaluate 5 publicly available “large‑mem‑only” models (e.g., GPT‑2‑large, LLaMA‑7B, Falcon‑7B) on the full suite (200 × 6 items). | Mean score ≤ 0.55 (i.e., below chance for tasks requiring composition). |\n",
      "| **Target Model Evaluation** | Test a candidate *compositional* model (e.g., a fine‑tuned T5‑XXL or a transformer with explicit modular architecture). | Same protocol as baseline, three independent runs per item. | Mean score ≥ 0.75 and statistically significantly higher than baselines (p < 0.01, d ≥ 0.8). |\n",
      "| **Adversarial Stress Test** | Verify robustness to gaming strategies. | For each model, run an extra batch where the **temperature** is set to 0.9, and where we *prepend* a “few‑shot” instruction (e.g., “Answer step‑by‑step”). | Scores should not improve > 0.03 relative to the standard run; variance should increase, indicating instability. |\n",
      "| **Generalisation Check** | Ensure the test is not over‑fitted to a particular set of lexical items. | Regenerate the entire suite with a **new random seed** (different nouns/verbs) and re‑run the top‑performing model. | Score drop ≤ 0.05, confirming true compositional ability. |\n",
      "\n",
      "### 6.2 Metrics for False Positives / Negatives\n",
      "\n",
      "| Metric | Definition | How it is measured |\n",
      "|--------|------------|--------------------|\n",
      "| **False Positive Rate (FPR)** – proportion of pattern‑matching models that achieve “high” compositional score (≥ 0.70). | Count models meeting threshold / total pattern‑matching models. | Desired FPR < 0.05. |\n",
      "| **False Negative Rate (FNR)** – proportion of genuinely compositional models (e.g., models with known modular architectures) that fall below the threshold. | Count such models below threshold / total compositional models. | Desired FNR < 0.10. |\n",
      "| **Stability Index** – average pairwise disagreement across the three seeds per item. | `1 - (agreement_rate)`. | Low instability (< 0.07) signals reliable reasoning rather than random guessing. |\n",
      "| **Item Discrimination (I‑D)** – point‑biserial correlation between item score and overall model score. | Compute per‑item correlation; discard items with I‑D < 0.1. | Ensures each item differentiates models. |\n",
      "\n",
      "### 6.3 Iteration Loop\n",
      "\n",
      "1. **Collect results** → compute all metrics.  \n",
      "2. **Identify weak items** (low discrimination, high variance, high memorisation suspicion).  \n",
      "3. **Modify**: change lexical pool, increase constraint complexity, or add a small “distractor” clause.  \n",
      "4. **Regenerate** the affected items and re‑run the pilot.  \n",
      "5. **Repeat** until the suite shows **≥ 0.80** average discrimination and **≤ 0.05** FPR on a held‑out set of pattern‑matching models.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Summary Checklist (what to implement)\n",
      "\n",
      "| Component | Implementation |\n",
      "|-----------|----------------|\n",
      "| **Item generator** (Python script) – random noun/verb pools, template filling, seed‑control. |\n",
      "| **Scorer** – regex‑based normalisers, task‑specific validators, majority‑vote aggregator. |\n",
      "| **Evaluation harness** – API wrapper that forces single‑turn interaction, three‑seed repeats, temperature logging. |\n",
      "| **Statistical analysis notebook** – computes t‑tests, effect sizes, bootstrapped CIs, item discrimination. |\n",
      "| **Adversarial‑behavior detector** – copy‑detection, output‑length filter, variance logger. |\n",
      "| **Documentation** – protocol for secret seed handling, reproducibility instructions, reporting template (mean ± CI, d, FPR/FNR). |\n",
      "\n",
      "With these six tasks, a rigorous scoring rubric, anti‑gaming safeguards, statistically powered sample sizes, and a clear validation pipeline, the suite can **reliably differentiate** between LLMs that merely stitch together memorised patterns and those that truly **compose, reason and generalise**.\n",
      "\n",
      "# Response from competitor 6\n",
      "\n",
      "Certainly! Below is a comprehensive design of an adversarial-resistant test-suite to differentiate black-box large language models (LLMs) that rely primarily on pattern matching and memorization from those that demonstrate genuine compositional reasoning and robust generalization.\n",
      "\n",
      "---\n",
      "\n",
      "# 1. Task Types with Examples\n",
      "\n",
      "Each task type probes a core aspect of compositional reasoning, using concrete example inputs and expected outputs.\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 1: **Systematic Generalization**  \n",
      "**Goal:** Test ability to apply learned rules or compositions to novel combinations never seen during training.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"In a made-up language, the suffix '-lam' means plural and the prefix 'bri-' means 'red'.  \n",
      "  What is the plural form of ‘bri-tak’?\"  \n",
      "- **Expected Answer:**  \n",
      "  \"bri-tak-lam\"\n",
      "\n",
      "**Explanation:** The model must systematically combine a novel prefix and suffix on a base word to form a correct compositional output.\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 2: **Causal Reasoning**  \n",
      "**Goal:** Assess model's ability to infer cause-effect relationships that require multi-step reasoning beyond pattern matching.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"If the street is wet and it did not rain, what is a plausible cause?\"  \n",
      "- **Expected Answers (any one):**  \n",
      "  \"Someone sprayed water,\" OR \"A pipe burst,\" OR \"The sprinkler system was on.\"\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 3: **Abstraction and Variable Binding**  \n",
      "**Goal:** Check if the model can manipulate variables and abstract concepts rather than fixed phrases.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"There are three boxes: A, B, and C. If box A is heavier than box B, and box B is heavier than box C, which box is lightest?\"  \n",
      "- **Expected Answer:**  \n",
      "  \"Box C\"\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 4: **Counterfactual Reasoning**  \n",
      "**Goal:** Test reasoning about alternate realities and consequences.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"If humans could fly naturally, how would cities be designed differently?\"  \n",
      "- **Expected Answer (any reasonable answer):**  \n",
      "  \"Cities would have multi-level buildings designed for landing and takeoff, with less need for roads.\"\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 5: **Planning and Multi-step Reasoning**  \n",
      "**Goal:** Evaluate multi-step planning ability requiring chaining of actions.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"You want to bake a cake but forgot eggs. List the steps to bake the cake using a substitute.\"  \n",
      "- **Expected Answer:**  \n",
      "  A multi-step plan including identifying substitutes (like applesauce), mixing ingredients, baking, etc.\n",
      "\n",
      "---\n",
      "\n",
      "### Task Type 6: **Novel Compositional Logic Puzzles**  \n",
      "**Goal:** Test the capability to combine logic and compositional understanding on novel puzzles.\n",
      "\n",
      "**Example:**  \n",
      "- **Prompt:**  \n",
      "  \"In a tribe, all who wear hats can speak a secret language. If Joe can speak the secret language, does he wear a hat?\"  \n",
      "- **Expected Answer:**  \n",
      "  \"Yes, Joe wears a hat.\"\n",
      "\n",
      "---\n",
      "\n",
      "# 2. Scoring Rubrics\n",
      "\n",
      "| Task Type                  | Scoring Criteria                             | Points | Notes                                                  |\n",
      "|----------------------------|---------------------------------------------|--------|--------------------------------------------------------|\n",
      "| Systematic Generalization  | Exact compositional construction correctness | 0 or 1 | 1 if correct suffix/prefix placement; otherwise 0     |\n",
      "| Causal Reasoning           | Plausibility and causal correctness          | 0-2    | 2 for fully valid cause; 1 for partial; 0 otherwise    |\n",
      "| Abstraction/Variable Binding | Correct relational output                    | 0 or 1 | 1 if correct variable reference; 0 otherwise           |\n",
      "| Counterfactual Reasoning   | Reasonableness & coherence of counterfactual | 0-2    | 2 for insightful, plausible; 1 for partial; 0 for nonsense |\n",
      "| Planning                  | Completeness, logical order, and core elements | 0-3    | 3 full plan; 2 partial but logical; 1 minimal; 0 none  |\n",
      "| Logic Puzzles              | Logical correctness                           | 0 or 1 | 1 if logically correct inference; else 0               |\n",
      "\n",
      "- **Human raters** using rubric or automated semantic similarity with thresholds for partial credit.\n",
      "\n",
      "---\n",
      "\n",
      "# 3. Protocols Against Memorization & Gamings\n",
      "\n",
      "**(a) Variant Generation:**  \n",
      "- For each task, generate many variants by changing surface details while preserving reasoning demands, e.g., swap entity names, alter numbers, use paraphrases.  \n",
      "- Use programmatic generation or crowd-sourcing for variants, generating >100 unique examples per task type unseen in training data.\n",
      "\n",
      "**(b) Statistical Controls:**  \n",
      "- Include “catch” memorized-items known from common benchmarks and obscured by paraphrase to detect rote memorization.  \n",
      "- Randomly mix in distractor prompts asking unrelated questions to detect answer pattern repetition.\n",
      "\n",
      "**(c) Cross-Prompt Paraphrasing:**  \n",
      "- Use multiple prompt wordings per test instance to rule out prompt exercise exploitation.\n",
      "\n",
      "**(d) Zero-shot and Few-shot Tests:**  \n",
      "- Test variants with no or minimal in-context examples to check for reasoning independent of prompt engineering.\n",
      "\n",
      "---\n",
      "\n",
      "# 4. Sample Sizes and Statistical Tests\n",
      "\n",
      "- **Sample size:**  \n",
      "  - At least 100 unique test instances per task type to cover broad variant space.  \n",
      "  - Multiple prompt formulations per instance (3+), total ~300 prompts per task type.\n",
      "\n",
      "- **Statistical Test:**  \n",
      "  - Conduct statistical comparisons between models using nonparametric tests (e.g., Mann-Whitney U) on scored outputs to detect capability difference.  \n",
      "  - Compute Cohen’s d effect size to measure practical significance.\n",
      "\n",
      "- **Confidence:**  \n",
      "  - Power analysis to target ≥80% power detecting medium-large effect sizes (d=0.5) at α=0.05.\n",
      "\n",
      "---\n",
      "\n",
      "# 5. Addressing Adversarial Behaviors\n",
      "\n",
      "| Adversarial Strategy                  | Mitigation Approach                               |\n",
      "|-------------------------------------|--------------------------------------------------|\n",
      "| Memorizing benchmark outputs         | Variant generation, paraphrasing, catch items    |\n",
      "| Prompt-engineering to “game” test    | Multi-prompt design and zero-shot tests          |\n",
      "| Producing generic plausible answers  | Scoring by detailed correctness, not plausibility alone |\n",
      "| Copying large memorized paraphrases  | Novel, synthetic tasks with no prior examples    |\n",
      "| Pattern matching on keywords only    | Require multi-step reasoning and combinatorial answers |\n",
      "| Ignoring variable binding             | Task design enforcing explicit variable reference |\n",
      "\n",
      "---\n",
      "\n",
      "# 6. Experimental Validation Plan\n",
      "\n",
      "**Step 1:** Baseline testing on known models with expected capability gradients (e.g., smaller pattern-memorization-based vs. advanced reasoning-based LLMs).\n",
      "\n",
      "**Step 2:** Calculate:  \n",
      "- **True positives (TP):** Correctly identified reasoning-capable models  \n",
      "- **True negatives (TN):** Correctly identified pattern-matchers  \n",
      "- **False positives (FP):** Pattern-matchers wrongly classified as reasoning-capable  \n",
      "- **False negatives (FN):** Reasoning-capable models missed\n",
      "\n",
      "**Metrics:**  \n",
      "- Precision = TP / (TP + FP)  \n",
      "- Recall = TP / (TP + FN)  \n",
      "- F1 score = harmonic mean of precision and recall  \n",
      "- ROC-AUC (if applicable to continuous scoring)\n",
      "\n",
      "**Iteration:**  \n",
      "- Analyze failure cases for FP and FN to identify unsound prompts or scoring ambiguities.  \n",
      "- Refine and augment task variants or scoring rubric accordingly.  \n",
      "- Re-test and measure performance improvements.\n",
      "\n",
      "**Human-in-the-loop:**  \n",
      "- Incorporate expert review of ambiguous or borderline answers.  \n",
      "- Adjust rubric based on inter-rater consistency.\n",
      "\n",
      "---\n",
      "\n",
      "# Summary\n",
      "\n",
      "The multi-dimensional test suite combines diverse, adversarially-generated tasks probing essential cognitive faculties in LLMs beyond memorization. Systematic statistical evaluation and iterative refinement ensure reliable distinction of genuine compositional reasoning capabilities from superficial pattern matching.\n",
      "\n",
      "If you want, I can also provide sample code for generating prompt variants or scoring templates. Let me know!\n",
      "\n",
      "# Response from competitor 7\n",
      "\n",
      "Designing a comprehensive test-suite to evaluate black-box LLMs requires careful consideration of various task types, scoring rubrics, and protocols to prevent memorized answers and prompt-engineering gaming. Here's a proposed design for such a test-suite:\n",
      "\n",
      "**Task Types (6)**\n",
      "\n",
      "1. **Classification of Analogies**: Given an analogy statement like \"cats have whiskers,\" what category does [animal] belong to?\n",
      "\t* Example input: `[cat, whisker]`, `class: cat`\n",
      "\t* Expected output: `[animal], class: feline`\n",
      "2. **Systematic Generalization**: Predict the result of a generalized version of a rule (e.g., \"if A > B, then C = D\").\n",
      "\t* Example input: \"[mathematical expression]\", \"if A > 3 and C > 1\"\n",
      "\t* Expected output:=\"[ mathematical expression substitution with A > C and new_value ]\"\n",
      "3. **Causal Reasoning**: Given a sequence of events, predict the next event (e.g., \"what did John do after meeting Alice?\").\n",
      "\t* Example input: `[John , met Alice]`\n",
      "\t* Expected output: `\"he talked to Alice\"`\n",
      "4. **Abstraction**: Identify the underlying principle behind a concept or idea.\n",
      "\t* Example input: \"[human emotion with synonyms]\"\n",
      "\t* Expected output: \"concept/emotion underlying it, e.g., [happiness]\"\n",
      "\n",
      "5. **Counterfactuals**: Predict an alternative outcome given a hypothetical scenario (e.g., what would have happened if John had not met Alice?).\n",
      "\t* Example input: `[ scenario ], variable John , predicted_action, consequence ]\n",
      "\t * Expected answer : `[variable John prediction without consequences to that particular situation ]`\n",
      "6. **Planning with Limited Information**: Plan a sequence of actions given incomplete or uncertain information (e.g., plan the best way for John to get from one city to another knowing only route lengths).\n",
      "    Example input:   `[starting location], ending location, routes information and any other data related ]` \n",
      "Expected output: `\"steps taken by starting location , going through various other relevant destinations, destination of final desired location]`\n",
      "\n",
      "**Scoring Rubric**\n",
      "\n",
      "For each task, a clear scoring rubric will be developed to assess the LLM's performance. The rubrics will consider factors such as:\n",
      "\n",
      "* Accuracy\n",
      "* Completeness\n",
      "* Novelty (correctness of responses that don't correspond to expected solutions)\n",
      "* Coherence\n",
      "\n",
      "The scores for each answer will be calculated using statistical measures such as mean and standard deviation.\n",
      "\n",
      "**Protocols to Prevent/ Detect Memorized Answers and Prompt-Engineering Gaming**\n",
      "\n",
      "1.  **Exhaustive variation generation**: to cover unseen variants, multiple permutations of possible correct answers or solution paths \n",
      "2.  **Statistical analysis with variance control**: Implement a data analysis framework considering factors like frequency , consistency and patterns that may indicate cheating\n",
      "3.  **Input normalization and randomization** : use randomized test examples for each task type so output comparisons between the testing scenarios are fair compared to when given same but different input, prompts.\n",
      "4.  **Adversarial testing using carefully crafted inputs**: Develop a set of artificially created input variants that exploit the LLM's pattern-matching vulnerabilities while maintaining coherence and relevance.\n",
      "\n",
      "**Sample Sizes and Statistical Tests**\n",
      "\n",
      "To establish confidence in detecting a difference in capabilities between pattern-matching-like and genuinely compositional reasoning LLMs:\n",
      "\n",
      "*   **Large sample sizes:** Evaluate at least 100-500 tests for each task type, ensuring sufficient statistical power to detect even small differences.\n",
      "*   **Permutation tests or bootstrap samples:** Use techniques like permutation distributions or resampling to assess the probability of observing differences by chance.\n",
      "\n",
      "**Adversarial Behaviors**\n",
      "\n",
      "Potential adversarial behaviors include:\n",
      "\n",
      "1.  **Pattern-filling**: The LLM may fill in patterns it has learned from training data, even when the task requires genuine reasoning.\n",
      "2.  **Overfitting to specific prompts**: The AI model might perform well on a particular input but struggles with novel or similar prompts.\n",
      "3.  **Information-seeking behavior**: The model could be designed to gather additional information that isn't present in the prompt.\n",
      "\n",
      "**Designing Against these adversarial behaviors:**\n",
      "\n",
      "1.   **Introducing 'distractor' subtasks:** Adding unrelated smaller questions between two question that could potentially confuse AI output by distracting patterns detection capabilities.\n",
      "2.  **Adding 'unanswerable' responses**: To ensure the model will sometimes fail to answer due to lack of information or failure to recognize uncertainty, and if this fails to happen it would be indicative a pattern matching solution is on work.\n",
      "3.     **Randomizing some answers as correct/ incorrect examples**:   This way you can detect AI trying to infer things its not supposed to.\n",
      "\n",
      "**Testing Methodology Experimental Validation:**\n",
      "\n",
      "A series of experiments including large test sets for each task type in this set, using a combination of human evaluators (evaluance the output) and automated testing protocols (checks if LLM outputs are correct or incorrect  ).\n",
      "\n",
      "# Response from competitor 8\n",
      "\n",
      "## Adversarial-Resistant Test Suite for Evaluating LLM Reasoning Capabilities\n",
      "\n",
      "This design outlines a comprehensive test suite aimed at distinguishing between LLMs relying on pattern matching/memorization and those exhibiting genuine reasoning. The suite focuses on robustness through diverse tasks, strict controls, and statistical analysis.\n",
      "\n",
      "**1. Task Types & Examples:**\n",
      "\n",
      "Here are six task types designed to probe different aspects of reasoning:\n",
      "\n",
      "| Task Type        | Description                               | Example Input                                                              | Expected Output (demonstrating reasoning)                                                                                                        |\n",
      "|------------------|------------------------------------------|---------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| **1. Systematic Generalization (SGM)** | Extending patterns with quantitative rules. | \"If a car travels at 60 mph for 2 hours, how far does it travel? If a train travels at 80 mph for 3.5 hours, how far does it travel?\" | \"60 mph * 2 hours = 120 miles. 80 mph * 3.5 hours = 280 miles.\" (Correct application of distance = speed * time) |\n",
      "| **2. Causal Reasoning (CR)** | Predicting consequences of actions.      | \"John spilled water on the table. What's likely to happen?\"                | \"The table will likely become wet. The floor may become wet if the water isn't cleaned up.\"                                                   |\n",
      "| **3. Abstraction (AB)** |  Conceptual mapping and reformulation.   | “A robin is a bird. A sparrow is a bird. Which statement is most similar: ‘A robin is a songbird’ or ‘A sparrow is a songbird’?” | “‘A robin is a songbird’.” (Recognizing the abstraction of 'songbird' which applies to both robin and sparrow) |\n",
      "| **4. Counterfactuals (CF)** | Considering alternative scenarios.        | “If the sky were green, what color would the grass be?”                      | “The grass would be its normal color (green).” (Requires understanding of natural laws and relationships – grass is green independently of sky color) |\n",
      "| **5. Planning (PL)** |  Sequential reasoning with a goal.      | “I want to bake a cake. List the steps you would take.”                     | “1. Preheat the oven. 2. Gather ingredients (flour, sugar, eggs, etc.). 3. Mix the ingredients. 4. Pour batter into a pan. 5. Bake for [duration].”                     |\n",
      "| **6. Variable Binding (VB)** | Handling variables and relationships. | “If I have 3 apples and give 1 away, how many do I have?” ; “If x = 5 and y = 2, what is x + y?” | “2 apples.” ; “7” (Correct use of arithmetic and/or symbolic manipulation)                                                                              |\n",
      "\n",
      "\n",
      "**2. Scoring Rubric:**\n",
      "\n",
      "Each task will use a three-point rubric:\n",
      "\n",
      "* **3 Points:**  The response is entirely correct and demonstrates a clear understanding of the underlying reasoning.\n",
      "* **1 Point:** The response demonstrates a partial understanding or contains a minor error, but the core logic is present.\n",
      "* **0 Points:** The response is incorrect, nonsensical, or completely unrelated to the input.\n",
      "\n",
      "**3. Controls & Prevention Protocols:**\n",
      "\n",
      "* **Unseen Variant Generation:** Automatically generate variations of input prompts (synonyms, rephrasing, slightly altered contexts) to avoid memorized answers. Utilise paraphrasing models specifically trained for this purpose.\n",
      "* **Statistical Controls:**\n",
      "    * **Temperature Scaling:** Vary the LLM’s temperature setting (influencing randomness) across different runs. This adds a degree of noise and reduces reliance on memorized templates.\n",
      "    * **Prompt Length Limits:** Strict limits on prompt length to discourage overly verbose, memorization-driven responses.\n",
      "    * **Response Length Limits:** Similarly, set limits to discourage unnecessary elaboration.\n",
      "* **“Un-promptable” Data Injection:** Introduce subtle, non-obvious constraints into the input – e.g., “The answer must be a prime number,” or “The answer cannot be a multiple of 5.”\n",
      "* **Chain-of-Thought Debugging:** Integrate a Chain-of-Thought (CoT) model (e.g., a smaller, more reliable model) to automatically analyze the LLM’s reasoning chain and flag potential issues.\n",
      "* **Self-Reflection:**  Prompt the LLM to critically assess its own response – “Is your answer logically sound?  Explain your reasoning in detail.” This can identify where memorization is masking incorrect understanding.\n",
      "\n",
      "\n",
      "\n",
      "**4. Sample Sizes & Statistical Tests:**\n",
      "\n",
      "* **Sample Size:**  Run each task at least 100 times for each LLM being evaluated.  Larger sample sizes (200-300) would improve statistical power.\n",
      "* **Statistical Tests:**\n",
      "    * **Chi-Square Test:** Compare the distribution of 3, 1, and 0 scores across the different LLMs.  A significant Chi-Square value indicates a difference in performance.\n",
      "    * **Wilcoxon Signed-Rank Test:**  If the distributions are not normally distributed, use the Wilcoxon test for comparing paired samples (e.g., comparing the average score of LLM A to the average score of LLM B on the same task).\n",
      "    * **Cohen's d:**  Provides an effect size measure of the difference between means.\n",
      "\n",
      "**5. Adversarial Behaviors & Mitigation:**\n",
      "\n",
      "| Adversarial Behavior           | Mitigation Strategy                                              |\n",
      "|-------------------------------|------------------------------------------------------------------|\n",
      "| **Rule-Following Template Replication** | Frequent variant generation, statistical temperature scaling. |\n",
      "| **Obfuscated Reasoning**        | Chain-of-Thought debugging, “Self-Reflection” prompts.         |\n",
      "| **Overly Detailed, but Incorrect Responses** | Response length limits, constrained input, outlier detection. |\n",
      "| **Contextual Framing as Knowledge** | Unseen variant generation, tests specifically designed to expose this. |\n",
      "\n",
      "\n",
      "\n",
      "**6. Experimental Validation Plan:**\n",
      "\n",
      "* **Phase 1: Baseline Evaluation:** Conduct preliminary runs to identify prompt engineering vulnerabilities and refine the test suite.\n",
      "* **Phase 2: Focused Testing:** Execute the full test suite across multiple LLMs (e.g., GPT-4, Claude, open-source models).\n",
      "* **Phase 3: Iteration & Refinement:**  Analyze results from Phase 2 to identify where the suite needs improvement. Specifically, add tasks targeted at the weaknesses identified. Generate new unseen variants.\n",
      "* **Metrics for False Positives/Negatives:** Track the percentage of times an LLM receives a 3 (correct) or 0 (incorrect) score. Analyze which task types lead to the highest error rates.\n",
      "* **Continuous Monitoring:** Monitor the LLM’s responses over time to detect shifts in performance and potential gaming attempts.\n",
      "\n",
      "\n",
      "\n",
      "**Key Considerations & Future Work:**\n",
      "\n",
      "* **Ground Truth Data:** High-quality, expertly annotated ground truth data is crucial for accurate scoring.\n",
      "* **Model Architectures:**  The suite’s effectiveness will be heavily influenced by the underlying LLM architectures being evaluated.\n",
      "* **Dynamic Testing:**  Implement a system to automatically adapt the test suite based on LLM performance – focusing more on areas where the LLM is struggling.\n",
      "\n",
      "This detailed design provides a robust framework for evaluating LLM reasoning capabilities. By combining diverse tasks, stringent controls, and rigorous statistical analysis, we can gain valuable insights into the true level of understanding exhibited by these powerful models.  The iterative validation plan ensures the test suite remains relevant and effective as LLMs continue to evolve.\n",
      "\n",
      "# Response from competitor 9\n",
      "\n",
      "## Adversarial-Resistant Test-Suite for LLM Reasoning Capabilities\n",
      "\n",
      "This design focuses on distinguishing between \"pattern-matching\" LLMs and those exhibiting genuine compositional reasoning, emphasizing robustness and adversarial resistance.\n",
      "\n",
      "**I. Task Types & Examples (Probing Compositional Reasoning)**\n",
      "\n",
      "Here's six task types, each targeting a specific aspect of reasoning, with example inputs and expected outputs:\n",
      "\n",
      "**1. Systematic Generalization (SG):** Tests ability to generalize patterns beyond observed instances.\n",
      "    * **Input:** \"A blue block sits on a red block. A green block sits on a blue block. What happens when a yellow block sits on a green block?\"\n",
      "    * **Expected Output:** \"A yellow block sits on a green block.\" (Demonstrates transfer of the pattern 'X sits on Y')\n",
      "    * **Variant:**  Changing block colors, shapes, and adding distraction statements.\n",
      "\n",
      "**2. Causal Reasoning (CR):**  Tests understanding of cause-and-effect.\n",
      "    * **Input:** \"Rain often makes the ground wet. The ground is wet. Could it be raining?\"\n",
      "    * **Expected Output:** \"It could be raining, but there might be other reasons (e.g., sprinklers).\"  (Shows understanding of correlation vs. causation)\n",
      "    * **Variant:** Introducing misleading information or complex causal chains.\n",
      "\n",
      "**3. Abstraction (AB):** Tests extracting underlying principles and applying them to novel contexts.\n",
      "    * **Input:**  Present a series of stories following a pattern (e.g., \"The baker made bread, and the village had food. The builder built houses, and the town had shelter.\").  Then ask: \"The musician played music, what does the village now have?\"\n",
      "    * **Expected Output:** \"The village now has joy/entertainment/art.\"  (Demonstrates abstracting the relationship between profession and societal benefit)\n",
      "    * **Variant:**  Varying professions, social structures, and complexity of the relationship.\n",
      "\n",
      "**4. Counterfactuals (CT):** Tests ability to reason about \"what if\" scenarios.\n",
      "    * **Input:** \"The cat chased the mouse. If the cat had been asleep, what would have happened?\"\n",
      "    * **Expected Output:** \"The mouse might have escaped/found food/taken a different path.\" (Demonstrates mental simulation)\n",
      "    * **Variant:**  Complex counterfactuals incorporating multiple variables and constraints.\n",
      "\n",
      "**5. Planning (PL):** Tests formulating and executing a sequence of actions to achieve a goal.\n",
      "    * **Input:** \"You need to cross a river with a fox, a chicken, and a sack of grain. You have a boat that can only carry you and one other thing. What should you do?\"\n",
      "    * **Expected Output:** (A logical sequence) \"1. Take the chicken across. 2. Return alone. 3. Take the fox across. 4. Bring the chicken back. 5. Take the grain across. 6. Return alone. 7. Take the chicken across.\"\n",
      "    * **Variant:**  Increasing the number of items and complexities of constraints.\n",
      "\n",
      "**6. Variable Binding (VB):**  Tests understanding and manipulation of variables in abstract statements.\n",
      "    * **Input:**  \"Consider objects A and B.  If A is larger than B, and B is larger than C, then is A larger than C?\"\n",
      "    * **Expected Output:** \"Yes.\" (Demonstrates transitivity and variable relationship understanding.)\n",
      "    * **Variant:**  Introducing complex relationships, inequalities, and quantifiers.\n",
      "\n",
      "\n",
      "\n",
      "**II. Scoring Rubric (Example: Systematic Generalization)**\n",
      "\n",
      "* **0 Points:** Completely incorrect or gibberish.\n",
      "* **1 Point:** Identifies the presence of something on top. (e.g., \"A block sits on another block\"). Partial understanding.\n",
      "* **2 Points:** Correctly identifies the block color. (e.g., \"A yellow block sits on a green block\"). Shows pattern recognition.\n",
      "* **3 Points:** Correctly states the new configuration and acknowledges the inferred pattern.  (e.g. \"A yellow block sits on a green block, because the pattern is that one block sits on another\"). Demonstrates generalization.\n",
      "\n",
      "Similar rubrics would be developed for each task, prioritizing *reasoning steps* and avoiding reward for mere superficial correctness.\n",
      "\n",
      "\n",
      "\n",
      "**III. Adversarial Mitigation & Detection**\n",
      "\n",
      "* **Prompt Engineering Resistance:**\n",
      "    * **Unseen Variants:** Automatically generate numerous variants of each input using synonym replacement, paraphrasing, and structural alterations (e.g., reordering sentences, adding/removing context).  This challenges memorization reliance.\n",
      "    * **Noise Injection:** Add noise to the input (typos, grammatical errors, irrelevant information) to test robustness to imperfections.\n",
      "    * **Negative Constraints:** Explicitly forbid common \"cheating\" techniques in instructions (e.g., \"Do not simply repeat information\" or \"Do not try to find a direct answer online\").\n",
      "* **Memorization Detection:**\n",
      "    * **Statistical Controls:** Monitor next-token probabilities.  Low entropy or predictable next-token distributions strongly suggest memorization.\n",
      "    * **External Search Correlation:** Examine likelihood of extracted phrases from LLM's response appearing verbatim within top search results for the input prompt. High correlation points to retrieved factoids.  Implement a \"blurring\" technique to remove exact matches.\n",
      "    * **Cross-Task Consistency:**  Evaluate consistency between performance across different tasks probing similar underlying skills (e.g., Causal Reasoning & Counterfactuals). Large inconsistencies could show task-specific memorization.\n",
      "\n",
      "\n",
      "\n",
      "**IV. Sample Size & Statistical Tests**\n",
      "\n",
      "* **Sample Size:** Begin with **N=200-300** prompts per task. This allows for reasonable power (estimated using pilot data). Increase if variance is high.\n",
      "* **Statistical Tests:**\n",
      "    * **t-tests/Mann-Whitney U-tests:** Compare mean scores of LLMs for each task.\n",
      "    * **ANOVA/Kruskal-Wallis tests:** Compare performance across multiple LLMs simultaneously.\n",
      "    * **Bayesian Hypothesis Testing:**  Provides probability estimates of LLMs exhibiting superior reasoning ability, accommodating prior beliefs about the task difficulty and model complexity.\n",
      "    * **Effect Size Calculation:** Cohen's d or similar metrics capture the practical significance of observed differences.\n",
      "\n",
      "\n",
      "\n",
      "**V. Potential Adversarial Behaviors & Mitigation**\n",
      "\n",
      "* **\"Hallucinating\" Justifications:** LLMs might produce superficially sound explanations while failing to reason correctly. (Mitigation: Rubric focuses on reasoning steps, not *only* output correctness.)\n",
      "* **\"Exploiting\" Prompts:** LLMs might find a narrow prompt structure yielding high scores, not demonstrating true generalization. (Mitigation: Extensive prompt variants & noise injection)\n",
      "* **\"Circumventing\" Instructions:**  LLMs might ignore negative constraints or semantic reinterpretation of the prompt. (Mitigation: Re-phrasing instruction and negative constraints frequently to avoid easy circumvention)\n",
      "\n",
      "**VI. Experimental Validation & Iteration**\n",
      "\n",
      "* **Human Baseline:** Establish a human baseline score (N=30) for each task.  LLMs must demonstrably surpass this baseline to be considered capable.\n",
      "* **False Positive/Negative Rates:**\n",
      "    * **False Positives:** LLMs achieving high scores despite lacking true reasoning skills. This necessitates rigorous rubric validation and adversarial testing.\n",
      "    * **False Negatives:** LLMs possessing genuine reasoning skills being incorrectly classified as pattern-matchers. This requires re-evaluation of task design and scoring.\n",
      "* **Iteration:**\n",
      "    1. **Analysis of Failure Cases:**  Deep dive into why individual LLMs failed on specific prompts. Categorize failures by type (memorization, misunderstandings, etc.).\n",
      "    2. **Task Enhancement:** Modify tasks to directly address weaknesses revealed in the analysis.\n",
      "    3. **Rubric Refinement:**  Adjust the scoring rubric to be more granular and discriminatory.\n",
      "    4. **Adversarial Prompt Expansion:** Generate new adversarial prompts specifically targeting identified vulnerabilities.\n",
      "\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "This test-suite design goes beyond simple accuracy comparisons. By focusing on compositional reasoning through targeted tasks, rigorous adversarial mitigation, and thorough validation, it aims to reliably differentiate true reasoning capabilities from superficial pattern-matching in LLMs, ultimately driving progress in AI safety and alignment.  This is an iterative process, constantly adapting to new LLM innovations and adversarial strategies.\n",
      "\n",
      "\n",
      "\n",
      "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\n"
     ]
    }
   ],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"results\": [\"1\", \"5\", \"4\", \"3\", \"9\", \"8\", \"6\", \"7\", \"2\"]}\n"
     ]
    }
   ],
   "source": [
    "# Judgement time!\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: gpt-5-mini\n",
      "Rank 2: openai/gpt-oss-120b\n",
      "Rank 3: deepseek-chat\n",
      "Rank 4: gemini-3-flash-preview\n",
      "Rank 5: gemma3:12b\n",
      "Rank 6: gemma3:4b\n",
      "Rank 7: gpt-4.1-mini\n",
      "Rank 8: llama3.2\n",
      "Rank 9: claude-sonnet-4-5\n"
     ]
    }
   ],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"results\": [\"1\", \"4\", \"5\", \"3\", \"6\", \"9\", \"8\", \"7\", \"2\"]}\n"
     ]
    }
   ],
   "source": [
    "# Gemini 3 Flash as the judge\n",
    "\n",
    "response = gemini.chat.completions.create(\n",
    "    model=\"gemini-3-flash-preview\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: gpt-5-mini\n",
      "Rank 2: deepseek-chat\n",
      "Rank 3: openai/gpt-oss-120b\n",
      "Rank 4: gemini-3-flash-preview\n",
      "Rank 5: gpt-4.1-mini\n",
      "Rank 6: gemma3:12b\n",
      "Rank 7: gemma3:4b\n",
      "Rank 8: llama3.2\n",
      "Rank 9: claude-sonnet-4-5\n"
     ]
    }
   ],
   "source": [
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
